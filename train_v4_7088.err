2025-11-29 23:35:23.526114: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-29 23:35:23.582890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-29 23:42:52.136771: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 00:32:26.935164: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 00:32:26.990174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 00:41:53.429897: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:11:22.406329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:11:22.406329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:11:22.421555: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:11:22.449881: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:11:22.460655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 01:11:22.460655: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 01:11:22.469033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 01:11:22.506751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 01:19:57.523509: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:19:57.523509: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:19:57.525673: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 01:19:57.526679: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:24,  4.82s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:24,  4.82s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:24,  4.89s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:24,  4.98s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.93s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.93s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.91s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.93s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.25s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.86s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.86s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.86s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.91s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.82s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.82s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.80s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.78s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.13s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.80s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.79s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.14s/it]

Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.15s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 32. Using DeepSpeed's value.
***** Running training *****
  Num examples = 181
  Num Epochs = 40
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 256
  Gradient Accumulation steps = 32
  Total optimization steps = 40
  Number of trainable parameters = 103,546,880
  0%|          | 0/40 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  2%|▎         | 1/40 [12:39<8:13:49, 759.73s/it]                                                   2%|▎         | 1/40 [12:39<8:13:49, 759.73s/it]  5%|▌         | 2/40 [25:28<8:04:22, 764.80s/it]                                                   5%|▌         | 2/40 [25:28<8:04:22, 764.80s/it]  8%|▊         | 3/40 [38:12<7:51:30, 764.62s/it]                                                   8%|▊         | 3/40 [38:12<7:51:30, 764.62s/it] 10%|█         | 4/40 [50:58<7:39:03, 765.10s/it]                                                  10%|█         | 4/40 [50:58<7:39:03, 765.10s/it] 12%|█▎        | 5/40 [1:03:39<7:25:28, 763.68s/it]                                                    12%|█▎        | 5/40 [1:03:39<7:25:28, 763.68s/it] 15%|█▌        | 6/40 [1:16:26<7:13:25, 764.88s/it]                                                    15%|█▌        | 6/40 [1:16:26<7:13:25, 764.88s/it] 18%|█▊        | 7/40 [1:29:10<7:00:27, 764.47s/it]                                                    18%|█▊        | 7/40 [1:29:10<7:00:27, 764.47s/it] 20%|██        | 8/40 [1:41:56<6:48:02, 765.08s/it]                                                    20%|██        | 8/40 [1:41:56<6:48:02, 765.08s/it] 22%|██▎       | 9/40 [1:54:42<6:35:26, 765.38s/it]                                                    22%|██▎       | 9/40 [1:54:42<6:35:26, 765.38s/it] 25%|██▌       | 10/40 [2:07:32<6:23:17, 766.58s/it]                                                     25%|██▌       | 10/40 [2:07:32<6:23:17, 766.58s/it] 28%|██▊       | 11/40 [2:20:18<6:10:29, 766.52s/it]                                                     28%|██▊       | 11/40 [2:20:18<6:10:29, 766.52s/it] 30%|███       | 12/40 [2:33:05<5:57:43, 766.56s/it]                                                     30%|███       | 12/40 [2:33:05<5:57:43, 766.56s/it] 32%|███▎      | 13/40 [2:45:49<5:44:37, 765.84s/it]                                                     32%|███▎      | 13/40 [2:45:49<5:44:37, 765.84s/it] 35%|███▌      | 14/40 [2:58:35<5:31:53, 765.92s/it]                                                     35%|███▌      | 14/40 [2:58:35<5:31:53, 765.92s/it] 38%|███▊      | 15/40 [3:11:21<5:19:10, 766.01s/it]                                                     38%|███▊      | 15/40 [3:11:21<5:19:10, 766.01s/it] 40%|████      | 16/40 [3:23:54<5:04:48, 762.00s/it]                                                     40%|████      | 16/40 [3:23:54<5:04:48, 762.00s/it] 42%|████▎     | 17/40 [3:36:20<4:50:19, 757.35s/it]                                                     42%|████▎     | 17/40 [3:36:20<4:50:19, 757.35s/it] 45%|████▌     | 18/40 [3:49:05<4:38:28, 759.49s/it]                                                     45%|████▌     | 18/40 [3:49:05<4:38:28, 759.49s/it] 48%|████▊     | 19/40 [4:01:51<4:26:32, 761.53s/it]                                                     48%|████▊     | 19/40 [4:01:51<4:26:32, 761.53s/it] 50%|█████     | 20/40 [4:14:37<4:14:18, 762.92s/it]                                                     50%|█████     | 20/40 [4:14:37<4:14:18, 762.92s/it] 52%|█████▎    | 21/40 [4:27:21<4:01:40, 763.20s/it]                                                     52%|█████▎    | 21/40 [4:27:21<4:01:40, 763.20s/it] 55%|█████▌    | 22/40 [4:40:05<3:48:59, 763.31s/it]                                                     55%|█████▌    | 22/40 [4:40:05<3:48:59, 763.31s/it] 57%|█████▊    | 23/40 [4:52:47<3:36:09, 762.92s/it]                                                     57%|█████▊    | 23/40 [4:52:47<3:36:09, 762.92s/it] 60%|██████    | 24/40 [5:05:31<3:23:35, 763.47s/it]                                                     60%|██████    | 24/40 [5:05:31<3:23:35, 763.47s/it] 62%|██████▎   | 25/40 [5:18:16<3:10:59, 763.95s/it]                                                     62%|██████▎   | 25/40 [5:18:16<3:10:59, 763.95s/it] 65%|██████▌   | 26/40 [5:30:50<2:57:31, 760.84s/it]                                                     65%|██████▌   | 26/40 [5:30:50<2:57:31, 760.84s/it] 68%|██████▊   | 27/40 [5:43:37<2:45:13, 762.56s/it]                                                     68%|██████▊   | 27/40 [5:43:37<2:45:13, 762.56s/it] 70%|███████   | 28/40 [5:56:19<2:32:28, 762.40s/it]                                                     70%|███████   | 28/40 [5:56:19<2:32:28, 762.40s/it] 72%|███████▎  | 29/40 [6:09:03<2:19:53, 763.06s/it]                                                     72%|███████▎  | 29/40 [6:09:03<2:19:53, 763.06s/it] 75%|███████▌  | 30/40 [6:21:39<2:06:48, 760.83s/it]                                                     75%|███████▌  | 30/40 [6:21:39<2:06:48, 760.83s/it] 78%|███████▊  | 31/40 [6:34:21<1:54:10, 761.22s/it]                                                     78%|███████▊  | 31/40 [6:34:21<1:54:10, 761.22s/it] 80%|████████  | 32/40 [6:47:08<1:41:43, 762.99s/it]                                                     80%|████████  | 32/40 [6:47:08<1:41:43, 762.99s/it] 82%|████████▎ | 33/40 [6:59:53<1:29:05, 763.58s/it]                                                     82%|████████▎ | 33/40 [6:59:53<1:29:05, 763.58s/it] 85%|████████▌ | 34/40 [7:12:37<1:16:21, 763.65s/it]                                                     85%|████████▌ | 34/40 [7:12:37<1:16:21, 763.65s/it] 88%|████████▊ | 35/40 [7:25:24<1:03:43, 764.60s/it]                                                     88%|████████▊ | 35/40 [7:25:24<1:03:43, 764.60s/it] 90%|█████████ | 36/40 [7:38:12<51:02, 765.64s/it]                                                     90%|█████████ | 36/40 [7:38:12<51:02, 765.64s/it] 92%|█████████▎| 37/40 [7:50:55<38:14, 764.84s/it]                                                   92%|█████████▎| 37/40 [7:50:55<38:14, 764.84s/it] 95%|█████████▌| 38/40 [8:03:41<25:30, 765.28s/it]                                                   95%|█████████▌| 38/40 [8:03:41<25:30, 765.28s/it] 98%|█████████▊| 39/40 [8:16:26<12:45, 765.22s/it]                                                   98%|█████████▊| 39/40 [8:16:26<12:45, 765.22s/it]100%|██████████| 40/40 [8:29:11<00:00, 765.17s/it]                                                  100%|██████████| 40/40 [8:29:11<00:00, 765.17s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40/checkpoint-40
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40/checkpoint-40/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40/checkpoint-40/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                  100%|██████████| 40/40 [8:29:40<00:00, 765.17s/it]100%|██████████| 40/40 [8:29:40<00:00, 764.50s/it]
Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40/special_tokens_map.json
tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_2_2_32_0.01_40/special_tokens_map.json
