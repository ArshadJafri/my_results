2025-12-06 14:56:45.621803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:56:45.673053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:56:48.587750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:00.086265: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:00.135659: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:02.324688: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:09.834634: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:09.911187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:10.276360: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:10.276360: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:10.276360: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:10.277685: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:10.282467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:10.327369: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:10.327369: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:10.327369: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:10.329069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:10.329069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:10.368501: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:10.407750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:10.418379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:10.457931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-06 14:57:12.503172: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:12.659603: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:12.876634: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:12.881840: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:12.958898: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:12.996320: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:13.188091: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-06 14:57:18.078277: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:17,  3.44s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:17,  3.47s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:17,  3.54s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:18,  3.60s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:18,  3.64s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:18,  3.73s/it]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:16,  4.14s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:16,  4.13s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:16,  4.16s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:16,  4.16s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:18,  3.60s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:16,  4.19s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:17,  4.27s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:17,  3.58s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:12<00:13,  4.37s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:12<00:13,  4.36s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:12<00:13,  4.38s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:12<00:13,  4.38s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:12<00:13,  4.39s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:16,  4.24s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:13<00:13,  4.47s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:08<00:16,  4.24s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:08,  4.48s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:08,  4.48s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:08,  4.47s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:08,  4.48s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:08,  4.49s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:13<00:13,  4.46s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:09,  4.57s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:13<00:13,  4.47s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:22<00:04,  4.56s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:22<00:04,  4.56s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:22<00:04,  4.54s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:22<00:04,  4.54s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:22<00:04,  4.56s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:09,  4.60s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:22<00:04,  4.65s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:17<00:09,  4.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  3.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  4.02s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  3.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  4.02s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  3.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  4.02s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  3.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  4.02s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  3.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  4.04s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  3.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  4.15s/it]
Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:23<00:04,  4.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:26<00:00,  4.45s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:26<00:00,  4.49s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:34<00:09,  9.07s/it]/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:43<00:00,  9.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:43<00:00,  7.32s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 10. Using DeepSpeed's value.
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
***** Running training *****
  Num examples = 181
  Num Epochs = 100
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 160
  Gradient Accumulation steps = 10
  Total optimization steps = 200
  Number of trainable parameters = 103,546,880
  0%|          | 0/200 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|          | 1/200 [05:43<18:58:19, 343.22s/it]                                                     0%|          | 1/200 [05:43<18:58:19, 343.22s/it]  1%|          | 2/200 [06:51<9:59:35, 181.70s/it]                                                     1%|          | 2/200 [06:51<9:59:35, 181.70s/it]  2%|â–         | 3/200 [12:35<13:59:34, 255.71s/it]                                                     2%|â–         | 3/200 [12:35<13:59:34, 255.71s/it]  2%|â–         | 4/200 [13:43<9:53:25, 181.66s/it]                                                     2%|â–         | 4/200 [13:43<9:53:25, 181.66s/it]  2%|â–Ž         | 5/200 [19:26<12:59:06, 239.73s/it]                                                     2%|â–Ž         | 5/200 [19:26<12:59:06, 239.73s/it]  3%|â–Ž         | 6/200 [20:35<9:46:57, 181.53s/it]                                                     3%|â–Ž         | 6/200 [20:35<9:46:57, 181.53s/it]  4%|â–Ž         | 7/200 [26:16<12:32:29, 233.94s/it]                                                     4%|â–Ž         | 7/200 [26:16<12:32:29, 233.94s/it]  4%|â–         | 8/200 [27:24<9:39:40, 181.15s/it]                                                     4%|â–         | 8/200 [27:24<9:39:40, 181.15s/it]  4%|â–         | 9/200 [33:06<12:16:07, 231.24s/it]                                                     4%|â–         | 9/200 [33:06<12:16:07, 231.24s/it]  5%|â–Œ         | 10/200 [34:15<9:33:23, 181.07s/it]                                                     5%|â–Œ         | 10/200 [34:15<9:33:23, 181.07s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.99s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.39s/it][A                                                   
                                             [A  5%|â–Œ         | 10/200 [34:51<9:33:23, 181.07s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.39s/it][A
                                             [A  6%|â–Œ         | 11/200 [40:32<12:40:04, 241.29s/it]                                                      6%|â–Œ         | 11/200 [40:32<12:40:04, 241.29s/it]  6%|â–Œ         | 12/200 [41:41<9:51:00, 188.62s/it]                                                      6%|â–Œ         | 12/200 [41:41<9:51:00, 188.62s/it]  6%|â–‹         | 13/200 [47:21<12:11:18, 234.64s/it]                                                      6%|â–‹         | 13/200 [47:21<12:11:18, 234.64s/it]  7%|â–‹         | 14/200 [48:29<9:31:38, 184.40s/it]                                                      7%|â–‹         | 14/200 [48:29<9:31:38, 184.40s/it]  8%|â–Š         | 15/200 [54:13<11:56:16, 232.31s/it]                                                      8%|â–Š         | 15/200 [54:13<11:56:16, 232.31s/it]  8%|â–Š         | 16/200 [55:21<9:21:11, 183.00s/it]                                                      8%|â–Š         | 16/200 [55:21<9:21:11, 183.00s/it]  8%|â–Š         | 17/200 [1:01:04<11:44:13, 230.89s/it]                                                        8%|â–Š         | 17/200 [1:01:04<11:44:13, 230.89s/it]  9%|â–‰         | 18/200 [1:02:12<9:12:36, 182.18s/it]                                                        9%|â–‰         | 18/200 [1:02:12<9:12:36, 182.18s/it] 10%|â–‰         | 19/200 [1:07:55<11:35:21, 230.51s/it]                                                       10%|â–‰         | 19/200 [1:07:55<11:35:21, 230.51s/it] 10%|â–ˆ         | 20/200 [1:09:03<9:05:08, 181.71s/it]                                                       10%|â–ˆ         | 20/200 [1:09:03<9:05:08, 181.71s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.31s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.35s/it][A                                                     
                                             [A 10%|â–ˆ         | 20/200 [1:09:40<9:05:08, 181.71s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.35s/it][A
                                             [A 10%|â–ˆ         | 21/200 [1:15:21<11:57:24, 240.47s/it]                                                       10%|â–ˆ         | 21/200 [1:15:21<11:57:24, 240.47s/it] 11%|â–ˆ         | 22/200 [1:16:29<9:20:04, 188.79s/it]                                                       11%|â–ˆ         | 22/200 [1:16:29<9:20:04, 188.79s/it] 12%|â–ˆâ–        | 23/200 [1:22:11<11:32:35, 234.78s/it]                                                       12%|â–ˆâ–        | 23/200 [1:22:11<11:32:35, 234.78s/it] 12%|â–ˆâ–        | 24/200 [1:23:20<9:02:56, 185.09s/it]                                                       12%|â–ˆâ–        | 24/200 [1:23:20<9:02:56, 185.09s/it] 12%|â–ˆâ–Ž        | 25/200 [1:29:02<11:16:57, 232.10s/it]                                                       12%|â–ˆâ–Ž        | 25/200 [1:29:02<11:16:57, 232.10s/it] 13%|â–ˆâ–Ž        | 26/200 [1:30:11<8:50:51, 183.05s/it]                                                       13%|â–ˆâ–Ž        | 26/200 [1:30:11<8:50:51, 183.05s/it] 14%|â–ˆâ–Ž        | 27/200 [1:35:51<11:03:47, 230.21s/it]                                                       14%|â–ˆâ–Ž        | 27/200 [1:35:51<11:03:47, 230.21s/it] 14%|â–ˆâ–        | 28/200 [1:36:59<8:40:28, 181.56s/it]                                                       14%|â–ˆâ–        | 28/200 [1:36:59<8:40:28, 181.56s/it] 14%|â–ˆâ–        | 29/200 [1:42:40<10:53:22, 229.25s/it]                                                       14%|â–ˆâ–        | 29/200 [1:42:40<10:53:22, 229.25s/it] 15%|â–ˆâ–Œ        | 30/200 [1:43:48<8:33:04, 181.08s/it]                                                       15%|â–ˆâ–Œ        | 30/200 [1:43:48<8:33:04, 181.08s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.36s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.34s/it][A                                                     
                                             [A 15%|â–ˆâ–Œ        | 30/200 [1:44:24<8:33:04, 181.08s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.34s/it][A
                                             [A 16%|â–ˆâ–Œ        | 31/200 [1:50:03<11:13:56, 239.27s/it]                                                       16%|â–ˆâ–Œ        | 31/200 [1:50:03<11:13:56, 239.27s/it] 16%|â–ˆâ–Œ        | 32/200 [1:51:12<8:46:55, 188.19s/it]                                                       16%|â–ˆâ–Œ        | 32/200 [1:51:12<8:46:55, 188.19s/it] 16%|â–ˆâ–‹        | 33/200 [1:56:53<10:51:15, 233.99s/it]                                                       16%|â–ˆâ–‹        | 33/200 [1:56:53<10:51:15, 233.99s/it] 17%|â–ˆâ–‹        | 34/200 [1:58:01<8:29:46, 184.26s/it]                                                       17%|â–ˆâ–‹        | 34/200 [1:58:01<8:29:46, 184.26s/it] 18%|â–ˆâ–Š        | 35/200 [2:03:41<10:34:59, 230.91s/it]                                                       18%|â–ˆâ–Š        | 35/200 [2:03:41<10:34:59, 230.91s/it] 18%|â–ˆâ–Š        | 36/200 [2:04:50<8:18:06, 182.24s/it]                                                       18%|â–ˆâ–Š        | 36/200 [2:04:50<8:18:06, 182.24s/it] 18%|â–ˆâ–Š        | 37/200 [2:10:31<10:25:00, 230.07s/it]                                                       18%|â–ˆâ–Š        | 37/200 [2:10:31<10:25:00, 230.07s/it] 19%|â–ˆâ–‰        | 38/200 [2:11:39<8:09:55, 181.46s/it]                                                       19%|â–ˆâ–‰        | 38/200 [2:11:39<8:09:55, 181.46s/it] 20%|â–ˆâ–‰        | 39/200 [2:17:21<10:15:22, 229.33s/it]                                                       20%|â–ˆâ–‰        | 39/200 [2:17:21<10:15:22, 229.33s/it] 20%|â–ˆâ–ˆ        | 40/200 [2:18:29<8:03:05, 181.16s/it]                                                       20%|â–ˆâ–ˆ        | 40/200 [2:18:29<8:03:05, 181.16s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.35s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.34s/it][A                                                     
                                             [A 20%|â–ˆâ–ˆ        | 40/200 [2:19:05<8:03:05, 181.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.34s/it][A
                                             [A 20%|â–ˆâ–ˆ        | 41/200 [2:24:47<10:36:41, 240.26s/it]                                                       20%|â–ˆâ–ˆ        | 41/200 [2:24:47<10:36:41, 240.26s/it] 21%|â–ˆâ–ˆ        | 42/200 [2:25:56<8:17:19, 188.86s/it]                                                       21%|â–ˆâ–ˆ        | 42/200 [2:25:56<8:17:19, 188.86s/it] 22%|â–ˆâ–ˆâ–       | 43/200 [2:31:38<10:13:44, 234.55s/it]                                                       22%|â–ˆâ–ˆâ–       | 43/200 [2:31:38<10:13:44, 234.55s/it] 22%|â–ˆâ–ˆâ–       | 44/200 [2:32:47<8:00:44, 184.90s/it]                                                       22%|â–ˆâ–ˆâ–       | 44/200 [2:32:47<8:00:44, 184.90s/it] 22%|â–ˆâ–ˆâ–Ž       | 45/200 [2:38:27<9:58:14, 231.58s/it]                                                      22%|â–ˆâ–ˆâ–Ž       | 45/200 [2:38:27<9:58:14, 231.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 46/200 [2:39:36<7:49:22, 182.87s/it]                                                      23%|â–ˆâ–ˆâ–Ž       | 46/200 [2:39:36<7:49:22, 182.87s/it] 24%|â–ˆâ–ˆâ–Ž       | 47/200 [2:45:19<9:48:37, 230.84s/it]                                                      24%|â–ˆâ–ˆâ–Ž       | 47/200 [2:45:19<9:48:37, 230.84s/it] 24%|â–ˆâ–ˆâ–       | 48/200 [2:46:28<7:41:29, 182.17s/it]                                                      24%|â–ˆâ–ˆâ–       | 48/200 [2:46:28<7:41:29, 182.17s/it] 24%|â–ˆâ–ˆâ–       | 49/200 [2:52:08<9:37:49, 229.60s/it]                                                      24%|â–ˆâ–ˆâ–       | 49/200 [2:52:08<9:37:49, 229.60s/it] 25%|â–ˆâ–ˆâ–Œ       | 50/200 [2:53:17<7:33:25, 181.37s/it]                                                      25%|â–ˆâ–ˆâ–Œ       | 50/200 [2:53:17<7:33:25, 181.37s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.26s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.35s/it][A                                                     
                                             [A 25%|â–ˆâ–ˆâ–Œ       | 50/200 [2:53:53<7:33:25, 181.37s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.35s/it][A
                                             [A 26%|â–ˆâ–ˆâ–Œ       | 51/200 [2:59:34<9:56:09, 240.06s/it]                                                      26%|â–ˆâ–ˆâ–Œ       | 51/200 [2:59:34<9:56:09, 240.06s/it] 26%|â–ˆâ–ˆâ–Œ       | 52/200 [3:00:43<7:45:36, 188.76s/it]                                                      26%|â–ˆâ–ˆâ–Œ       | 52/200 [3:00:43<7:45:36, 188.76s/it] 26%|â–ˆâ–ˆâ–‹       | 53/200 [3:06:24<9:34:35, 234.53s/it]                                                      26%|â–ˆâ–ˆâ–‹       | 53/200 [3:06:24<9:34:35, 234.53s/it] 27%|â–ˆâ–ˆâ–‹       | 54/200 [3:07:33<7:29:33, 184.75s/it]                                                      27%|â–ˆâ–ˆâ–‹       | 54/200 [3:07:33<7:29:33, 184.75s/it] 28%|â–ˆâ–ˆâ–Š       | 55/200 [3:13:14<9:19:36, 231.56s/it]                                                      28%|â–ˆâ–ˆâ–Š       | 55/200 [3:13:14<9:19:36, 231.56s/it] 28%|â–ˆâ–ˆâ–Š       | 56/200 [3:14:22<7:18:11, 182.58s/it]                                                      28%|â–ˆâ–ˆâ–Š       | 56/200 [3:14:22<7:18:11, 182.58s/it] 28%|â–ˆâ–ˆâ–Š       | 57/200 [3:20:03<9:08:12, 230.02s/it]                                                      28%|â–ˆâ–ˆâ–Š       | 57/200 [3:20:03<9:08:12, 230.02s/it] 29%|â–ˆâ–ˆâ–‰       | 58/200 [3:21:11<7:09:25, 181.44s/it]                                                      29%|â–ˆâ–ˆâ–‰       | 58/200 [3:21:11<7:09:25, 181.44s/it] 30%|â–ˆâ–ˆâ–‰       | 59/200 [3:26:52<8:59:13, 229.46s/it]                                                      30%|â–ˆâ–ˆâ–‰       | 59/200 [3:26:52<8:59:13, 229.46s/it] 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [3:28:00<7:02:18, 180.99s/it]                                                      30%|â–ˆâ–ˆâ–ˆ       | 60/200 [3:28:00<7:02:18, 180.99s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.34s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.38s/it][A                                                     
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [3:28:36<7:02:18, 180.99s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.38s/it][A
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [3:34:17<9:15:28, 239.77s/it]                                                      30%|â–ˆâ–ˆâ–ˆ       | 61/200 [3:34:17<9:15:28, 239.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [3:35:25<7:12:47, 188.17s/it]                                                      31%|â–ˆâ–ˆâ–ˆ       | 62/200 [3:35:25<7:12:47, 188.17s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [3:41:07<8:55:05, 234.35s/it]                                                      32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [3:41:07<8:55:05, 234.35s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [3:42:16<6:58:51, 184.79s/it]                                                      32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [3:42:16<6:58:51, 184.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [3:47:56<8:40:35, 231.38s/it]                                                      32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [3:47:56<8:40:35, 231.38s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [3:49:04<6:47:19, 182.38s/it]                                                      33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [3:49:04<6:47:19, 182.38s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [3:54:42<8:28:00, 229.17s/it]                                                      34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [3:54:42<8:28:00, 229.17s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [3:55:51<6:38:05, 180.95s/it]                                                      34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [3:55:51<6:38:05, 180.95s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [4:01:33<8:20:26, 229.21s/it]                                                      34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [4:01:33<8:20:26, 229.21s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [4:02:41<6:31:54, 180.88s/it]                                                      35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [4:02:41<6:31:54, 180.88s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.39s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.35s/it][A                                                     
                                             [A 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [4:03:17<6:31:54, 180.88s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.35s/it][A
                                             [A 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [4:09:00<8:36:44, 240.35s/it]                                                      36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [4:09:00<8:36:44, 240.35s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [4:10:08<6:42:45, 188.79s/it]                                                      36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [4:10:08<6:42:45, 188.79s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [4:15:51<8:17:28, 235.03s/it]                                                      36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [4:15:51<8:17:28, 235.03s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [4:17:00<6:28:49, 185.15s/it]                                                      37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [4:17:00<6:28:49, 185.15s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [4:22:41<8:02:45, 231.73s/it]                                                      38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [4:22:41<8:02:45, 231.73s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [4:23:49<6:17:25, 182.62s/it]                                                      38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [4:23:49<6:17:25, 182.62s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [4:29:31<7:52:36, 230.54s/it]                                                      38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [4:29:31<7:52:36, 230.54s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [4:30:40<6:10:05, 182.02s/it]                                                      39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [4:30:40<6:10:05, 182.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [4:36:21<7:43:42, 229.94s/it]                                                      40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [4:36:21<7:43:42, 229.94s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [4:37:31<6:03:30, 181.76s/it]                                                      40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [4:37:31<6:03:30, 181.76s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.36s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.34s/it][A                                                     
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [4:38:07<6:03:30, 181.76s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.34s/it][A
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [4:43:47<7:56:09, 240.08s/it]                                                      40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [4:43:47<7:56:09, 240.08s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [4:44:55<6:10:38, 188.46s/it]                                                      41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [4:44:55<6:10:38, 188.46s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [4:50:37<7:37:08, 234.43s/it]                                                      42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [4:50:37<7:37:08, 234.43s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [4:51:45<5:57:02, 184.67s/it]                                                      42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [4:51:45<5:57:02, 184.67s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [4:57:27<7:24:07, 231.71s/it]                                                      42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [4:57:27<7:24:07, 231.71s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [4:58:35<5:46:51, 182.56s/it]                                                      43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [4:58:35<5:46:51, 182.56s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [5:04:17<7:13:55, 230.40s/it]                                                      44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [5:04:17<7:13:55, 230.40s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [5:05:25<5:39:06, 181.67s/it]                                                      44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [5:05:25<5:39:06, 181.67s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [5:11:08<7:05:40, 230.10s/it]                                                      44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [5:11:08<7:05:40, 230.10s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [5:12:16<5:32:53, 181.58s/it]                                                      45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [5:12:16<5:32:53, 181.58s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.32s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.36s/it][A                                                     
                                             [A 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [5:12:52<5:32:53, 181.58s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.36s/it][A
                                             [A 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [5:18:35<7:17:27, 240.80s/it]                                                      46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [5:18:35<7:17:27, 240.80s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [5:19:43<5:40:17, 189.05s/it]                                                      46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [5:19:43<5:40:17, 189.05s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [5:25:26<6:59:10, 235.05s/it]                                                      46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [5:25:26<6:59:10, 235.05s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [5:26:34<5:26:55, 185.05s/it]                                                      47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [5:26:34<5:26:55, 185.05s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [5:32:15<6:45:45, 231.86s/it]                                                      48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [5:32:15<6:45:45, 231.86s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [5:33:24<5:16:57, 182.86s/it]                                                      48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [5:33:24<5:16:57, 182.86s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [5:39:04<6:35:01, 230.12s/it]                                                      48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [5:39:04<6:35:01, 230.12s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [5:40:12<5:08:28, 181.45s/it]                                                      49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [5:40:12<5:08:28, 181.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [5:45:54<6:26:42, 229.73s/it]                                                      50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [5:45:54<6:26:42, 229.73s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [5:47:04<5:02:46, 181.66s/it]                                                       50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [5:47:04<5:02:46, 181.66s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.38s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.35s/it][A                                                      
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [5:47:40<5:02:46, 181.66s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.35s/it][A
                                             [ASaving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/checkpoint-100
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/checkpoint-100/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/checkpoint-100/special_tokens_map.json
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [5:53:45<6:48:07, 247.35s/it]                                                       50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [5:53:45<6:48:07, 247.35s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [5:54:53<5:16:32, 193.80s/it]                                                       51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [5:54:53<5:16:32, 193.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [6:00:37<6:26:08, 238.85s/it]                                                       52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [6:00:37<6:26:08, 238.85s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [6:01:46<5:00:25, 187.77s/it]                                                       52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [6:01:46<5:00:25, 187.77s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [6:07:27<6:09:54, 233.63s/it]                                                       52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [6:07:27<6:09:54, 233.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [6:08:35<4:48:27, 184.13s/it]                                                       53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [6:08:35<4:48:27, 184.13s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [6:14:16<5:58:20, 231.19s/it]                                                       54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [6:14:16<5:58:20, 231.19s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [6:15:25<4:40:00, 182.62s/it]                                                       54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [6:15:25<4:40:00, 182.62s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [6:21:07<5:49:16, 230.29s/it]                                                       55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [6:21:07<5:49:16, 230.29s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [6:22:16<4:33:03, 182.04s/it]                                                       55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [6:22:16<4:33:03, 182.04s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.25s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.38s/it][A                                                      
                                             [A 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [6:22:53<4:33:03, 182.04s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.38s/it][A
                                             [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [6:28:34<5:57:09, 240.78s/it]                                                       56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [6:28:34<5:57:09, 240.78s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [6:29:43<4:37:27, 189.18s/it]                                                       56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [6:29:43<4:37:27, 189.18s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [6:35:25<5:40:44, 234.99s/it]                                                       56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [6:35:25<5:40:44, 234.99s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [6:36:33<4:24:51, 184.79s/it]                                                       57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [6:36:33<4:24:51, 184.79s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [6:42:13<5:28:01, 231.55s/it]                                                       57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [6:42:13<5:28:01, 231.55s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [6:43:22<4:15:35, 182.56s/it]                                                       58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [6:43:22<4:15:35, 182.56s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [6:49:02<5:17:58, 229.86s/it]                                                       58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [6:49:02<5:17:58, 229.86s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [6:50:10<4:08:00, 181.46s/it]                                                       59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [6:50:10<4:08:00, 181.46s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [6:55:50<5:09:07, 228.99s/it]                                                       60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [6:55:50<5:09:07, 228.99s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [6:56:59<4:01:24, 181.05s/it]                                                       60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [6:56:59<4:01:24, 181.05s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.31s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.37s/it][A                                                      
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [6:57:36<4:01:24, 181.05s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.37s/it][A
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [7:03:17<5:15:58, 239.98s/it]                                                       60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [7:03:17<5:15:58, 239.98s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [7:04:25<4:04:51, 188.36s/it]                                                       61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [7:04:25<4:04:51, 188.36s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [7:10:06<5:00:43, 234.33s/it]                                                       62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [7:10:06<5:00:43, 234.33s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [7:11:15<3:53:59, 184.73s/it]                                                       62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [7:11:15<3:53:59, 184.73s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [7:16:56<4:49:33, 231.64s/it]                                                       62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [7:16:56<4:49:33, 231.64s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [7:18:05<3:45:27, 182.80s/it]                                                       63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [7:18:05<3:45:27, 182.80s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [7:23:48<4:40:50, 230.83s/it]                                                       64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [7:23:48<4:40:50, 230.83s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [7:24:57<3:38:38, 182.21s/it]                                                       64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [7:24:57<3:38:38, 182.21s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [7:30:38<4:31:49, 229.72s/it]                                                       64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [7:30:38<4:31:49, 229.72s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [7:31:46<3:31:35, 181.37s/it]                                                       65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [7:31:46<3:31:35, 181.37s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.40s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.35s/it][A                                                      
                                             [A 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [7:32:22<3:31:35, 181.37s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.35s/it][A
                                             [A 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [7:38:05<4:36:51, 240.74s/it]                                                       66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [7:38:05<4:36:51, 240.74s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [7:39:14<3:34:17, 189.07s/it]                                                       66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [7:39:14<3:34:17, 189.07s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [7:44:54<4:21:48, 234.45s/it]                                                       66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [7:44:54<4:21:48, 234.45s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [7:46:03<3:23:12, 184.73s/it]                                                       67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [7:46:03<3:23:12, 184.73s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [7:51:44<4:10:49, 231.53s/it]                                                       68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [7:51:44<4:10:49, 231.53s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [7:52:52<3:14:52, 182.69s/it]                                                       68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [7:52:52<3:14:52, 182.69s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [7:58:34<4:02:01, 230.50s/it]                                                       68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [7:58:34<4:02:01, 230.50s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [7:59:42<3:07:42, 181.66s/it]                                                       69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [7:59:42<3:07:42, 181.66s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [8:05:24<3:53:36, 229.77s/it]                                                       70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [8:05:24<3:53:36, 229.77s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [8:06:33<3:01:24, 181.41s/it]                                                       70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [8:06:33<3:01:24, 181.41s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.41s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.36s/it][A                                                      
                                             [A 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [8:07:09<3:01:24, 181.41s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.36s/it][A
                                             [A 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [8:12:50<3:56:06, 240.10s/it]                                                       70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [8:12:50<3:56:06, 240.10s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [8:13:58<3:02:12, 188.49s/it]                                                       71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [8:13:58<3:02:12, 188.49s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [8:19:40<3:42:47, 234.52s/it]                                                       72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [8:19:40<3:42:47, 234.52s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [8:20:48<2:52:27, 184.77s/it]                                                       72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [8:20:48<2:52:27, 184.77s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [8:26:29<3:32:17, 231.59s/it]                                                       72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [8:26:29<3:32:17, 231.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [8:27:38<2:44:33, 182.84s/it]                                                       73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [8:27:38<2:44:33, 182.84s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [8:33:21<3:23:53, 230.82s/it]                                                       74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [8:33:21<3:23:53, 230.82s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [8:34:30<2:37:51, 182.15s/it]                                                       74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [8:34:30<2:37:51, 182.15s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [8:40:12<3:15:44, 230.28s/it]                                                       74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [8:40:12<3:15:44, 230.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [8:41:21<2:31:34, 181.90s/it]                                                       75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [8:41:21<2:31:34, 181.90s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.39s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.33s/it][A                                                      
                                             [A 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [8:41:58<2:31:34, 181.90s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.33s/it][A
                                             [A 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [8:47:39<3:16:26, 240.55s/it]                                                       76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [8:47:39<3:16:26, 240.55s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [8:48:47<2:31:09, 188.96s/it]                                                       76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [8:48:47<2:31:09, 188.96s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [8:54:29<3:03:53, 234.77s/it]                                                       76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [8:54:29<3:03:53, 234.77s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [8:55:37<2:21:44, 184.87s/it]                                                       77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [8:55:37<2:21:44, 184.87s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [9:01:19<2:53:57, 231.95s/it]                                                       78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [9:01:19<2:53:57, 231.95s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [9:02:28<2:14:05, 182.86s/it]                                                       78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [9:02:28<2:14:05, 182.86s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [9:08:10<2:45:23, 230.79s/it]                                                       78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [9:08:10<2:45:23, 230.79s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [9:09:19<2:07:31, 182.18s/it]                                                       79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [9:09:19<2:07:31, 182.18s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [9:15:01<2:37:16, 230.17s/it]                                                       80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [9:15:01<2:37:16, 230.17s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [9:16:09<2:01:05, 181.64s/it]                                                       80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [9:16:09<2:01:05, 181.64s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.33s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.35s/it][A                                                      
                                             [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [9:16:46<2:01:05, 181.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.35s/it][A
                                             [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [9:22:27<2:36:19, 240.49s/it]                                                       80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [9:22:27<2:36:19, 240.49s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [9:23:35<1:59:28, 188.64s/it]                                                       81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [9:23:35<1:59:28, 188.64s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [9:29:15<2:24:22, 234.11s/it]                                                       82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [9:29:15<2:24:22, 234.11s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [9:30:24<1:50:44, 184.58s/it]                                                       82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [9:30:24<1:50:44, 184.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [9:36:06<2:15:11, 231.77s/it]                                                       82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [9:36:06<2:15:11, 231.77s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [9:37:14<1:43:28, 182.62s/it]                                                       83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [9:37:14<1:43:28, 182.62s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [9:42:56<2:06:44, 230.43s/it]                                                       84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [9:42:56<2:06:44, 230.43s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [9:44:05<1:37:07, 182.10s/it]                                                       84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [9:44:05<1:37:07, 182.10s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [9:49:45<1:58:28, 229.30s/it]                                                       84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [9:49:45<1:58:28, 229.30s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [9:50:54<1:30:37, 181.24s/it]                                                       85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [9:50:54<1:30:37, 181.24s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.35s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.36s/it][A                                                      
                                             [A 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [9:51:30<1:30:37, 181.24s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.36s/it][A
                                             [A 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [9:57:12<1:56:08, 240.29s/it]                                                       86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [9:57:12<1:56:08, 240.29s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [9:58:21<1:28:08, 188.86s/it]                                                       86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [9:58:21<1:28:08, 188.86s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [10:04:02<1:45:31, 234.49s/it]                                                        86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [10:04:02<1:45:31, 234.49s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [10:05:10<1:19:58, 184.57s/it]                                                        87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [10:05:10<1:19:58, 184.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [10:10:52<1:36:33, 231.73s/it]                                                        88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [10:10:52<1:36:33, 231.73s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [10:11:59<1:13:01, 182.54s/it]                                                        88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [10:11:59<1:13:01, 182.54s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [10:17:41<1:28:14, 230.21s/it]                                                        88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [10:17:41<1:28:14, 230.21s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [10:18:49<1:06:37, 181.69s/it]                                                        89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [10:18:49<1:06:37, 181.69s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [10:24:30<1:20:19, 229.50s/it]                                                        90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [10:24:30<1:20:19, 229.50s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [10:25:39<1:00:23, 181.19s/it]                                                        90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [10:25:39<1:00:23, 181.19s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.32s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.36s/it][A                                                       
                                             [A 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [10:26:15<1:00:23, 181.19s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.36s/it][A
                                             [A 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [10:31:54<1:15:50, 239.48s/it]                                                        90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [10:31:54<1:15:50, 239.48s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [10:33:04<56:36, 188.67s/it]                                                        91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [10:33:04<56:36, 188.67s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [10:38:47<1:06:30, 234.72s/it]                                                        92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [10:38:47<1:06:30, 234.72s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [10:39:54<49:14, 184.64s/it]                                                        92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [10:39:54<49:14, 184.64s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [10:45:37<57:59, 231.97s/it]                                                      92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [10:45:37<57:59, 231.97s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [10:46:46<42:43, 183.12s/it]                                                      93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [10:46:46<42:43, 183.12s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [10:52:27<49:57, 230.54s/it]                                                      94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [10:52:27<49:57, 230.54s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [10:53:35<36:20, 181.69s/it]                                                      94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [10:53:35<36:20, 181.69s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [10:59:15<42:01, 229.27s/it]                                                      94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [10:59:15<42:01, 229.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [11:00:24<30:11, 181.12s/it]                                                      95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [11:00:24<30:11, 181.12s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.39s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.33s/it][A                                                     
                                             [A 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [11:01:00<30:11, 181.12s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.33s/it][A
                                             [A 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [11:06:42<36:01, 240.13s/it]                                                      96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [11:06:42<36:01, 240.13s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [11:07:49<25:07, 188.44s/it]                                                      96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [11:07:49<25:07, 188.44s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [11:13:32<27:22, 234.66s/it]                                                      96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [11:13:32<27:22, 234.66s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [11:14:40<18:28, 184.75s/it]                                                      97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [11:14:40<18:28, 184.75s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [11:20:21<19:17, 231.43s/it]                                                      98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [11:20:21<19:17, 231.43s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [11:21:30<12:11, 182.77s/it]                                                      98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [11:21:30<12:11, 182.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [11:27:11<11:30, 230.31s/it]                                                      98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [11:27:11<11:30, 230.31s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [11:28:20<06:03, 181.77s/it]                                                      99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [11:28:20<06:03, 181.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [11:34:01<03:49, 229.75s/it]                                                     100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [11:34:01<03:49, 229.75s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:35:10<00:00, 181.41s/it]                                                     100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:35:10<00:00, 181.41s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.34s/it][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  7.35s/it][A                                                     
                                             [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:35:46<00:00, 181.41s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  7.35s/it][A
                                             [ASaving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/checkpoint-200
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/checkpoint-200/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/checkpoint-200/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:36:07<00:00, 181.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:36:07<00:00, 208.84s/it]
Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/special_tokens_map.json
tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_100/special_tokens_map.json
[rank6]:[W1207 02:37:33.924957636 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank1]:[W1207 02:37:33.003161546 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank4]:[W1207 02:37:33.018396577 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank7]:[W1207 02:37:33.037087743 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank5]:[W1207 02:37:34.162167702 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank3]:[W1207 02:37:34.192915510 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank2]:[W1207 02:37:34.275303999 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank0]:[W1207 02:37:38.530237323 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1207 02:37:46.772112077 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1207 02:37:48.530100413 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
