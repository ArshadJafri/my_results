Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-12-01 15:35:02,283] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-12-01 15:35:02,283] [INFO] [runner.py:630:main] cmd = /home/axs7716/anaconda3/envs/arsh_env/bin/python3.12 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --log_level=info Finetuning_V5.py 0.0002 8870 2 2 10 0.01 115
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-12-01 15:35:16,350] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-12-01 15:35:16,350] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-12-01 15:35:16,350] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-12-01 15:35:16,350] [INFO] [launch.py:180:main] dist_world_size=8
[2025-12-01 15:35:16,350] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-12-01 15:35:16,351] [INFO] [launch.py:272:main] process 26485 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=0', '0.0002', '8870', '2', '2', '10', '0.01', '115']
[2025-12-01 15:35:16,352] [INFO] [launch.py:272:main] process 26486 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=1', '0.0002', '8870', '2', '2', '10', '0.01', '115']
[2025-12-01 15:35:16,354] [INFO] [launch.py:272:main] process 26487 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=2', '0.0002', '8870', '2', '2', '10', '0.01', '115']
[2025-12-01 15:35:16,355] [INFO] [launch.py:272:main] process 26488 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=3', '0.0002', '8870', '2', '2', '10', '0.01', '115']
[2025-12-01 15:35:16,356] [INFO] [launch.py:272:main] process 26489 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=4', '0.0002', '8870', '2', '2', '10', '0.01', '115']
[2025-12-01 15:35:16,357] [INFO] [launch.py:272:main] process 26490 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=5', '0.0002', '8870', '2', '2', '10', '0.01', '115']
[2025-12-01 15:35:16,358] [INFO] [launch.py:272:main] process 26491 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=6', '0.0002', '8870', '2', '2', '10', '0.01', '115']
[2025-12-01 15:35:16,359] [INFO] [launch.py:272:main] process 26492 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=7', '0.0002', '8870', '2', '2', '10', '0.01', '115']

======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================


======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================


======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================


======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================


======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================


======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================


======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================


======================================================================
ğŸ“Š CONFIGURATION
======================================================================
Learning Rate:              0.0002
Max Seq Length:             8870
Train Batch Size:           2
Eval Batch Size:            2
Gradient Accumulation:      10
Weight Decay:               0.01
Num Epochs:                 115
======================================================================

ğŸ“¥ Downloading datasets...
ğŸ“‚ Loading datasets...
ğŸ“¥ Downloading datasets...
ğŸ“¥ Downloading datasets...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...
ğŸ“‚ Loading datasets...
ğŸ“‚ Loading datasets...
ğŸ“¥ Downloading datasets...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...
ğŸ“‚ Loading datasets...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...
âš™ï¸ Tokenizing train dataset...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...
âš™ï¸ Tokenizing eval dataset...

ğŸ¤– Loading model...
âš™ï¸ Tokenizing train dataset...
âš™ï¸ Tokenizing train dataset...
âš™ï¸ Tokenizing eval dataset...
âš™ï¸ Tokenizing eval dataset...

ğŸ¤– Loading model...
âš™ï¸ Tokenizing train dataset...

ğŸ¤– Loading model...
âš™ï¸ Tokenizing eval dataset...

ğŸ¤– Loading model...
ğŸ“¥ Downloading datasets...
ğŸ“¥ Downloading datasets...
ğŸ“‚ Loading datasets...
ğŸ“‚ Loading datasets...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...
ğŸ“¥ Downloading datasets...
ğŸ“¥ Downloading datasets...
ğŸ“‚ Loading datasets...
âš™ï¸ Tokenizing train dataset...
ğŸ“‚ Loading datasets...
âš™ï¸ Tokenizing eval dataset...
âš™ï¸ Tokenizing train dataset...

ğŸ¤– Loading model...
âš™ï¸ Tokenizing eval dataset...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...

ğŸ¤– Loading model...
âœ… Train samples: 181
âœ… Eval samples: 61

ğŸ”¤ Loading tokenizer...
âš™ï¸ Tokenizing train dataset...
âš™ï¸ Tokenizing train dataset...
âš™ï¸ Tokenizing eval dataset...
âš™ï¸ Tokenizing eval dataset...

ğŸ¤– Loading model...

ğŸ¤– Loading model...
ğŸ”§ Preparing model for k-bit training...
ğŸ¯ Configuring LoRA...

ğŸ“Š Model Statistics:
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

======================================================================
ğŸš€ STARTING TRAINING
======================================================================
Output directory: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115
Effective batch size per GPU: 20
======================================================================

[2025-12-01 15:36:27,418] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26485
[2025-12-01 15:36:29,109] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26486
[2025-12-01 15:36:30,882] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26487
[2025-12-01 15:36:32,080] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26488
[2025-12-01 15:36:33,277] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26489
ğŸ”§ Preparing model for k-bit training...
ğŸ”§ Preparing model for k-bit training...
ğŸ¯ Configuring LoRA...
ğŸ¯ Configuring LoRA...
[2025-12-01 15:36:34,605] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26490
[2025-12-01 15:36:34,605] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26491
[2025-12-01 15:36:36,031] [INFO] [launch.py:335:sigkill_handler] Killing subprocess 26492
[2025-12-01 15:36:37,306] [ERROR] [launch.py:341:sigkill_handler] ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=7', '0.0002', '8870', '2', '2', '10', '0.01', '115'] exits with return code = 1
