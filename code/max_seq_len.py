# -*- coding: utf-8 -*-
"""max_seq_len.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kAuFjq5tueXWJ4m6R409IZowjS1LSKsu
"""

from transformers import AutoTokenizer
from datasets import load_dataset, Features, Value
from huggingface_hub import hf_hub_download
import numpy as np
import matplotlib.pyplot as plt

TRAIN_REPO_ID = "arshad101/cognition_task_psych"
TRAIN_FILE_NAME= "combined_train_fixed.jsonl"
EVAL_REPO_ID ="arshad101/cognition_task_psych_test"
EVAL_FILE_NAME ="combined_test_fixed.jsonl"

custom_features = Features({
    'text':Value('string'),
    'experiment':Value('string'),
    'participant':Value('string')
})

print("Downloading Datasets ....")

train_file_path = hf_hub_download(
    repo_id = TRAIN_REPO_ID,
    filename = TRAIN_FILE_NAME,
    repo_type ="dataset",
    token = True
)


eval_file_path = hf_hub_download(
    repo_id =EVAL_REPO_ID,
    filename =EVAL_FILE_NAME,
    repo_type = "dataset",
    token = True
)

print("Loading Datasets")
raw_train_dataset = load_dataset(
    'json',
    data_files = {'train': train_file_path},
    features = custom_features,
)

train_dataset = raw_train_dataset["train"]

raw_eval_dataset = load_dataset(
    'json',
    data_files = {'test':eval_file_path},
    features = custom_features,
)


eval_dataset = raw_eval_dataset["test"]

print("Loading Tokenizer")
tokenizer = AutoTokenizer.from_pretrained("marcelbinz/Llama-3.1-Centaur-70B-adapter")
print("Analyse Train data")
train_length =[]

for example in train_dataset:
  tokens = tokenizer(example["text"], truncation= False)
  train_lengths.append(len(tokens["input_ids"]))

print("For eval_data")
eval_length = []
for example in eval_dataset:
  tokens = tokenizer(example["text"], truncation = False)
  eval_length.append(len(tokens["input_ids"]))

all_length = train_length +eval_length

print("Sequence Length Statistics")
print(f"Total examples :{len(all_length)}")
print(f"  - Training: {len(train_lengths)}")
print(f"  - Evaluation: {len(eval_lengths)}")
print()

print(f"Minimum length: {np.min(all_lengths)}")
print(f"Maximum length: {np.max(all_lengths)}")
print(f"Mean length: {np.mean(all_lengths):.2f}")
print(f"Median length: {np.median(all_lengths):.2f}")
print(f"Std deviation: {np.std(all_lengths):.2f}")
print()
print("Percentiles:")
print(f"  50th (median): {np.percentile(all_lengths, 50):.0f}")
print(f"  75th: {np.percentile(all_lengths, 75):.0f}")
print(f"  90th: {np.percentile(all_lengths, 90):.0f}")
print(f"  95th: {np.percentile(all_lengths, 95):.0f}")
print(f"  99th: {np.percentile(all_lengths, 99):.0f}")
print(f"  99.5th: {np.percentile(all_lengths, 99.5):.0f}")
print()

test_length = [2048, 4096, 8192, 16384, 32768]v

for test_len in test_length:
  truncated = sum(1 for l in all_length is l >test_len)

from huggingface_hub import notebook_login
notebook_login()

"""
Script to analyze dataset and find optimal max_seq_length for training.
This will tokenize all your data and show you the distribution of sequence lengths.
"""

from transformers import AutoTokenizer
from datasets import load_dataset, Features, Value
from huggingface_hub import hf_hub_download
import numpy as np
import matplotlib.pyplot as plt

# --- REPOSITORY AND FILE CONSTANTS ---
TRAIN_REPO_ID = "arshad101/cognition_task_psych"
TRAIN_FILE_NAME = "combined_train_fixed.jsonl"
EVAL_REPO_ID = "arshad101/cognition_task_psych_test"
EVAL_FILE_NAME = "combined_test_fixed.jsonl"

# Define the mandatory schema
custom_features = Features({
    'text': Value('string'),
    'experiment': Value('string'),
    'participant': Value('string'),
})

print("Downloading datasets...")
# Download the private files
train_file_path = hf_hub_download(
    repo_id=TRAIN_REPO_ID,
    filename=TRAIN_FILE_NAME,
    repo_type="dataset",
    token=True
)

eval_file_path = hf_hub_download(
    repo_id=EVAL_REPO_ID,
    filename=EVAL_FILE_NAME,
    repo_type="dataset",
    token=True
)

# Load datasets
print("Loading datasets...")
raw_train_dataset = load_dataset(
    'json',
    data_files={'train': train_file_path},
    features=custom_features,
)
train_dataset = raw_train_dataset['train']

raw_eval_dataset = load_dataset(
    'json',
    data_files={'test': eval_file_path},
    features=custom_features,
)
eval_dataset = raw_eval_dataset['test']

# Load the same tokenizer used in training
print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("marcelbinz/Llama-3.1-Centaur-70B-adapter")

# Tokenize all examples and collect lengths
print("Analyzing training data...")
train_lengths = []
for example in train_dataset:
    tokens = tokenizer(example['text'], truncation=False)
    train_lengths.append(len(tokens['input_ids']))

print("Analyzing evaluation data...")
eval_lengths = []
for example in eval_dataset:
    tokens = tokenizer(example['text'], truncation=False)
    eval_lengths.append(len(tokens['input_ids']))

# Combine all lengths
all_lengths = train_lengths + eval_lengths

# Calculate statistics
print("\n" + "=" * 60)
print("SEQUENCE LENGTH STATISTICS")
print("=" * 60)
print(f"Total examples: {len(all_lengths)}")
print(f"  - Training: {len(train_lengths)}")
print(f"  - Evaluation: {len(eval_lengths)}")
print()
print(f"Minimum length: {np.min(all_lengths)}")
print(f"Maximum length: {np.max(all_lengths)}")
print(f"Mean length: {np.mean(all_lengths):.2f}")
print(f"Median length: {np.median(all_lengths):.2f}")
print(f"Std deviation: {np.std(all_lengths):.2f}")
print()
print("Percentiles:")
print(f"  50th (median): {np.percentile(all_lengths, 50):.0f}")
print(f"  75th: {np.percentile(all_lengths, 75):.0f}")
print(f"  90th: {np.percentile(all_lengths, 90):.0f}")
print(f"  95th: {np.percentile(all_lengths, 95):.0f}")
print(f"  99th: {np.percentile(all_lengths, 99):.0f}")
print(f"  99.5th: {np.percentile(all_lengths, 99.5):.0f}")
print()

# Recommendations
print("=" * 60)
print("RECOMMENDATIONS")
print("=" * 60)

# Calculate how many examples would be truncated at different lengths
test_lengths = [2048, 4096, 8192, 16384, 32768]
for test_len in test_lengths:
    truncated = sum(1 for l in all_lengths if l > test_len)
    pct = (truncated / len(all_lengths)) * 100
    print(f"max_seq_length={test_len:5d}: {truncated:3d} examples truncated ({pct:5.2f}%)")

print()
print("RECOMMENDED max_seq_length:")
# Find the length that covers 95% of examples
recommended_95 = int(np.percentile(all_lengths, 95))
# Round up to nearest power of 2 or common size
common_sizes = [512, 1024, 2048, 4096, 8192, 16384, 32768]
recommended = min([s for s in common_sizes if s >= recommended_95])
print(f"  - For 95% coverage: {recommended}")
print(f"    (This truncates only {sum(1 for l in all_lengths if l > recommended)} examples)")

# Find the length that covers 99% of examples
recommended_99 = int(np.percentile(all_lengths, 99))
recommended_99_rounded = min([s for s in common_sizes if s >= recommended_99])
print(f"  - For 99% coverage: {recommended_99_rounded}")
print(f"    (This truncates only {sum(1 for l in all_lengths if l > recommended_99_rounded)} examples)")

print()
print("Memory vs Coverage Tradeoff:")
print("  - Lower max_seq_length = Less GPU memory, faster training, but more truncation")
print("  - Higher max_seq_length = More GPU memory, slower training, but less truncation")
print()
print(f"SUGGESTION: Start with max_seq_length={recommended} for good balance")
print("=" * 60)

# Plot distribution
plt.figure(figsize=(12, 6))
plt.hist(all_lengths, bins=50, edgecolor='black', alpha=0.7)
plt.axvline(np.mean(all_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(all_lengths):.0f}')
plt.axvline(np.percentile(all_lengths, 95), color='green', linestyle='--', label=f'95th percentile: {np.percentile(all_lengths, 95):.0f}')
plt.axvline(np.percentile(all_lengths, 99), color='orange', linestyle='--', label=f'99th percentile: {np.percentile(all_lengths, 99):.0f}')
plt.xlabel('Sequence Length (tokens)')
plt.ylabel('Number of Examples')
plt.title('Distribution of Sequence Lengths in Dataset')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('sequence_length_distribution.png', dpi=150, bbox_inches='tight')
print("\nSaved plot to: sequence_length_distribution.png")

