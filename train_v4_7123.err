2025-12-01 13:11:17.402457: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:17.461528: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:20.684073: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:32.936306: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:32.992641: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:35.451013: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.576679: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.635242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:43.794453: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.794453: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.794453: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.838792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.848866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:43.848866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:43.848866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:43.896387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:43.911177: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.946695: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:43.967736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:44.001474: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:46.359522: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:46.360045: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:46.429243: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:46.475103: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:46.526427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:46.566287: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:46.640159: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:49.342411: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:11:49.398743: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:11:51.833070: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
`torch_dtype` is deprecated! Use `dtype` instead!
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.87s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.88s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.98s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.98s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.99s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:20,  4.02s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:20,  4.02s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:20,  4.06s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.89s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.89s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.89s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.91s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.95s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.94s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.96s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:20,  5.01s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.43s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.43s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.46s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.50s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.47s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.49s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.50s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:16,  5.57s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:11,  5.81s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:11,  5.80s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:11,  5.83s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:11,  5.79s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.84s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.86s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.87s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:22<00:11,  5.91s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:27<00:05,  5.86s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:27<00:05,  5.86s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:27<00:05,  5.87s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.88s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.88s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.92s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:05,  5.96s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:28<00:06,  6.02s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.73s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.73s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.74s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.08s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.73s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.09s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.75s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.09s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.10s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  4.73s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.13s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:31<00:00,  4.93s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:31<00:00,  5.22s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 10. Using DeepSpeed's value.
***** Running training *****
  Num examples = 181
  Num Epochs = 115
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 160
  Gradient Accumulation steps = 10
  Total optimization steps = 230
  Number of trainable parameters = 103,546,880
  0%|          | 0/230 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/230 [05:43<21:51:50, 343.72s/it]                                                     0%|          | 1/230 [05:43<21:51:50, 343.72s/it]  1%|          | 2/230 [06:52<11:31:50, 182.06s/it]                                                     1%|          | 2/230 [06:52<11:31:50, 182.06s/it]  1%|▏         | 3/230 [12:35<16:06:07, 255.36s/it]                                                     1%|▏         | 3/230 [12:35<16:06:07, 255.36s/it]  2%|▏         | 4/230 [13:43<11:24:15, 181.66s/it]                                                     2%|▏         | 4/230 [13:43<11:24:15, 181.66s/it]  2%|▏         | 5/230 [19:27<15:00:45, 240.20s/it]                                                     2%|▏         | 5/230 [19:27<15:00:45, 240.20s/it]  3%|▎         | 6/230 [20:36<11:18:59, 181.87s/it]                                                     3%|▎         | 6/230 [20:36<11:18:59, 181.87s/it]  3%|▎         | 7/230 [26:18<14:30:23, 234.18s/it]                                                     3%|▎         | 7/230 [26:18<14:30:23, 234.18s/it]  3%|▎         | 8/230 [27:27<11:12:08, 181.66s/it]                                                     3%|▎         | 8/230 [27:27<11:12:08, 181.66s/it]  4%|▍         | 9/230 [33:08<14:12:56, 231.57s/it]                                                     4%|▍         | 9/230 [33:08<14:12:56, 231.57s/it]  4%|▍         | 10/230 [34:17<11:05:08, 181.40s/it]                                                      4%|▍         | 10/230 [34:17<11:05:08, 181.40s/it]  5%|▍         | 11/230 [40:00<14:01:35, 230.57s/it]                                                      5%|▍         | 11/230 [40:00<14:01:35, 230.57s/it]  5%|▌         | 12/230 [41:09<10:59:28, 181.51s/it]                                                      5%|▌         | 12/230 [41:09<10:59:28, 181.51s/it]  6%|▌         | 13/230 [46:49<13:50:42, 229.69s/it]                                                      6%|▌         | 13/230 [46:49<13:50:42, 229.69s/it]  6%|▌         | 14/230 [47:58<10:51:23, 180.94s/it]                                                      6%|▌         | 14/230 [47:58<10:51:23, 180.94s/it]  7%|▋         | 15/230 [53:40<13:42:41, 229.59s/it]                                                      7%|▋         | 15/230 [53:40<13:42:41, 229.59s/it]  7%|▋         | 16/230 [54:48<10:45:38, 181.02s/it]                                                      7%|▋         | 16/230 [54:48<10:45:38, 181.02s/it]  7%|▋         | 17/230 [1:00:31<13:34:50, 229.53s/it]                                                        7%|▋         | 17/230 [1:00:31<13:34:50, 229.53s/it]  8%|▊         | 18/230 [1:01:40<10:40:48, 181.36s/it]                                                        8%|▊         | 18/230 [1:01:40<10:40:48, 181.36s/it]  8%|▊         | 19/230 [1:07:22<13:27:31, 229.63s/it]                                                        8%|▊         | 19/230 [1:07:22<13:27:31, 229.63s/it]  9%|▊         | 20/230 [1:08:31<10:34:33, 181.30s/it]                                                        9%|▊         | 20/230 [1:08:31<10:34:33, 181.30s/it]  9%|▉         | 21/230 [1:14:14<13:20:45, 229.88s/it]                                                        9%|▉         | 21/230 [1:14:14<13:20:45, 229.88s/it] 10%|▉         | 22/230 [1:15:22<10:29:11, 181.50s/it]                                                       10%|▉         | 22/230 [1:15:22<10:29:11, 181.50s/it] 10%|█         | 23/230 [1:21:05<13:12:31, 229.72s/it]                                                       10%|█         | 23/230 [1:21:05<13:12:31, 229.72s/it] 10%|█         | 24/230 [1:22:14<10:23:43, 181.67s/it]                                                       10%|█         | 24/230 [1:22:14<10:23:43, 181.67s/it] 11%|█         | 25/230 [1:27:56<13:05:10, 229.81s/it]                                                       11%|█         | 25/230 [1:27:56<13:05:10, 229.81s/it] 11%|█▏        | 26/230 [1:29:05<10:16:55, 181.45s/it]                                                       11%|█▏        | 26/230 [1:29:05<10:16:55, 181.45s/it] 12%|█▏        | 27/230 [1:34:46<12:56:07, 229.39s/it]                                                       12%|█▏        | 27/230 [1:34:46<12:56:07, 229.39s/it] 12%|█▏        | 28/230 [1:35:55<10:10:10, 181.24s/it]                                                       12%|█▏        | 28/230 [1:35:55<10:10:10, 181.24s/it] 13%|█▎        | 29/230 [1:41:37<12:49:05, 229.58s/it]                                                       13%|█▎        | 29/230 [1:41:37<12:49:05, 229.58s/it] 13%|█▎        | 30/230 [1:42:46<10:04:33, 181.37s/it]                                                       13%|█▎        | 30/230 [1:42:46<10:04:33, 181.37s/it] 13%|█▎        | 31/230 [1:48:26<12:39:02, 228.86s/it]                                                       13%|█▎        | 31/230 [1:48:26<12:39:02, 228.86s/it] 14%|█▍        | 32/230 [1:49:35<9:56:46, 180.84s/it]                                                       14%|█▍        | 32/230 [1:49:35<9:56:46, 180.84s/it] 14%|█▍        | 33/230 [1:55:15<12:30:50, 228.68s/it]                                                       14%|█▍        | 33/230 [1:55:15<12:30:50, 228.68s/it] 15%|█▍        | 34/230 [1:56:23<9:49:45, 180.54s/it]                                                       15%|█▍        | 34/230 [1:56:23<9:49:45, 180.54s/it] 15%|█▌        | 35/230 [2:02:05<12:23:43, 228.84s/it]                                                       15%|█▌        | 35/230 [2:02:05<12:23:43, 228.84s/it] 16%|█▌        | 36/230 [2:03:13<9:44:20, 180.72s/it]                                                       16%|█▌        | 36/230 [2:03:13<9:44:20, 180.72s/it] 16%|█▌        | 37/230 [2:08:55<12:16:41, 229.02s/it]                                                       16%|█▌        | 37/230 [2:08:55<12:16:41, 229.02s/it]slurmstepd-h100-8s-02: error: *** JOB 7123 ON h100-8s-02 CANCELLED AT 2025-12-01T15:25:50 ***
