2025-12-01 13:35:10.957966: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:35:11.016529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 13:40:45.322818: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:16:19.556405: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:16:19.614229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:22:17.808586: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.651603: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.651606: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.651603: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.651603: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.660319: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.660319: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.660319: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:26.705688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:43:26.705688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:43:26.705688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:43:26.705688: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:43:26.717359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:43:26.717359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:43:26.717359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:43:28.745733: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:43:28.801437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 14:49:38.077625: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:49:38.078330: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:49:38.077693: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:49:38.078021: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:49:38.078021: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:49:38.078023: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:49:38.078660: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:49:38.079789: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:21,  4.37s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:21,  4.39s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:21,  4.39s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:21,  4.39s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:22,  4.57s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:23,  4.64s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:23,  4.64s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:23,  4.65s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.88s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.89s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.92s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.91s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.96s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.95s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.95s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:20,  5.01s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.15s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.15s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.15s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.42s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.40s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.41s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.43s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:10,  5.46s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:10,  5.46s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:10,  5.44s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:21<00:10,  5.49s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.50s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.51s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.52s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.52s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.54s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.53s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.58s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.58s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.85s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.85s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.86s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.48s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.87s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.89s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.88s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.53s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.89s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.58s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.94s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 10. Using DeepSpeed's value.
***** Running training *****
  Num examples = 181
  Num Epochs = 115
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 160
  Gradient Accumulation steps = 10
  Total optimization steps = 230
  Number of trainable parameters = 103,546,880
  0%|          | 0/230 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/230 [05:43<21:52:50, 343.98s/it]                                                     0%|          | 1/230 [05:43<21:52:50, 343.98s/it]  1%|          | 2/230 [06:52<11:32:21, 182.20s/it]                                                     1%|          | 2/230 [06:52<11:32:21, 182.20s/it]  1%|▏         | 3/230 [12:36<16:08:19, 255.94s/it]                                                     1%|▏         | 3/230 [12:36<16:08:19, 255.94s/it]  2%|▏         | 4/230 [13:45<11:25:47, 182.07s/it]                                                     2%|▏         | 4/230 [13:45<11:25:47, 182.07s/it]  2%|▏         | 5/230 [19:28<15:00:48, 240.22s/it]                                                     2%|▏         | 5/230 [19:28<15:00:48, 240.22s/it]  3%|▎         | 6/230 [20:37<11:18:53, 181.85s/it]                                                     3%|▎         | 6/230 [20:37<11:18:53, 181.85s/it]  3%|▎         | 7/230 [26:20<14:31:32, 234.50s/it]                                                     3%|▎         | 7/230 [26:20<14:31:32, 234.50s/it]  3%|▎         | 8/230 [27:28<11:11:26, 181.47s/it]                                                     3%|▎         | 8/230 [27:28<11:11:26, 181.47s/it]  4%|▍         | 9/230 [33:10<14:13:13, 231.65s/it]                                                     4%|▍         | 9/230 [33:10<14:13:13, 231.65s/it]  4%|▍         | 10/230 [34:18<11:04:21, 181.19s/it]                                                      4%|▍         | 10/230 [34:18<11:04:21, 181.19s/it]  5%|▍         | 11/230 [40:00<14:01:24, 230.52s/it]                                                      5%|▍         | 11/230 [40:00<14:01:24, 230.52s/it]  5%|▌         | 12/230 [41:09<10:58:46, 181.31s/it]                                                      5%|▌         | 12/230 [41:09<10:58:46, 181.31s/it]  6%|▌         | 13/230 [46:51<13:51:26, 229.89s/it]                                                      6%|▌         | 13/230 [46:51<13:51:26, 229.89s/it]  6%|▌         | 14/230 [47:58<10:50:45, 180.77s/it]                                                      6%|▌         | 14/230 [47:58<10:50:45, 180.77s/it]  7%|▋         | 15/230 [53:41<13:43:38, 229.85s/it]                                                      7%|▋         | 15/230 [53:42<13:43:38, 229.85s/it]  7%|▋         | 16/230 [54:50<10:46:39, 181.31s/it]                                                      7%|▋         | 16/230 [54:50<10:46:39, 181.31s/it]  7%|▋         | 17/230 [1:00:32<13:34:44, 229.51s/it]                                                        7%|▋         | 17/230 [1:00:32<13:34:44, 229.51s/it]  8%|▊         | 18/230 [1:01:41<10:40:37, 181.31s/it]                                                        8%|▊         | 18/230 [1:01:41<10:40:37, 181.31s/it]  8%|▊         | 19/230 [1:07:23<13:27:39, 229.66s/it]                                                        8%|▊         | 19/230 [1:07:23<13:27:39, 229.66s/it]  9%|▊         | 20/230 [1:08:31<10:33:27, 180.99s/it]                                                        9%|▊         | 20/230 [1:08:31<10:33:27, 180.99s/it]  9%|▉         | 21/230 [1:14:14<13:19:50, 229.62s/it]                                                        9%|▉         | 21/230 [1:14:14<13:19:50, 229.62s/it] 10%|▉         | 22/230 [1:15:22<10:28:27, 181.28s/it]                                                       10%|▉         | 22/230 [1:15:22<10:28:27, 181.28s/it] 10%|█         | 23/230 [1:21:04<13:11:38, 229.46s/it]                                                       10%|█         | 23/230 [1:21:04<13:11:38, 229.46s/it] 10%|█         | 24/230 [1:22:13<10:22:57, 181.44s/it]                                                       10%|█         | 24/230 [1:22:13<10:22:57, 181.44s/it] 11%|█         | 25/230 [1:27:55<13:04:21, 229.57s/it]                                                       11%|█         | 25/230 [1:27:55<13:04:21, 229.57s/it] 11%|█▏        | 26/230 [1:29:04<10:16:10, 181.23s/it]                                                       11%|█▏        | 26/230 [1:29:04<10:16:10, 181.23s/it] 12%|█▏        | 27/230 [1:34:44<12:54:15, 228.85s/it]                                                       12%|█▏        | 27/230 [1:34:44<12:54:15, 228.85s/it] 12%|█▏        | 28/230 [1:35:53<10:08:58, 180.88s/it]                                                       12%|█▏        | 28/230 [1:35:53<10:08:58, 180.88s/it] 13%|█▎        | 29/230 [1:41:35<12:48:17, 229.34s/it]                                                       13%|█▎        | 29/230 [1:41:35<12:48:17, 229.34s/it] 13%|█▎        | 30/230 [1:42:44<10:03:57, 181.19s/it]                                                       13%|█▎        | 30/230 [1:42:44<10:03:57, 181.19s/it] 13%|█▎        | 31/230 [1:48:24<12:39:00, 228.85s/it]                                                       13%|█▎        | 31/230 [1:48:24<12:39:00, 228.85s/it] 14%|█▍        | 32/230 [1:49:33<9:56:54, 180.88s/it]                                                       14%|█▍        | 32/230 [1:49:33<9:56:54, 180.88s/it] 14%|█▍        | 33/230 [1:55:14<12:31:58, 229.03s/it]                                                       14%|█▍        | 33/230 [1:55:14<12:31:58, 229.03s/it] 15%|█▍        | 34/230 [1:56:23<9:50:36, 180.80s/it]                                                       15%|█▍        | 34/230 [1:56:23<9:50:36, 180.80s/it] 15%|█▌        | 35/230 [2:02:03<12:23:38, 228.81s/it]                                                       15%|█▌        | 35/230 [2:02:03<12:23:38, 228.81s/it] 16%|█▌        | 36/230 [2:03:12<9:44:33, 180.79s/it]                                                       16%|█▌        | 36/230 [2:03:12<9:44:33, 180.79s/it] 16%|█▌        | 37/230 [2:08:54<12:16:58, 229.11s/it]                                                       16%|█▌        | 37/230 [2:08:54<12:16:58, 229.11s/it] 17%|█▋        | 38/230 [2:10:03<9:39:12, 181.00s/it]                                                       17%|█▋        | 38/230 [2:10:03<9:39:12, 181.00s/it] 17%|█▋        | 39/230 [2:15:45<12:10:00, 229.32s/it]                                                       17%|█▋        | 39/230 [2:15:45<12:10:00, 229.32s/it] 17%|█▋        | 40/230 [2:16:53<9:33:18, 181.04s/it]                                                       17%|█▋        | 40/230 [2:16:53<9:33:18, 181.04s/it] 18%|█▊        | 41/230 [2:22:35<12:02:05, 229.23s/it]                                                       18%|█▊        | 41/230 [2:22:35<12:02:05, 229.23s/it] 18%|█▊        | 42/230 [2:23:43<9:26:49, 180.90s/it]                                                       18%|█▊        | 42/230 [2:23:43<9:26:49, 180.90s/it] 19%|█▊        | 43/230 [2:29:25<11:54:12, 229.16s/it]                                                       19%|█▊        | 43/230 [2:29:25<11:54:12, 229.16s/it] 19%|█▉        | 44/230 [2:30:34<9:21:17, 181.06s/it]                                                       19%|█▉        | 44/230 [2:30:34<9:21:17, 181.06s/it] 20%|█▉        | 45/230 [2:36:15<11:46:29, 229.13s/it]                                                       20%|█▉        | 45/230 [2:36:15<11:46:29, 229.13s/it] 20%|██        | 46/230 [2:37:24<9:15:46, 181.23s/it]                                                       20%|██        | 46/230 [2:37:24<9:15:46, 181.23s/it] 20%|██        | 47/230 [2:43:07<11:40:01, 229.51s/it]                                                       20%|██        | 47/230 [2:43:07<11:40:01, 229.51s/it] 21%|██        | 48/230 [2:44:15<9:09:20, 181.10s/it]                                                       21%|██        | 48/230 [2:44:15<9:09:20, 181.10s/it] 21%|██▏       | 49/230 [2:49:55<11:30:43, 228.97s/it]                                                       21%|██▏       | 49/230 [2:49:55<11:30:43, 228.97s/it] 22%|██▏       | 50/230 [2:51:04<9:02:48, 180.94s/it]                                                       22%|██▏       | 50/230 [2:51:04<9:02:48, 180.94s/it] 22%|██▏       | 51/230 [2:56:46<11:24:10, 229.33s/it]                                                       22%|██▏       | 51/230 [2:56:46<11:24:10, 229.33s/it] 23%|██▎       | 52/230 [2:57:55<8:57:22, 181.14s/it]                                                       23%|██▎       | 52/230 [2:57:55<8:57:22, 181.14s/it] 23%|██▎       | 53/230 [3:03:37<11:16:30, 229.32s/it]                                                       23%|██▎       | 53/230 [3:03:37<11:16:30, 229.32s/it] 23%|██▎       | 54/230 [3:04:47<8:52:38, 181.58s/it]                                                       23%|██▎       | 54/230 [3:04:47<8:52:38, 181.58s/it] 24%|██▍       | 55/230 [3:10:29<11:09:44, 229.63s/it]                                                       24%|██▍       | 55/230 [3:10:29<11:09:44, 229.63s/it] 24%|██▍       | 56/230 [3:11:38<8:46:17, 181.48s/it]                                                       24%|██▍       | 56/230 [3:11:38<8:46:17, 181.48s/it] 25%|██▍       | 57/230 [3:17:19<11:01:19, 229.36s/it]                                                       25%|██▍       | 57/230 [3:17:19<11:01:19, 229.36s/it] 25%|██▌       | 58/230 [3:18:26<8:38:10, 180.76s/it]                                                       25%|██▌       | 58/230 [3:18:26<8:38:10, 180.76s/it] 26%|██▌       | 59/230 [3:24:09<10:53:17, 229.23s/it]                                                       26%|██▌       | 59/230 [3:24:09<10:53:17, 229.23s/it] 26%|██▌       | 60/230 [3:25:17<8:32:17, 180.81s/it]                                                       26%|██▌       | 60/230 [3:25:17<8:32:17, 180.81s/it] 27%|██▋       | 61/230 [3:30:58<10:45:06, 229.03s/it]                                                       27%|██▋       | 61/230 [3:30:58<10:45:06, 229.03s/it] 27%|██▋       | 62/230 [3:32:06<8:25:38, 180.59s/it]                                                       27%|██▋       | 62/230 [3:32:06<8:25:38, 180.59s/it] 27%|██▋       | 63/230 [3:37:47<10:36:40, 228.74s/it]                                                       27%|██▋       | 63/230 [3:37:47<10:36:40, 228.74s/it] 28%|██▊       | 64/230 [3:38:56<8:20:32, 180.92s/it]                                                       28%|██▊       | 64/230 [3:38:56<8:20:32, 180.92s/it] 28%|██▊       | 65/230 [3:44:37<10:29:11, 228.80s/it]                                                       28%|██▊       | 65/230 [3:44:37<10:29:11, 228.80s/it] 29%|██▊       | 66/230 [3:45:46<8:14:43, 181.00s/it]                                                       29%|██▊       | 66/230 [3:45:46<8:14:43, 181.00s/it] 29%|██▉       | 67/230 [3:51:25<10:20:10, 228.29s/it]                                                       29%|██▉       | 67/230 [3:51:25<10:20:10, 228.29s/it] 30%|██▉       | 68/230 [3:52:33<8:07:02, 180.38s/it]                                                       30%|██▉       | 68/230 [3:52:33<8:07:02, 180.38s/it] 30%|███       | 69/230 [3:58:15<10:14:01, 228.83s/it]                                                       30%|███       | 69/230 [3:58:15<10:14:01, 228.83s/it] 30%|███       | 70/230 [3:59:23<8:01:48, 180.68s/it]                                                       30%|███       | 70/230 [3:59:23<8:01:48, 180.68s/it] 31%|███       | 71/230 [4:05:07<10:08:00, 229.44s/it]                                                       31%|███       | 71/230 [4:05:07<10:08:00, 229.44s/it] 31%|███▏      | 72/230 [4:06:15<7:57:03, 181.16s/it]                                                       31%|███▏      | 72/230 [4:06:15<7:57:03, 181.16s/it] 32%|███▏      | 73/230 [4:11:58<10:00:52, 229.63s/it]                                                       32%|███▏      | 73/230 [4:11:58<10:00:52, 229.63s/it] 32%|███▏      | 74/230 [4:13:07<7:51:50, 181.48s/it]                                                       32%|███▏      | 74/230 [4:13:07<7:51:50, 181.48s/it] 33%|███▎      | 75/230 [4:18:47<9:51:46, 229.08s/it]                                                      33%|███▎      | 75/230 [4:18:47<9:51:46, 229.08s/it] 33%|███▎      | 76/230 [4:19:55<7:43:56, 180.76s/it]                                                      33%|███▎      | 76/230 [4:19:55<7:43:56, 180.76s/it] 33%|███▎      | 77/230 [4:25:39<9:46:00, 229.81s/it]                                                      33%|███▎      | 77/230 [4:25:39<9:46:00, 229.81s/it] 34%|███▍      | 78/230 [4:26:48<7:39:37, 181.43s/it]                                                      34%|███▍      | 78/230 [4:26:48<7:39:37, 181.43s/it] 34%|███▍      | 79/230 [4:32:30<9:37:42, 229.55s/it]                                                      34%|███▍      | 79/230 [4:32:30<9:37:42, 229.55s/it] 35%|███▍      | 80/230 [4:33:39<7:33:52, 181.55s/it]                                                      35%|███▍      | 80/230 [4:33:39<7:33:52, 181.55s/it] 35%|███▌      | 81/230 [4:39:21<9:30:20, 229.67s/it]                                                      35%|███▌      | 81/230 [4:39:21<9:30:20, 229.67s/it] 36%|███▌      | 82/230 [4:40:29<7:26:46, 181.13s/it]                                                      36%|███▌      | 82/230 [4:40:29<7:26:46, 181.13s/it] 36%|███▌      | 83/230 [4:46:12<9:22:14, 229.49s/it]                                                      36%|███▌      | 83/230 [4:46:12<9:22:14, 229.49s/it] 37%|███▋      | 84/230 [4:47:20<7:20:38, 181.09s/it]                                                      37%|███▋      | 84/230 [4:47:20<7:20:38, 181.09s/it] 37%|███▋      | 85/230 [4:53:01<9:13:55, 229.21s/it]                                                      37%|███▋      | 85/230 [4:53:01<9:13:55, 229.21s/it] 37%|███▋      | 86/230 [4:54:09<7:14:09, 180.90s/it]                                                      37%|███▋      | 86/230 [4:54:09<7:14:09, 180.90s/it] 38%|███▊      | 87/230 [4:59:51<9:06:23, 229.26s/it]                                                      38%|███▊      | 87/230 [4:59:51<9:06:23, 229.26s/it] 38%|███▊      | 88/230 [5:01:00<7:08:32, 181.07s/it]                                                      38%|███▊      | 88/230 [5:01:00<7:08:32, 181.07s/it] 39%|███▊      | 89/230 [5:06:44<9:00:11, 229.87s/it]                                                      39%|███▊      | 89/230 [5:06:44<9:00:11, 229.87s/it] 39%|███▉      | 90/230 [5:07:52<7:03:12, 181.38s/it]                                                      39%|███▉      | 90/230 [5:07:52<7:03:12, 181.38s/it] 40%|███▉      | 91/230 [5:13:35<8:52:15, 229.75s/it]                                                      40%|███▉      | 91/230 [5:13:35<8:52:15, 229.75s/it] 40%|████      | 92/230 [5:14:43<6:57:15, 181.42s/it]                                                      40%|████      | 92/230 [5:14:43<6:57:15, 181.42s/it] 40%|████      | 93/230 [5:20:25<8:44:21, 229.64s/it]                                                      40%|████      | 93/230 [5:20:25<8:44:21, 229.64s/it] 41%|████      | 94/230 [5:21:34<6:51:08, 181.39s/it]                                                      41%|████      | 94/230 [5:21:34<6:51:08, 181.39s/it] 41%|████▏     | 95/230 [5:27:16<8:36:15, 229.45s/it]                                                      41%|████▏     | 95/230 [5:27:16<8:36:15, 229.45s/it] 42%|████▏     | 96/230 [5:28:24<6:44:16, 181.02s/it]                                                      42%|████▏     | 96/230 [5:28:24<6:44:16, 181.02s/it] 42%|████▏     | 97/230 [5:34:06<8:28:05, 229.22s/it]                                                      42%|████▏     | 97/230 [5:34:06<8:28:05, 229.22s/it] 43%|████▎     | 98/230 [5:35:13<6:37:45, 180.80s/it]                                                      43%|████▎     | 98/230 [5:35:13<6:37:45, 180.80s/it] 43%|████▎     | 99/230 [5:40:56<8:20:37, 229.30s/it]                                                      43%|████▎     | 99/230 [5:40:56<8:20:37, 229.30s/it] 43%|████▎     | 100/230 [5:42:05<6:32:35, 181.20s/it]                                                       43%|████▎     | 100/230 [5:42:05<6:32:35, 181.20s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|█████     | 2/4 [00:08<00:08,  4.31s/it][A
 75%|███████▌  | 3/4 [00:17<00:06,  6.25s/it][A
100%|██████████| 4/4 [00:26<00:00,  7.32s/it][A                                                      
                                             [A 43%|████▎     | 100/230 [5:42:41<6:32:35, 181.20s/it]
100%|██████████| 4/4 [00:27<00:00,  7.32s/it][A
                                             [ASaving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-100
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-100/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-100/special_tokens_map.json
 44%|████▍     | 101/230 [5:48:50<8:53:47, 248.28s/it]                                                       44%|████▍     | 101/230 [5:48:50<8:53:47, 248.28s/it] 44%|████▍     | 102/230 [5:49:59<6:54:55, 194.49s/it]                                                       44%|████▍     | 102/230 [5:49:59<6:54:55, 194.49s/it] 45%|████▍     | 103/230 [5:55:43<8:27:10, 239.61s/it]                                                       45%|████▍     | 103/230 [5:55:43<8:27:10, 239.61s/it] 45%|████▌     | 104/230 [5:56:52<6:35:35, 188.37s/it]                                                       45%|████▌     | 104/230 [5:56:52<6:35:35, 188.37s/it] 46%|████▌     | 105/230 [6:02:34<8:08:07, 234.30s/it]                                                       46%|████▌     | 105/230 [6:02:34<8:08:07, 234.30s/it] 46%|████▌     | 106/230 [6:03:42<6:21:04, 184.39s/it]                                                       46%|████▌     | 106/230 [6:03:42<6:21:04, 184.39s/it] 47%|████▋     | 107/230 [6:09:24<7:55:04, 231.75s/it]                                                       47%|████▋     | 107/230 [6:09:24<7:55:04, 231.75s/it] 47%|████▋     | 108/230 [6:10:33<6:12:04, 182.99s/it]                                                       47%|████▋     | 108/230 [6:10:33<6:12:04, 182.99s/it] 47%|████▋     | 109/230 [6:16:15<7:44:55, 230.54s/it]                                                       47%|████▋     | 109/230 [6:16:15<7:44:55, 230.54s/it] 48%|████▊     | 110/230 [6:17:24<6:04:18, 182.15s/it]                                                       48%|████▊     | 110/230 [6:17:24<6:04:18, 182.15s/it] 48%|████▊     | 111/230 [6:23:05<7:35:59, 229.91s/it]                                                       48%|████▊     | 111/230 [6:23:05<7:35:59, 229.91s/it] 49%|████▊     | 112/230 [6:24:15<5:57:30, 181.78s/it]                                                       49%|████▊     | 112/230 [6:24:15<5:57:30, 181.78s/it] 49%|████▉     | 113/230 [6:29:57<7:28:31, 230.01s/it]                                                       49%|████▉     | 113/230 [6:29:57<7:28:31, 230.01s/it] 50%|████▉     | 114/230 [6:31:05<5:50:41, 181.39s/it]                                                       50%|████▉     | 114/230 [6:31:05<5:50:41, 181.39s/it] 50%|█████     | 115/230 [6:36:48<7:20:13, 229.68s/it]                                                       50%|█████     | 115/230 [6:36:48<7:20:13, 229.68s/it] 50%|█████     | 116/230 [6:37:56<5:44:19, 181.23s/it]                                                       50%|█████     | 116/230 [6:37:56<5:44:19, 181.23s/it] 51%|█████     | 117/230 [6:43:36<7:11:06, 228.91s/it]                                                       51%|█████     | 117/230 [6:43:36<7:11:06, 228.91s/it] 51%|█████▏    | 118/230 [6:44:44<5:37:27, 180.78s/it]                                                       51%|█████▏    | 118/230 [6:44:44<5:37:27, 180.78s/it] 52%|█████▏    | 119/230 [6:50:25<7:03:13, 228.77s/it]                                                       52%|█████▏    | 119/230 [6:50:25<7:03:13, 228.77s/it] 52%|█████▏    | 120/230 [6:51:35<5:31:49, 180.99s/it]                                                       52%|█████▏    | 120/230 [6:51:35<5:31:49, 180.99s/it] 53%|█████▎    | 121/230 [6:57:17<6:56:41, 229.37s/it]                                                       53%|█████▎    | 121/230 [6:57:17<6:56:41, 229.37s/it] 53%|█████▎    | 122/230 [6:58:25<5:25:43, 180.95s/it]                                                       53%|█████▎    | 122/230 [6:58:25<5:25:43, 180.95s/it] 53%|█████▎    | 123/230 [7:04:06<6:48:35, 229.12s/it]                                                       53%|█████▎    | 123/230 [7:04:06<6:48:35, 229.12s/it] 54%|█████▍    | 124/230 [7:05:17<5:20:31, 181.43s/it]                                                       54%|█████▍    | 124/230 [7:05:17<5:20:31, 181.43s/it] 54%|█████▍    | 125/230 [7:10:57<6:41:14, 229.28s/it]                                                       54%|█████▍    | 125/230 [7:10:57<6:41:14, 229.28s/it] 55%|█████▍    | 126/230 [7:12:06<5:13:47, 181.03s/it]                                                       55%|█████▍    | 126/230 [7:12:06<5:13:47, 181.03s/it] 55%|█████▌    | 127/230 [7:17:50<6:34:30, 229.81s/it]                                                       55%|█████▌    | 127/230 [7:17:50<6:34:30, 229.81s/it] 56%|█████▌    | 128/230 [7:18:58<5:08:35, 181.52s/it]                                                       56%|█████▌    | 128/230 [7:18:58<5:08:35, 181.52s/it] 56%|█████▌    | 129/230 [7:24:40<6:26:30, 229.61s/it]                                                       56%|█████▌    | 129/230 [7:24:40<6:26:30, 229.61s/it] 57%|█████▋    | 130/230 [7:25:49<5:02:07, 181.27s/it]                                                       57%|█████▋    | 130/230 [7:25:49<5:02:07, 181.27s/it] 57%|█████▋    | 131/230 [7:31:30<6:18:24, 229.33s/it]                                                       57%|█████▋    | 131/230 [7:31:30<6:18:24, 229.33s/it] 57%|█████▋    | 132/230 [7:32:38<4:55:32, 180.94s/it]                                                       57%|█████▋    | 132/230 [7:32:38<4:55:32, 180.94s/it] 58%|█████▊    | 133/230 [7:38:19<6:10:15, 229.02s/it]                                                       58%|█████▊    | 133/230 [7:38:19<6:10:15, 229.02s/it] 58%|█████▊    | 134/230 [7:39:28<4:49:23, 180.87s/it]                                                       58%|█████▊    | 134/230 [7:39:28<4:49:23, 180.87s/it] 59%|█████▊    | 135/230 [7:45:08<6:01:47, 228.50s/it]                                                       59%|█████▊    | 135/230 [7:45:08<6:01:47, 228.50s/it] 59%|█████▉    | 136/230 [7:46:17<4:43:06, 180.71s/it]                                                       59%|█████▉    | 136/230 [7:46:17<4:43:06, 180.71s/it] 60%|█████▉    | 137/230 [7:51:59<5:55:24, 229.29s/it]                                                       60%|█████▉    | 137/230 [7:51:59<5:55:24, 229.29s/it] 60%|██████    | 138/230 [7:53:08<4:37:38, 181.07s/it]                                                       60%|██████    | 138/230 [7:53:08<4:37:38, 181.07s/it] 60%|██████    | 139/230 [7:58:48<5:46:55, 228.75s/it]                                                       60%|██████    | 139/230 [7:58:48<5:46:55, 228.75s/it] 61%|██████    | 140/230 [7:59:57<4:31:02, 180.69s/it]                                                       61%|██████    | 140/230 [7:59:57<4:31:02, 180.69s/it] 61%|██████▏   | 141/230 [8:05:38<5:39:32, 228.91s/it]                                                       61%|██████▏   | 141/230 [8:05:38<5:39:32, 228.91s/it] 62%|██████▏   | 142/230 [8:06:46<4:24:56, 180.64s/it]                                                       62%|██████▏   | 142/230 [8:06:46<4:24:56, 180.64s/it] 62%|██████▏   | 143/230 [8:12:28<5:32:11, 229.09s/it]                                                       62%|██████▏   | 143/230 [8:12:28<5:32:11, 229.09s/it] 63%|██████▎   | 144/230 [8:13:37<4:19:16, 180.89s/it]                                                       63%|██████▎   | 144/230 [8:13:37<4:19:16, 180.89s/it] 63%|██████▎   | 145/230 [8:19:17<5:24:13, 228.87s/it]                                                       63%|██████▎   | 145/230 [8:19:17<5:24:13, 228.87s/it] 63%|██████▎   | 146/230 [8:20:26<4:13:15, 180.90s/it]                                                       63%|██████▎   | 146/230 [8:20:26<4:13:15, 180.90s/it] 64%|██████▍   | 147/230 [8:26:08<5:17:01, 229.18s/it]                                                       64%|██████▍   | 147/230 [8:26:08<5:17:01, 229.18s/it] 64%|██████▍   | 148/230 [8:27:17<4:07:25, 181.05s/it]                                                       64%|██████▍   | 148/230 [8:27:17<4:07:25, 181.05s/it] 65%|██████▍   | 149/230 [8:32:59<5:09:50, 229.52s/it]                                                       65%|██████▍   | 149/230 [8:32:59<5:09:50, 229.52s/it] 65%|██████▌   | 150/230 [8:34:08<4:01:48, 181.36s/it]                                                       65%|██████▌   | 150/230 [8:34:09<4:01:48, 181.36s/it] 66%|██████▌   | 151/230 [8:39:51<5:02:21, 229.64s/it]                                                       66%|██████▌   | 151/230 [8:39:51<5:02:21, 229.64s/it] 66%|██████▌   | 152/230 [8:41:00<3:55:46, 181.37s/it]                                                       66%|██████▌   | 152/230 [8:41:00<3:55:46, 181.37s/it] 67%|██████▋   | 153/230 [8:46:39<4:53:37, 228.80s/it]                                                       67%|██████▋   | 153/230 [8:46:39<4:53:37, 228.80s/it] 67%|██████▋   | 154/230 [8:47:48<3:49:04, 180.85s/it]                                                       67%|██████▋   | 154/230 [8:47:48<3:49:04, 180.85s/it] 67%|██████▋   | 155/230 [8:53:30<4:46:28, 229.18s/it]                                                       67%|██████▋   | 155/230 [8:53:30<4:46:28, 229.18s/it] 68%|██████▊   | 156/230 [8:54:38<3:43:09, 180.94s/it]                                                       68%|██████▊   | 156/230 [8:54:38<3:43:09, 180.94s/it] 68%|██████▊   | 157/230 [9:00:21<4:39:06, 229.40s/it]                                                       68%|██████▊   | 157/230 [9:00:21<4:39:06, 229.40s/it] 69%|██████▊   | 158/230 [9:01:30<3:37:27, 181.21s/it]                                                       69%|██████▊   | 158/230 [9:01:30<3:37:27, 181.21s/it] 69%|██████▉   | 159/230 [9:07:13<4:31:55, 229.80s/it]                                                       69%|██████▉   | 159/230 [9:07:13<4:31:55, 229.80s/it] 70%|██████▉   | 160/230 [9:08:21<3:31:42, 181.47s/it]                                                       70%|██████▉   | 160/230 [9:08:21<3:31:42, 181.47s/it] 70%|███████   | 161/230 [9:14:04<4:24:25, 229.93s/it]                                                       70%|███████   | 161/230 [9:14:04<4:24:25, 229.93s/it] 70%|███████   | 162/230 [9:15:12<3:25:28, 181.31s/it]                                                       70%|███████   | 162/230 [9:15:12<3:25:28, 181.31s/it] 71%|███████   | 163/230 [9:20:53<4:15:44, 229.03s/it]                                                       71%|███████   | 163/230 [9:20:53<4:15:44, 229.03s/it] 71%|███████▏  | 164/230 [9:22:02<3:19:07, 181.02s/it]                                                       71%|███████▏  | 164/230 [9:22:02<3:19:07, 181.02s/it] 72%|███████▏  | 165/230 [9:27:43<4:08:13, 229.12s/it]                                                       72%|███████▏  | 165/230 [9:27:43<4:08:13, 229.12s/it] 72%|███████▏  | 166/230 [9:28:51<3:12:56, 180.89s/it]                                                       72%|███████▏  | 166/230 [9:28:51<3:12:56, 180.89s/it] 73%|███████▎  | 167/230 [9:34:34<4:00:50, 229.38s/it]                                                       73%|███████▎  | 167/230 [9:34:34<4:00:50, 229.38s/it] 73%|███████▎  | 168/230 [9:35:43<3:07:17, 181.25s/it]                                                       73%|███████▎  | 168/230 [9:35:43<3:07:17, 181.25s/it] 73%|███████▎  | 169/230 [9:41:23<3:52:35, 228.78s/it]                                                       73%|███████▎  | 169/230 [9:41:23<3:52:35, 228.78s/it] 74%|███████▍  | 170/230 [9:42:31<3:00:42, 180.70s/it]                                                       74%|███████▍  | 170/230 [9:42:31<3:00:42, 180.70s/it] 74%|███████▍  | 171/230 [9:48:14<3:45:37, 229.46s/it]                                                       74%|███████▍  | 171/230 [9:48:14<3:45:37, 229.46s/it] 75%|███████▍  | 172/230 [9:49:24<2:55:22, 181.42s/it]                                                       75%|███████▍  | 172/230 [9:49:24<2:55:22, 181.42s/it] 75%|███████▌  | 173/230 [9:55:06<3:38:08, 229.63s/it]                                                       75%|███████▌  | 173/230 [9:55:06<3:38:08, 229.63s/it] 76%|███████▌  | 174/230 [9:56:14<2:49:09, 181.25s/it]                                                       76%|███████▌  | 174/230 [9:56:14<2:49:09, 181.25s/it] 76%|███████▌  | 175/230 [10:01:56<3:30:16, 229.39s/it]                                                        76%|███████▌  | 175/230 [10:01:56<3:30:16, 229.39s/it] 77%|███████▋  | 176/230 [10:03:03<2:42:48, 180.89s/it]                                                        77%|███████▋  | 176/230 [10:03:03<2:42:48, 180.89s/it] 77%|███████▋  | 177/230 [10:08:45<3:22:24, 229.14s/it]                                                        77%|███████▋  | 177/230 [10:08:45<3:22:24, 229.14s/it] 77%|███████▋  | 178/230 [10:09:53<2:36:36, 180.71s/it]                                                        77%|███████▋  | 178/230 [10:09:53<2:36:36, 180.71s/it] 78%|███████▊  | 179/230 [10:15:33<3:14:13, 228.50s/it]                                                        78%|███████▊  | 179/230 [10:15:33<3:14:13, 228.50s/it] 78%|███████▊  | 180/230 [10:16:42<2:30:28, 180.56s/it]                                                        78%|███████▊  | 180/230 [10:16:42<2:30:28, 180.56s/it] 79%|███████▊  | 181/230 [10:22:23<3:06:53, 228.85s/it]                                                        79%|███████▊  | 181/230 [10:22:23<3:06:53, 228.85s/it] 79%|███████▉  | 182/230 [10:23:32<2:24:42, 180.89s/it]                                                        79%|███████▉  | 182/230 [10:23:32<2:24:42, 180.89s/it] 80%|███████▉  | 183/230 [10:29:16<3:00:01, 229.82s/it]                                                        80%|███████▉  | 183/230 [10:29:16<3:00:01, 229.82s/it] 80%|████████  | 184/230 [10:30:24<2:18:53, 181.16s/it]                                                        80%|████████  | 184/230 [10:30:24<2:18:53, 181.16s/it] 80%|████████  | 185/230 [10:36:07<2:52:14, 229.65s/it]                                                        80%|████████  | 185/230 [10:36:07<2:52:14, 229.65s/it] 81%|████████  | 186/230 [10:37:15<2:13:02, 181.42s/it]                                                        81%|████████  | 186/230 [10:37:15<2:13:02, 181.42s/it] 81%|████████▏ | 187/230 [10:42:57<2:44:24, 229.41s/it]                                                        81%|████████▏ | 187/230 [10:42:57<2:44:24, 229.41s/it] 82%|████████▏ | 188/230 [10:44:05<2:06:45, 181.09s/it]                                                        82%|████████▏ | 188/230 [10:44:05<2:06:45, 181.09s/it] 82%|████████▏ | 189/230 [10:49:47<2:36:39, 229.26s/it]                                                        82%|████████▏ | 189/230 [10:49:47<2:36:39, 229.26s/it] 83%|████████▎ | 190/230 [10:50:55<2:00:37, 180.93s/it]                                                        83%|████████▎ | 190/230 [10:50:55<2:00:37, 180.93s/it] 83%|████████▎ | 191/230 [10:56:37<2:29:00, 229.25s/it]                                                        83%|████████▎ | 191/230 [10:56:37<2:29:00, 229.25s/it] 83%|████████▎ | 192/230 [10:57:46<1:54:40, 181.06s/it]                                                        83%|████████▎ | 192/230 [10:57:46<1:54:40, 181.06s/it] 84%|████████▍ | 193/230 [11:03:27<2:21:18, 229.14s/it]                                                        84%|████████▍ | 193/230 [11:03:27<2:21:18, 229.14s/it] 84%|████████▍ | 194/230 [11:04:36<1:48:36, 181.01s/it]                                                        84%|████████▍ | 194/230 [11:04:36<1:48:36, 181.01s/it] 85%|████████▍ | 195/230 [11:10:18<2:13:46, 229.32s/it]                                                        85%|████████▍ | 195/230 [11:10:18<2:13:46, 229.32s/it] 85%|████████▌ | 196/230 [11:11:27<1:42:44, 181.30s/it]                                                        85%|████████▌ | 196/230 [11:11:27<1:42:44, 181.30s/it] 86%|████████▌ | 197/230 [11:17:09<2:06:12, 229.46s/it]                                                        86%|████████▌ | 197/230 [11:17:09<2:06:12, 229.46s/it] 86%|████████▌ | 198/230 [11:18:17<1:36:38, 181.21s/it]                                                        86%|████████▌ | 198/230 [11:18:17<1:36:38, 181.21s/it] 87%|████████▋ | 199/230 [11:23:58<1:58:15, 228.90s/it]                                                        87%|████████▋ | 199/230 [11:23:58<1:58:15, 228.90s/it] 87%|████████▋ | 200/230 [11:25:06<1:30:24, 180.81s/it]                                                        87%|████████▋ | 200/230 [11:25:06<1:30:24, 180.81s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 2

  0%|          | 0/4 [00:00<?, ?it/s][A
 50%|█████     | 2/4 [00:08<00:08,  4.34s/it][A
 75%|███████▌  | 3/4 [00:17<00:06,  6.27s/it][A
100%|██████████| 4/4 [00:26<00:00,  7.32s/it][A                                                       
                                             [A 87%|████████▋ | 200/230 [11:25:43<1:30:24, 180.81s/it]
100%|██████████| 4/4 [00:27<00:00,  7.32s/it][A
                                             [ASaving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-200
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-200/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-200/special_tokens_map.json
 87%|████████▋ | 201/230 [11:31:45<1:59:02, 246.30s/it]                                                        87%|████████▋ | 201/230 [11:31:45<1:59:02, 246.30s/it] 88%|████████▊ | 202/230 [11:32:54<1:30:03, 192.97s/it]                                                        88%|████████▊ | 202/230 [11:32:54<1:30:03, 192.97s/it] 88%|████████▊ | 203/230 [11:38:36<1:46:58, 237.72s/it]                                                        88%|████████▊ | 203/230 [11:38:36<1:46:58, 237.72s/it] 89%|████████▊ | 204/230 [11:39:45<1:21:07, 187.20s/it]                                                        89%|████████▊ | 204/230 [11:39:45<1:21:07, 187.20s/it] 89%|████████▉ | 205/230 [11:45:28<1:37:30, 234.00s/it]                                                        89%|████████▉ | 205/230 [11:45:28<1:37:30, 234.00s/it] 90%|████████▉ | 206/230 [11:46:38<1:13:48, 184.53s/it]                                                        90%|████████▉ | 206/230 [11:46:38<1:13:48, 184.53s/it] 90%|█████████ | 207/230 [11:52:21<1:29:01, 232.24s/it]                                                        90%|█████████ | 207/230 [11:52:21<1:29:01, 232.24s/it] 90%|█████████ | 208/230 [11:53:30<1:07:09, 183.15s/it]                                                        90%|█████████ | 208/230 [11:53:30<1:07:09, 183.15s/it] 91%|█████████ | 209/230 [11:59:12<1:20:50, 230.97s/it]                                                        91%|█████████ | 209/230 [11:59:12<1:20:50, 230.97s/it] 91%|█████████▏| 210/230 [12:00:22<1:00:49, 182.46s/it]                                                        91%|█████████▏| 210/230 [12:00:22<1:00:49, 182.46s/it] 92%|█████████▏| 211/230 [12:06:04<1:12:58, 230.47s/it]                                                        92%|█████████▏| 211/230 [12:06:04<1:12:58, 230.47s/it] 92%|█████████▏| 212/230 [12:07:12<54:32, 181.82s/it]                                                        92%|█████████▏| 212/230 [12:07:12<54:32, 181.82s/it] 93%|█████████▎| 213/230 [12:12:52<1:04:57, 229.25s/it]                                                        93%|█████████▎| 213/230 [12:12:52<1:04:57, 229.25s/it] 93%|█████████▎| 214/230 [12:14:00<48:13, 180.84s/it]                                                        93%|█████████▎| 214/230 [12:14:00<48:13, 180.84s/it] 93%|█████████▎| 215/230 [12:19:40<57:08, 228.53s/it]                                                      93%|█████████▎| 215/230 [12:19:40<57:08, 228.53s/it] 94%|█████████▍| 216/230 [12:20:48<42:04, 180.30s/it]                                                      94%|█████████▍| 216/230 [12:20:48<42:04, 180.30s/it] 94%|█████████▍| 217/230 [12:26:31<49:40, 229.26s/it]                                                      94%|█████████▍| 217/230 [12:26:31<49:40, 229.26s/it] 95%|█████████▍| 218/230 [12:27:41<36:15, 181.29s/it]                                                      95%|█████████▍| 218/230 [12:27:41<36:15, 181.29s/it] 95%|█████████▌| 219/230 [12:33:22<42:01, 229.20s/it]                                                      95%|█████████▌| 219/230 [12:33:22<42:01, 229.20s/it] 96%|█████████▌| 220/230 [12:34:30<30:10, 181.07s/it]                                                      96%|█████████▌| 220/230 [12:34:30<30:10, 181.07s/it] 96%|█████████▌| 221/230 [12:40:11<34:21, 229.04s/it]                                                      96%|█████████▌| 221/230 [12:40:11<34:21, 229.04s/it] 97%|█████████▋| 222/230 [12:41:20<24:07, 180.88s/it]                                                      97%|█████████▋| 222/230 [12:41:20<24:07, 180.88s/it] 97%|█████████▋| 223/230 [12:47:03<26:46, 229.50s/it]                                                      97%|█████████▋| 223/230 [12:47:03<26:46, 229.50s/it] 97%|█████████▋| 224/230 [12:48:12<18:07, 181.33s/it]                                                      97%|█████████▋| 224/230 [12:48:12<18:07, 181.33s/it] 98%|█████████▊| 225/230 [12:53:53<19:06, 229.23s/it]                                                      98%|█████████▊| 225/230 [12:53:53<19:06, 229.23s/it] 98%|█████████▊| 226/230 [12:55:01<12:03, 180.99s/it]                                                      98%|█████████▊| 226/230 [12:55:01<12:03, 180.99s/it] 99%|█████████▊| 227/230 [13:00:43<11:27, 229.20s/it]                                                      99%|█████████▊| 227/230 [13:00:43<11:27, 229.20s/it] 99%|█████████▉| 228/230 [13:01:52<06:02, 181.11s/it]                                                      99%|█████████▉| 228/230 [13:01:52<06:02, 181.11s/it]100%|█████████▉| 229/230 [13:07:35<03:49, 229.87s/it]                                                     100%|█████████▉| 229/230 [13:07:35<03:49, 229.87s/it]100%|██████████| 230/230 [13:08:44<00:00, 181.54s/it]                                                     100%|██████████| 230/230 [13:08:44<00:00, 181.54s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-230
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-230/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-230/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     100%|██████████| 230/230 [13:09:08<00:00, 181.54s/it]100%|██████████| 230/230 [13:09:08<00:00, 205.86s/it]
Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/special_tokens_map.json
tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/special_tokens_map.json
[rank6]:[W1202 04:31:40.534627630 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank5]:[W1202 04:31:41.282661445 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank4]:[W1202 04:31:41.707992107 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank2]:[W1202 04:31:41.122076026 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank1]:[W1202 04:31:42.686316283 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank3]:[W1202 04:31:42.929364430 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank7]:[W1202 04:31:43.162490097 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank0]:[W1202 04:31:47.525099853 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1202 04:31:54.983431082 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1202 04:31:57.357240068 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
run_train.sh: error reading input file: Stale file handle
