2025-12-01 16:28:53.144530: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:28:53.196438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:28:56.266438: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:07.937727: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:07.984895: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:10.144898: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.299102: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.321904: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.347450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:18.371525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:18.376790: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.387024: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.414109: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.425473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:18.434925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:18.463197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:18.469599: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.506761: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.518684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:18.536062: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:18.555640: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:18.584464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-01 16:29:20.844590: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:21.113006: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:21.157489: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:21.204687: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:21.312653: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:21.358208: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:21.485550: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 16:29:21.676609: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:23,  4.65s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:23,  4.70s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:23,  4.71s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:23,  4.70s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:23,  4.72s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:23,  4.73s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:24,  4.80s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:04<00:24,  4.90s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:21,  5.46s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:21,  5.46s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:21,  5.48s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:21,  5.49s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:21,  5.47s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:21,  5.49s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:22,  5.50s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:22,  5.55s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.68s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.69s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.72s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.71s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:16<00:17,  5.77s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.72s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.79s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.78s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.79s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.79s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.78s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.78s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:22<00:11,  5.80s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.75s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.76s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.79s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.81s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.82s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.80s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.80s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:28<00:05,  5.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  4.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.12s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  4.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.13s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  4.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.14s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  4.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  4.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  4.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.16s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:31<00:00,  4.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:31<00:00,  5.20s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V5.py:235: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 7. Using DeepSpeed's value.
***** Running training *****
  Num examples = 181
  Num Epochs = 120
  Instantaneous batch size per device = 3
  Total train batch size (w. parallel, distributed & accumulation) = 168
  Gradient Accumulation steps = 7
  Total optimization steps = 240
  Number of trainable parameters = 103,546,880
  0%|          | 0/240 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/240 [01:52<7:29:18, 112.80s/it]                                                    0%|          | 1/240 [01:52<7:29:18, 112.80s/it]  1%|          | 2/240 [02:05<3:34:20, 54.03s/it]                                                    1%|          | 2/240 [02:05<3:34:20, 54.03s/it]  1%|â–         | 3/240 [03:53<5:11:09, 78.77s/it]                                                   1%|â–         | 3/240 [03:53<5:11:09, 78.77s/it]  2%|â–         | 4/240 [04:10<3:33:27, 54.27s/it]                                                   2%|â–         | 4/240 [04:10<3:33:27, 54.27s/it]  2%|â–         | 5/240 [06:01<4:53:06, 74.84s/it]                                                   2%|â–         | 5/240 [06:01<4:53:06, 74.84s/it]  2%|â–Ž         | 6/240 [06:18<3:35:04, 55.15s/it]                                                   2%|â–Ž         | 6/240 [06:18<3:35:04, 55.15s/it]  3%|â–Ž         | 7/240 [08:11<4:47:02, 73.92s/it]                                                   3%|â–Ž         | 7/240 [08:11<4:47:02, 73.92s/it]  3%|â–Ž         | 8/240 [08:24<3:30:47, 54.52s/it]                                                   3%|â–Ž         | 8/240 [08:24<3:30:47, 54.52s/it]  4%|â–         | 9/240 [10:15<4:38:30, 72.34s/it]                                                   4%|â–         | 9/240 [10:15<4:38:30, 72.34s/it]  4%|â–         | 10/240 [10:32<3:31:52, 55.27s/it]                                                    4%|â–         | 10/240 [10:32<3:31:52, 55.27s/it]  5%|â–         | 11/240 [12:24<4:36:39, 72.49s/it]                                                    5%|â–         | 11/240 [12:24<4:36:39, 72.49s/it]  5%|â–Œ         | 12/240 [12:37<3:26:39, 54.39s/it]                                                    5%|â–Œ         | 12/240 [12:37<3:26:39, 54.39s/it]  5%|â–Œ         | 13/240 [14:24<4:26:21, 70.40s/it]                                                    5%|â–Œ         | 13/240 [14:24<4:26:21, 70.40s/it]  6%|â–Œ         | 14/240 [14:41<3:24:42, 54.35s/it]                                                    6%|â–Œ         | 14/240 [14:41<3:24:42, 54.35s/it]  6%|â–‹         | 15/240 [16:26<4:20:52, 69.57s/it]                                                    6%|â–‹         | 15/240 [16:26<4:20:52, 69.57s/it]  7%|â–‹         | 16/240 [16:39<3:16:05, 52.52s/it]                                                    7%|â–‹         | 16/240 [16:39<3:16:05, 52.52s/it]  7%|â–‹         | 17/240 [18:27<4:16:47, 69.09s/it]                                                    7%|â–‹         | 17/240 [18:27<4:16:47, 69.09s/it]  8%|â–Š         | 18/240 [18:44<3:17:24, 53.36s/it]                                                    8%|â–Š         | 18/240 [18:44<3:17:24, 53.36s/it]  8%|â–Š         | 19/240 [20:31<4:16:06, 69.53s/it]                                                    8%|â–Š         | 19/240 [20:31<4:16:06, 69.53s/it]  8%|â–Š         | 20/240 [20:48<3:16:46, 53.67s/it]                                                    8%|â–Š         | 20/240 [20:48<3:16:46, 53.67s/it]  9%|â–‰         | 21/240 [22:32<4:11:21, 68.86s/it]                                                    9%|â–‰         | 21/240 [22:32<4:11:21, 68.86s/it]  9%|â–‰         | 22/240 [22:48<3:13:13, 53.18s/it]                                                    9%|â–‰         | 22/240 [22:48<3:13:13, 53.18s/it] 10%|â–‰         | 23/240 [24:36<4:11:25, 69.52s/it]                                                   10%|â–‰         | 23/240 [24:36<4:11:25, 69.52s/it] 10%|â–ˆ         | 24/240 [24:53<3:13:27, 53.74s/it]                                                   10%|â–ˆ         | 24/240 [24:53<3:13:27, 53.74s/it] 10%|â–ˆ         | 25/240 [26:45<4:15:41, 71.36s/it]                                                   10%|â–ˆ         | 25/240 [26:45<4:15:41, 71.36s/it] 11%|â–ˆ         | 26/240 [27:03<3:16:35, 55.12s/it]                                                   11%|â–ˆ         | 26/240 [27:03<3:16:35, 55.12s/it] 11%|â–ˆâ–        | 27/240 [29:00<4:22:16, 73.88s/it]                                                   11%|â–ˆâ–        | 27/240 [29:00<4:22:16, 73.88s/it] 12%|â–ˆâ–        | 28/240 [29:13<3:16:30, 55.62s/it]                                                   12%|â–ˆâ–        | 28/240 [29:13<3:16:30, 55.62s/it] 12%|â–ˆâ–        | 29/240 [31:06<4:15:27, 72.64s/it]                                                   12%|â–ˆâ–        | 29/240 [31:06<4:15:27, 72.64s/it] 12%|â–ˆâ–Ž        | 30/240 [31:19<3:11:35, 54.74s/it]                                                   12%|â–ˆâ–Ž        | 30/240 [31:19<3:11:35, 54.74s/it] 13%|â–ˆâ–Ž        | 31/240 [33:07<4:06:52, 70.87s/it]                                                   13%|â–ˆâ–Ž        | 31/240 [33:07<4:06:52, 70.87s/it] 13%|â–ˆâ–Ž        | 32/240 [33:24<3:09:56, 54.79s/it]                                                   13%|â–ˆâ–Ž        | 32/240 [33:24<3:09:56, 54.79s/it] 14%|â–ˆâ–        | 33/240 [35:09<4:00:49, 69.81s/it]                                                   14%|â–ˆâ–        | 33/240 [35:09<4:00:49, 69.81s/it] 14%|â–ˆâ–        | 34/240 [35:26<3:05:26, 54.01s/it]                                                   14%|â–ˆâ–        | 34/240 [35:26<3:05:26, 54.01s/it] 15%|â–ˆâ–        | 35/240 [37:15<4:00:01, 70.25s/it]                                                   15%|â–ˆâ–        | 35/240 [37:15<4:00:01, 70.25s/it] 15%|â–ˆâ–Œ        | 36/240 [37:31<3:04:12, 54.18s/it]                                                   15%|â–ˆâ–Œ        | 36/240 [37:31<3:04:12, 54.18s/it] 15%|â–ˆâ–Œ        | 37/240 [39:23<4:01:27, 71.37s/it]                                                   15%|â–ˆâ–Œ        | 37/240 [39:23<4:01:27, 71.37s/it] 16%|â–ˆâ–Œ        | 38/240 [39:36<3:01:16, 53.84s/it]                                                   16%|â–ˆâ–Œ        | 38/240 [39:36<3:01:16, 53.84s/it] 16%|â–ˆâ–‹        | 39/240 [41:24<3:54:49, 70.10s/it]                                                   16%|â–ˆâ–‹        | 39/240 [41:24<3:54:49, 70.10s/it] 17%|â–ˆâ–‹        | 40/240 [41:40<3:00:15, 54.08s/it]                                                   17%|â–ˆâ–‹        | 40/240 [41:40<3:00:15, 54.08s/it] 17%|â–ˆâ–‹        | 41/240 [43:36<4:00:32, 72.52s/it]                                                   17%|â–ˆâ–‹        | 41/240 [43:36<4:00:32, 72.52s/it] 18%|â–ˆâ–Š        | 42/240 [43:49<3:00:21, 54.65s/it]                                                   18%|â–ˆâ–Š        | 42/240 [43:49<3:00:21, 54.65s/it] 18%|â–ˆâ–Š        | 43/240 [45:41<3:55:42, 71.79s/it]                                                   18%|â–ˆâ–Š        | 43/240 [45:41<3:55:42, 71.79s/it] 18%|â–ˆâ–Š        | 44/240 [45:58<3:01:24, 55.53s/it]                                                   18%|â–ˆâ–Š        | 44/240 [45:58<3:01:24, 55.53s/it] 19%|â–ˆâ–‰        | 45/240 [47:50<3:55:01, 72.32s/it]                                                   19%|â–ˆâ–‰        | 45/240 [47:50<3:55:01, 72.32s/it] 19%|â–ˆâ–‰        | 46/240 [48:07<3:00:21, 55.78s/it]                                                   19%|â–ˆâ–‰        | 46/240 [48:07<3:00:21, 55.78s/it] 20%|â–ˆâ–‰        | 47/240 [49:59<3:53:21, 72.55s/it]                                                   20%|â–ˆâ–‰        | 47/240 [49:59<3:53:21, 72.55s/it] 20%|â–ˆâ–ˆ        | 48/240 [50:16<2:59:18, 56.04s/it]                                                   20%|â–ˆâ–ˆ        | 48/240 [50:16<2:59:18, 56.04s/it] 20%|â–ˆâ–ˆ        | 49/240 [52:07<3:51:00, 72.57s/it]                                                   20%|â–ˆâ–ˆ        | 49/240 [52:07<3:51:00, 72.57s/it] 21%|â–ˆâ–ˆ        | 50/240 [52:25<2:57:16, 55.98s/it]                                                   21%|â–ˆâ–ˆ        | 50/240 [52:25<2:57:16, 55.98s/it] 21%|â–ˆâ–ˆâ–       | 51/240 [54:16<3:48:57, 72.69s/it]                                                   21%|â–ˆâ–ˆâ–       | 51/240 [54:16<3:48:57, 72.69s/it] 22%|â–ˆâ–ˆâ–       | 52/240 [54:33<2:55:27, 56.00s/it]                                                   22%|â–ˆâ–ˆâ–       | 52/240 [54:33<2:55:27, 56.00s/it] 22%|â–ˆâ–ˆâ–       | 53/240 [56:22<3:43:30, 71.71s/it]                                                   22%|â–ˆâ–ˆâ–       | 53/240 [56:22<3:43:30, 71.71s/it] 22%|â–ˆâ–ˆâ–Ž       | 54/240 [56:39<2:51:18, 55.26s/it]                                                   22%|â–ˆâ–ˆâ–Ž       | 54/240 [56:39<2:51:18, 55.26s/it] 23%|â–ˆâ–ˆâ–Ž       | 55/240 [58:26<3:38:32, 70.88s/it]                                                   23%|â–ˆâ–ˆâ–Ž       | 55/240 [58:26<3:38:32, 70.88s/it] 23%|â–ˆâ–ˆâ–Ž       | 56/240 [58:43<2:48:01, 54.79s/it]                                                   23%|â–ˆâ–ˆâ–Ž       | 56/240 [58:43<2:48:01, 54.79s/it] 24%|â–ˆâ–ˆâ–       | 57/240 [1:00:31<3:35:28, 70.65s/it]                                                     24%|â–ˆâ–ˆâ–       | 57/240 [1:00:31<3:35:28, 70.65s/it] 24%|â–ˆâ–ˆâ–       | 58/240 [1:00:48<2:45:46, 54.65s/it]                                                     24%|â–ˆâ–ˆâ–       | 58/240 [1:00:48<2:45:46, 54.65s/it] 25%|â–ˆâ–ˆâ–       | 59/240 [1:02:36<3:32:51, 70.56s/it]                                                     25%|â–ˆâ–ˆâ–       | 59/240 [1:02:36<3:32:51, 70.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 60/240 [1:02:53<2:43:32, 54.51s/it]                                                     25%|â–ˆâ–ˆâ–Œ       | 60/240 [1:02:53<2:43:32, 54.51s/it] 25%|â–ˆâ–ˆâ–Œ       | 61/240 [1:04:38<3:28:19, 69.83s/it]                                                     25%|â–ˆâ–ˆâ–Œ       | 61/240 [1:04:38<3:28:19, 69.83s/it] 26%|â–ˆâ–ˆâ–Œ       | 62/240 [1:04:55<2:39:51, 53.89s/it]                                                     26%|â–ˆâ–ˆâ–Œ       | 62/240 [1:04:55<2:39:51, 53.89s/it] 26%|â–ˆâ–ˆâ–‹       | 63/240 [1:06:36<3:20:37, 68.01s/it]                                                     26%|â–ˆâ–ˆâ–‹       | 63/240 [1:06:36<3:20:37, 68.01s/it] 27%|â–ˆâ–ˆâ–‹       | 64/240 [1:06:49<2:31:02, 51.49s/it]                                                     27%|â–ˆâ–ˆâ–‹       | 64/240 [1:06:49<2:31:02, 51.49s/it] 27%|â–ˆâ–ˆâ–‹       | 65/240 [1:08:38<3:20:13, 68.65s/it]                                                     27%|â–ˆâ–ˆâ–‹       | 65/240 [1:08:38<3:20:13, 68.65s/it] 28%|â–ˆâ–ˆâ–Š       | 66/240 [1:08:55<2:34:19, 53.21s/it]                                                     28%|â–ˆâ–ˆâ–Š       | 66/240 [1:08:55<2:34:19, 53.21s/it] 28%|â–ˆâ–ˆâ–Š       | 67/240 [1:10:50<3:27:22, 71.92s/it]                                                     28%|â–ˆâ–ˆâ–Š       | 67/240 [1:10:50<3:27:22, 71.92s/it] 28%|â–ˆâ–ˆâ–Š       | 68/240 [1:11:03<2:35:29, 54.24s/it]                                                     28%|â–ˆâ–ˆâ–Š       | 68/240 [1:11:03<2:35:29, 54.24s/it] 29%|â–ˆâ–ˆâ–‰       | 69/240 [1:12:56<3:24:44, 71.84s/it]                                                     29%|â–ˆâ–ˆâ–‰       | 69/240 [1:12:56<3:24:44, 71.84s/it] 29%|â–ˆâ–ˆâ–‰       | 70/240 [1:13:13<2:36:38, 55.29s/it]                                                     29%|â–ˆâ–ˆâ–‰       | 70/240 [1:13:13<2:36:38, 55.29s/it] 30%|â–ˆâ–ˆâ–‰       | 71/240 [1:15:01<3:20:31, 71.19s/it]                                                     30%|â–ˆâ–ˆâ–‰       | 71/240 [1:15:01<3:20:31, 71.19s/it] 30%|â–ˆâ–ˆâ–ˆ       | 72/240 [1:15:18<2:33:34, 54.85s/it]                                                     30%|â–ˆâ–ˆâ–ˆ       | 72/240 [1:15:18<2:33:34, 54.85s/it] 30%|â–ˆâ–ˆâ–ˆ       | 73/240 [1:17:07<3:17:36, 71.00s/it]                                                     30%|â–ˆâ–ˆâ–ˆ       | 73/240 [1:17:07<3:17:36, 71.00s/it] 31%|â–ˆâ–ˆâ–ˆ       | 74/240 [1:17:24<2:31:30, 54.76s/it]                                                     31%|â–ˆâ–ˆâ–ˆ       | 74/240 [1:17:24<2:31:30, 54.76s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 75/240 [1:19:12<3:14:40, 70.79s/it]                                                     31%|â–ˆâ–ˆâ–ˆâ–      | 75/240 [1:19:12<3:14:40, 70.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 76/240 [1:19:25<2:26:05, 53.45s/it]                                                     32%|â–ˆâ–ˆâ–ˆâ–      | 76/240 [1:19:25<2:26:05, 53.45s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 77/240 [1:21:10<3:07:41, 69.09s/it]                                                     32%|â–ˆâ–ˆâ–ˆâ–      | 77/240 [1:21:10<3:07:41, 69.09s/it] 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 78/240 [1:21:28<2:25:05, 53.74s/it]                                                     32%|â–ˆâ–ˆâ–ˆâ–Ž      | 78/240 [1:21:28<2:25:05, 53.74s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 79/240 [1:23:21<3:11:38, 71.42s/it]                                                     33%|â–ˆâ–ˆâ–ˆâ–Ž      | 79/240 [1:23:21<3:11:38, 71.42s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 80/240 [1:23:38<2:26:52, 55.08s/it]                                                     33%|â–ˆâ–ˆâ–ˆâ–Ž      | 80/240 [1:23:38<2:26:52, 55.08s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 81/240 [1:25:29<3:10:41, 71.96s/it]                                                     34%|â–ˆâ–ˆâ–ˆâ–      | 81/240 [1:25:29<3:10:41, 71.96s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 82/240 [1:25:46<2:26:11, 55.52s/it]                                                     34%|â–ˆâ–ˆâ–ˆâ–      | 82/240 [1:25:46<2:26:11, 55.52s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 83/240 [1:27:39<3:09:43, 72.51s/it]                                                     35%|â–ˆâ–ˆâ–ˆâ–      | 83/240 [1:27:39<3:09:43, 72.51s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 84/240 [1:27:55<2:25:08, 55.82s/it]                                                     35%|â–ˆâ–ˆâ–ˆâ–Œ      | 84/240 [1:27:55<2:25:08, 55.82s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [1:29:48<3:08:01, 72.78s/it]                                                     35%|â–ˆâ–ˆâ–ˆâ–Œ      | 85/240 [1:29:48<3:08:01, 72.78s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 86/240 [1:30:05<2:23:48, 56.03s/it]                                                     36%|â–ˆâ–ˆâ–ˆâ–Œ      | 86/240 [1:30:05<2:23:48, 56.03s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 87/240 [1:31:53<3:02:29, 71.57s/it]                                                     36%|â–ˆâ–ˆâ–ˆâ–‹      | 87/240 [1:31:53<3:02:29, 71.57s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 88/240 [1:32:09<2:19:35, 55.10s/it]                                                     37%|â–ˆâ–ˆâ–ˆâ–‹      | 88/240 [1:32:09<2:19:35, 55.10s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 89/240 [1:33:58<2:59:14, 71.22s/it]                                                     37%|â–ˆâ–ˆâ–ˆâ–‹      | 89/240 [1:33:58<2:59:14, 71.22s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 90/240 [1:34:15<2:17:36, 55.04s/it]                                                     38%|â–ˆâ–ˆâ–ˆâ–Š      | 90/240 [1:34:15<2:17:36, 55.04s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 91/240 [1:36:03<2:55:59, 70.87s/it]                                                     38%|â–ˆâ–ˆâ–ˆâ–Š      | 91/240 [1:36:03<2:55:59, 70.87s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 92/240 [1:36:20<2:15:00, 54.74s/it]                                                     38%|â–ˆâ–ˆâ–ˆâ–Š      | 92/240 [1:36:20<2:15:00, 54.74s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 93/240 [1:38:12<2:56:11, 71.92s/it]                                                     39%|â–ˆâ–ˆâ–ˆâ–‰      | 93/240 [1:38:12<2:56:11, 71.92s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 94/240 [1:38:29<2:15:04, 55.51s/it]                                                     39%|â–ˆâ–ˆâ–ˆâ–‰      | 94/240 [1:38:29<2:15:04, 55.51s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 95/240 [1:40:23<2:56:11, 72.91s/it]                                                     40%|â–ˆâ–ˆâ–ˆâ–‰      | 95/240 [1:40:23<2:56:11, 72.91s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 96/240 [1:40:40<2:14:38, 56.10s/it]                                                     40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 96/240 [1:40:40<2:14:38, 56.10s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 97/240 [1:42:28<2:51:13, 71.84s/it]                                                     40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 97/240 [1:42:28<2:51:13, 71.84s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 98/240 [1:42:46<2:11:15, 55.46s/it]                                                     41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 98/240 [1:42:46<2:11:15, 55.46s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/240 [1:44:30<2:45:02, 70.23s/it]                                                     41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 99/240 [1:44:30<2:45:02, 70.23s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/240 [1:44:48<2:06:55, 54.39s/it]                                                      42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/240 [1:44:48<2:06:55, 54.39s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 3

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.38s/it][A                                                     
                                             [A 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/240 [1:45:02<2:06:55, 54.39s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.38s/it][A
                                             [ASaving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/special_tokens_map.json
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 101/240 [1:47:23<3:15:43, 84.49s/it]                                                      42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 101/240 [1:47:23<3:15:43, 84.49s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 102/240 [1:47:40<2:27:46, 64.25s/it]                                                      42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 102/240 [1:47:40<2:27:46, 64.25s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/240 [1:49:28<2:56:47, 77.43s/it]                                                      43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 103/240 [1:49:28<2:56:47, 77.43s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 104/240 [1:49:41<2:11:41, 58.10s/it]                                                      43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 104/240 [1:49:41<2:11:41, 58.10s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/240 [1:51:34<2:47:39, 74.52s/it]                                                      44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 105/240 [1:51:34<2:47:39, 74.52s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/240 [1:51:51<2:08:02, 57.33s/it]                                                      44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 106/240 [1:51:51<2:08:02, 57.33s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 107/240 [1:53:43<2:43:31, 73.77s/it]                                                      45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 107/240 [1:53:43<2:43:31, 73.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 108/240 [1:53:56<2:02:10, 55.53s/it]                                                      45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 108/240 [1:53:56<2:02:10, 55.53s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 109/240 [1:55:47<2:37:51, 72.30s/it]                                                      45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 109/240 [1:55:47<2:37:51, 72.30s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 110/240 [1:56:05<2:01:01, 55.85s/it]                                                      46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 110/240 [1:56:05<2:01:01, 55.85s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 111/240 [1:57:51<2:32:15, 70.82s/it]                                                      46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 111/240 [1:57:51<2:32:15, 70.82s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 112/240 [1:58:08<1:56:46, 54.74s/it]                                                      47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 112/240 [1:58:08<1:56:46, 54.74s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 113/240 [2:00:04<2:35:10, 73.31s/it]                                                      47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 113/240 [2:00:04<2:35:10, 73.31s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 114/240 [2:00:21<1:58:23, 56.38s/it]                                                      48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 114/240 [2:00:21<1:58:23, 56.38s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/240 [2:02:13<2:32:10, 73.04s/it]                                                      48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 115/240 [2:02:13<2:32:10, 73.04s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 116/240 [2:02:26<1:53:41, 55.01s/it]                                                      48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 116/240 [2:02:26<1:53:41, 55.01s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 117/240 [2:04:18<2:27:38, 72.02s/it]                                                      49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 117/240 [2:04:18<2:27:38, 72.02s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 118/240 [2:04:35<1:52:43, 55.44s/it]                                                      49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 118/240 [2:04:35<1:52:43, 55.44s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 119/240 [2:06:20<2:21:45, 70.29s/it]                                                      50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 119/240 [2:06:20<2:21:45, 70.29s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 120/240 [2:06:37<1:48:37, 54.31s/it]                                                      50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 120/240 [2:06:37<1:48:37, 54.31s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/240 [2:08:21<2:17:33, 69.35s/it]                                                      50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 121/240 [2:08:21<2:17:33, 69.35s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 122/240 [2:08:38<1:45:40, 53.74s/it]                                                      51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 122/240 [2:08:38<1:45:40, 53.74s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 123/240 [2:10:27<2:16:46, 70.14s/it]                                                      51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 123/240 [2:10:27<2:16:46, 70.14s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 124/240 [2:10:44<1:44:43, 54.17s/it]                                                      52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 124/240 [2:10:44<1:44:43, 54.17s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 125/240 [2:12:36<2:17:10, 71.57s/it]                                                      52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 125/240 [2:12:36<2:17:10, 71.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 126/240 [2:12:53<1:44:49, 55.17s/it]                                                      52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 126/240 [2:12:53<1:44:49, 55.17s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/240 [2:14:46<2:16:59, 72.74s/it]                                                      53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 127/240 [2:14:46<2:16:59, 72.74s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 128/240 [2:15:04<1:44:43, 56.11s/it]                                                      53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 128/240 [2:15:04<1:44:43, 56.11s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/240 [2:16:48<2:10:38, 70.62s/it]                                                      54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 129/240 [2:16:48<2:10:38, 70.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/240 [2:17:05<1:40:07, 54.62s/it]                                                      54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 130/240 [2:17:05<1:40:07, 54.62s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 131/240 [2:18:58<2:10:42, 71.95s/it]                                                      55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 131/240 [2:18:58<2:10:42, 71.95s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 132/240 [2:19:15<1:39:54, 55.51s/it]                                                      55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 132/240 [2:19:15<1:39:54, 55.51s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 133/240 [2:21:07<2:09:16, 72.49s/it]                                                      55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 133/240 [2:21:07<2:09:16, 72.49s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 134/240 [2:21:24<1:38:28, 55.74s/it]                                                      56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 134/240 [2:21:24<1:38:28, 55.74s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 135/240 [2:23:08<2:03:02, 70.30s/it]                                                      56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 135/240 [2:23:08<2:03:02, 70.30s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 136/240 [2:23:21<1:32:03, 53.11s/it]                                                      57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 136/240 [2:23:21<1:32:03, 53.11s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 137/240 [2:25:08<1:59:03, 69.35s/it]                                                      57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 137/240 [2:25:08<1:59:03, 69.35s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 138/240 [2:25:26<1:31:19, 53.72s/it]                                                      57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 138/240 [2:25:26<1:31:19, 53.72s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 139/240 [2:27:13<1:57:37, 69.88s/it]                                                      58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 139/240 [2:27:13<1:57:37, 69.88s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 140/240 [2:27:30<1:30:04, 54.05s/it]                                                      58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 140/240 [2:27:30<1:30:04, 54.05s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 141/240 [2:29:23<1:58:14, 71.66s/it]                                                      59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 141/240 [2:29:23<1:58:14, 71.66s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 142/240 [2:29:36<1:28:15, 54.04s/it]                                                      59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 142/240 [2:29:36<1:28:15, 54.04s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 143/240 [2:31:32<1:57:15, 72.53s/it]                                                      60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 143/240 [2:31:32<1:57:15, 72.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 144/240 [2:31:49<1:29:27, 55.91s/it]                                                      60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 144/240 [2:31:49<1:29:27, 55.91s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 145/240 [2:33:33<1:51:40, 70.53s/it]                                                      60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 145/240 [2:33:33<1:51:40, 70.53s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 146/240 [2:33:47<1:23:39, 53.40s/it]                                                      61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 146/240 [2:33:47<1:23:39, 53.40s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 147/240 [2:35:34<1:47:59, 69.68s/it]                                                      61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 147/240 [2:35:34<1:47:59, 69.68s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/240 [2:35:52<1:23:00, 54.13s/it]                                                      62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 148/240 [2:35:52<1:23:00, 54.13s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 149/240 [2:37:44<1:48:21, 71.45s/it]                                                      62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 149/240 [2:37:44<1:48:21, 71.45s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/240 [2:38:01<1:22:38, 55.09s/it]                                                      62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 150/240 [2:38:01<1:22:38, 55.09s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 151/240 [2:39:49<1:45:21, 71.03s/it]                                                      63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 151/240 [2:39:49<1:45:21, 71.03s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 152/240 [2:40:07<1:20:29, 54.88s/it]                                                      63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 152/240 [2:40:07<1:20:29, 54.88s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 153/240 [2:41:56<1:43:07, 71.12s/it]                                                      64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 153/240 [2:41:56<1:43:07, 71.12s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 154/240 [2:42:09<1:16:55, 53.67s/it]                                                      64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 154/240 [2:42:09<1:16:55, 53.67s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/240 [2:43:56<1:39:04, 69.94s/it]                                                      65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 155/240 [2:43:56<1:39:04, 69.94s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 156/240 [2:44:14<1:15:47, 54.13s/it]                                                      65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 156/240 [2:44:14<1:15:47, 54.13s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 157/240 [2:46:03<1:37:35, 70.54s/it]                                                      65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 157/240 [2:46:03<1:37:35, 70.54s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 158/240 [2:46:19<1:14:25, 54.46s/it]                                                      66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 158/240 [2:46:19<1:14:25, 54.46s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 159/240 [2:48:08<1:35:21, 70.63s/it]                                                      66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 159/240 [2:48:08<1:35:21, 70.63s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 160/240 [2:48:25<1:12:43, 54.54s/it]                                                      67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 160/240 [2:48:25<1:12:43, 54.54s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 161/240 [2:50:10<1:31:59, 69.87s/it]                                                      67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 161/240 [2:50:10<1:31:59, 69.87s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 162/240 [2:50:27<1:10:01, 53.87s/it]                                                      68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 162/240 [2:50:27<1:10:01, 53.87s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 163/240 [2:52:19<1:31:40, 71.43s/it]                                                      68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 163/240 [2:52:19<1:31:40, 71.43s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 164/240 [2:52:37<1:09:52, 55.16s/it]                                                      68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 164/240 [2:52:37<1:09:52, 55.16s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/240 [2:54:31<1:31:12, 72.97s/it]                                                      69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/240 [2:54:31<1:31:12, 72.97s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 166/240 [2:54:48<1:09:17, 56.19s/it]                                                      69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 166/240 [2:54:48<1:09:17, 56.19s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 167/240 [2:56:36<1:27:17, 71.75s/it]                                                      70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 167/240 [2:56:36<1:27:17, 71.75s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 168/240 [2:56:53<1:06:14, 55.20s/it]                                                      70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 168/240 [2:56:53<1:06:14, 55.20s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/240 [2:58:45<1:25:23, 72.16s/it]                                                      70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 169/240 [2:58:45<1:25:23, 72.16s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 170/240 [2:59:02<1:04:57, 55.67s/it]                                                      71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 170/240 [2:59:02<1:04:57, 55.67s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/240 [3:00:50<1:22:08, 71.43s/it]                                                      71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 171/240 [3:00:50<1:22:08, 71.43s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 172/240 [3:01:07<1:02:20, 55.01s/it]                                                      72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 172/240 [3:01:07<1:02:20, 55.01s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 173/240 [3:02:59<1:20:31, 72.12s/it]                                                      72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 173/240 [3:02:59<1:20:31, 72.12s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 174/240 [3:03:12<59:48, 54.38s/it]                                                      72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 174/240 [3:03:12<59:48, 54.38s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 175/240 [3:04:59<1:16:09, 70.30s/it]                                                      73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 175/240 [3:04:59<1:16:09, 70.30s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 176/240 [3:05:16<57:50, 54.22s/it]                                                      73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 176/240 [3:05:16<57:50, 54.22s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 177/240 [3:07:04<1:13:57, 70.44s/it]                                                      74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 177/240 [3:07:04<1:13:57, 70.44s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 178/240 [3:07:21<56:18, 54.49s/it]                                                      74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 178/240 [3:07:21<56:18, 54.49s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 179/240 [3:09:14<1:13:05, 71.90s/it]                                                      75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 179/240 [3:09:14<1:13:05, 71.90s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 180/240 [3:09:27<54:13, 54.22s/it]                                                      75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 180/240 [3:09:27<54:13, 54.22s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 181/240 [3:11:15<1:09:20, 70.52s/it]                                                      75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 181/240 [3:11:15<1:09:20, 70.52s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 182/240 [3:11:32<52:32, 54.35s/it]                                                      76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 182/240 [3:11:32<52:32, 54.35s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 183/240 [3:13:24<1:07:59, 71.58s/it]                                                      76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 183/240 [3:13:24<1:07:59, 71.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 184/240 [3:13:41<51:30, 55.19s/it]                                                      77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 184/240 [3:13:41<51:30, 55.19s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 185/240 [3:15:29<1:05:10, 71.09s/it]                                                      77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 185/240 [3:15:29<1:05:10, 71.09s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 186/240 [3:15:46<49:23, 54.89s/it]                                                      78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 186/240 [3:15:46<49:23, 54.89s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 187/240 [3:17:37<1:03:27, 71.83s/it]                                                      78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 187/240 [3:17:37<1:03:27, 71.83s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 188/240 [3:17:55<48:03, 55.46s/it]                                                      78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 188/240 [3:17:55<48:03, 55.46s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 189/240 [3:19:43<1:00:39, 71.36s/it]                                                      79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 189/240 [3:19:43<1:00:39, 71.36s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 190/240 [3:19:56<44:52, 53.85s/it]                                                      79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 190/240 [3:19:56<44:52, 53.85s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 191/240 [3:21:45<57:22, 70.25s/it]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 191/240 [3:21:45<57:22, 70.25s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 192/240 [3:21:58<42:27, 53.06s/it]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 192/240 [3:21:58<42:27, 53.06s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 193/240 [3:23:50<55:34, 70.95s/it]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 193/240 [3:23:50<55:34, 70.95s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 194/240 [3:24:03<41:03, 53.56s/it]                                                    81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 194/240 [3:24:03<41:03, 53.56s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 195/240 [3:26:00<54:18, 72.41s/it]                                                    81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 195/240 [3:26:00<54:18, 72.41s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 196/240 [3:26:16<40:50, 55.69s/it]                                                    82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 196/240 [3:26:16<40:50, 55.69s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 197/240 [3:28:05<51:23, 71.71s/it]                                                    82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 197/240 [3:28:05<51:23, 71.71s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 198/240 [3:28:23<38:45, 55.38s/it]                                                    82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 198/240 [3:28:23<38:45, 55.38s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 199/240 [3:30:11<48:40, 71.24s/it]                                                    83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 199/240 [3:30:11<48:40, 71.24s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [3:30:28<36:34, 54.86s/it]                                                    83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [3:30:28<36:34, 54.86s/it]
***** Running Evaluation *****
  Num examples = 61
  Batch size = 3

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.37s/it][A                                                   
                                             [A 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 200/240 [3:30:42<36:34, 54.86s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.37s/it][A
                                             [ASaving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-200
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-200/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-200/special_tokens_map.json
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 201/240 [3:32:58<54:19, 83.57s/it]                                                    84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 201/240 [3:32:58<54:19, 83.57s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 202/240 [3:33:15<40:12, 63.48s/it]                                                    84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 202/240 [3:33:15<40:12, 63.48s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 203/240 [3:35:11<48:54, 79.30s/it]                                                    85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 203/240 [3:35:11<48:54, 79.30s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 204/240 [3:35:28<36:20, 60.56s/it]                                                    85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 204/240 [3:35:28<36:20, 60.56s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 205/240 [3:37:16<43:37, 74.79s/it]                                                    85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 205/240 [3:37:16<43:37, 74.79s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 206/240 [3:37:33<32:35, 57.52s/it]                                                    86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 206/240 [3:37:33<32:35, 57.52s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 207/240 [3:39:25<40:38, 73.90s/it]                                                    86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 207/240 [3:39:25<40:38, 73.90s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 208/240 [3:39:38<29:39, 55.61s/it]                                                    87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 208/240 [3:39:38<29:39, 55.61s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 209/240 [3:41:27<36:55, 71.48s/it]                                                    87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 209/240 [3:41:27<36:55, 71.48s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 210/240 [3:41:44<27:39, 55.31s/it]                                                    88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 210/240 [3:41:44<27:39, 55.31s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 211/240 [3:43:40<35:29, 73.43s/it]                                                    88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 211/240 [3:43:40<35:29, 73.43s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 212/240 [3:43:57<26:21, 56.50s/it]                                                    88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 212/240 [3:43:57<26:21, 56.50s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 213/240 [3:45:46<32:33, 72.34s/it]                                                    89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 213/240 [3:45:46<32:33, 72.34s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 214/240 [3:46:03<24:05, 55.61s/it]                                                    89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 214/240 [3:46:03<24:05, 55.61s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 215/240 [3:47:48<29:19, 70.38s/it]                                                    90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 215/240 [3:47:48<29:19, 70.38s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 216/240 [3:48:05<21:44, 54.36s/it]                                                    90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 216/240 [3:48:05<21:44, 54.36s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 217/240 [3:49:57<27:27, 71.65s/it]                                                    90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 217/240 [3:49:57<27:27, 71.65s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 218/240 [3:50:09<19:49, 54.05s/it]                                                    91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 218/240 [3:50:10<19:49, 54.05s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 219/240 [3:52:01<24:56, 71.28s/it]                                                    91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 219/240 [3:52:01<24:56, 71.28s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 220/240 [3:52:14<17:55, 53.79s/it]                                                    92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 220/240 [3:52:14<17:55, 53.79s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 221/240 [3:54:02<22:09, 69.97s/it]                                                    92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 221/240 [3:54:02<22:09, 69.97s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 222/240 [3:54:15<15:51, 52.88s/it]                                                    92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 222/240 [3:54:15<15:51, 52.88s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 223/240 [3:56:02<19:35, 69.18s/it]                                                    93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 223/240 [3:56:02<19:35, 69.18s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 224/240 [3:56:19<14:17, 53.61s/it]                                                    93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 224/240 [3:56:19<14:17, 53.61s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 225/240 [3:58:11<17:45, 71.05s/it]                                                    94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 225/240 [3:58:11<17:45, 71.05s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 226/240 [3:58:28<12:47, 54.81s/it]                                                    94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 226/240 [3:58:28<12:47, 54.81s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 227/240 [4:00:20<15:35, 71.97s/it]                                                    95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 227/240 [4:00:20<15:35, 71.97s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 228/240 [4:00:37<11:07, 55.62s/it]                                                    95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 228/240 [4:00:37<11:07, 55.62s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 229/240 [4:02:22<12:54, 70.37s/it]                                                    95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 229/240 [4:02:22<12:54, 70.37s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 230/240 [4:02:39<09:04, 54.41s/it]                                                    96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 230/240 [4:02:39<09:04, 54.41s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 231/240 [4:04:25<10:27, 69.68s/it]                                                    96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 231/240 [4:04:25<10:27, 69.68s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 232/240 [4:04:41<07:09, 53.73s/it]                                                    97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 232/240 [4:04:41<07:09, 53.73s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 233/240 [4:06:32<08:16, 70.99s/it]                                                    97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 233/240 [4:06:32<08:16, 70.99s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 234/240 [4:06:45<05:21, 53.59s/it]                                                    98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 234/240 [4:06:45<05:21, 53.59s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 235/240 [4:08:37<05:55, 71.01s/it]                                                    98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 235/240 [4:08:37<05:55, 71.01s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 236/240 [4:08:54<03:39, 54.87s/it]                                                    98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 236/240 [4:08:54<03:39, 54.87s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 237/240 [4:10:39<03:29, 69.79s/it]                                                    99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 237/240 [4:10:39<03:29, 69.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 238/240 [4:10:56<01:48, 54.02s/it]                                                    99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 238/240 [4:10:56<01:48, 54.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 239/240 [4:12:43<01:10, 70.02s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 239/240 [4:12:43<01:10, 70.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [4:13:00<00:00, 53.98s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [4:13:00<00:00, 53.98s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-240
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-240/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-240/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [4:13:31<00:00, 53.98s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [4:13:31<00:00, 63.38s/it]
Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/special_tokens_map.json
tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/special_tokens_map.json
[rank5]:[W1201 20:47:14.829558909 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank6]:[W1201 20:47:14.857306218 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank3]:[W1201 20:47:14.876167115 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank4]:[W1201 20:47:14.883952753 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank1]:[W1201 20:47:14.933771812 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank7]:[W1201 20:47:14.975732144 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank2]:[W1201 20:47:14.010397205 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank0]:[W1201 20:47:17.478205383 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1201 20:47:27.543496387 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1201 20:47:29.276909009 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
