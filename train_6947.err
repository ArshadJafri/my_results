/home/axs7716/my_model/Finetuning_V3_11-19-2025.py:23: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.

Please restructure your imports with 'import unsloth' at the top of your file.
  import unsloth
2025-11-27 11:43:43.122518: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-27 11:43:44.782534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-27 11:43:49.697380: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:11<00:58, 11.63s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:22<00:45, 11.44s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:34<00:34, 11.55s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:45<00:22, 11.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:56<00:11, 11.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:03<00:00,  9.85s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:03<00:00, 10.62s/it]
Unsloth 2025.11.3 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.
Unsloth: Already have LoRA adapters! We shall skip this step.
Using auto half precision backend
The following columns in the Training set don't have a corresponding argument in `DataParallel.forward` and have been ignored: attention_mask, input_ids. If attention_mask, input_ids are not expected by `DataParallel.forward`,  you can safely ignore this message.
Traceback (most recent call last):
  File "/home/axs7716/my_model/Finetuning_V3_11-19-2025.py", line 298, in <module>
    main(model_args, data_args, training_args)
  File "/home/axs7716/my_model/Finetuning_V3_11-19-2025.py", line 245, in main
    trainer.train(resume_from_checkpoint=None)
  File "/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/trl/trainer/sft_trainer.py", line 361, in train
    output = super().train(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 23, in _fast_inner_training_loop
  File "/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/trainer.py", line 1140, in get_train_dataloader
    return self._get_dataloader(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/trainer.py", line 1095, in _get_dataloader
    dataset = self._remove_unused_columns(dataset, description=description)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/trainer.py", line 1021, in _remove_unused_columns
    raise ValueError(
ValueError: No columns in the dataset match the model's forward method signature: (inputs, kwargs, label, label_ids). The following columns have been ignored: [attention_mask, input_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
