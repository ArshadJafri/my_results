2025-11-30 14:08:51.552552: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:08:51.605596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:08:54.712954: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:06.477390: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:06.527931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:08.694309: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:16.829660: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:16.832774: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:16.876197: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:16.881067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:16.884583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:16.929210: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:16.984096: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:17.009982: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:17.018047: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:17.035114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:17.062866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:17.067168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:17.165333: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:17.215240: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:19.445251: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:19.552835: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:19.668503: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:19.674204: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:19.753839: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:19.926081: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:21.989834: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:22.040939: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 14:09:24.670327: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 14:09:25.007736: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:18,  3.80s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:18,  3.79s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.83s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.85s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.87s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.85s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.92s/it]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.82s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.37s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.37s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.43s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.43s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.41s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.42s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.48s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.41s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.54s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.51s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.55s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.55s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.55s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.55s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:13<00:13,  4.61s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:18<00:09,  4.87s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:18<00:09,  4.86s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:14,  4.89s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:18<00:09,  4.89s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:18<00:09,  4.93s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:18<00:09,  4.93s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:18<00:09,  4.93s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:19<00:09,  4.99s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:24<00:05,  5.21s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:24<00:05,  5.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:19<00:10,  5.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:24<00:05,  5.22s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:24<00:05,  5.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:24<00:05,  5.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:24<00:05,  5.27s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:25<00:05,  5.32s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.32s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.51s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.52s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.52s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.34s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.55s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.39s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.60s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
Loading checkpoint shards:  83%|████████▎ | 5/6 [00:34<00:08,  8.51s/it]/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Loading checkpoint shards: 100%|██████████| 6/6 [00:38<00:00,  7.07s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:38<00:00,  6.42s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Using auto half precision backend
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
***** Running training *****
  Num examples = 181
  Num Epochs = 40
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 240
  Number of trainable parameters = 103,546,880
  0%|          | 0/240 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/240 [00:41<2:45:11, 41.47s/it]                                                   0%|          | 1/240 [00:41<2:45:11, 41.47s/it]  1%|          | 2/240 [01:21<2:40:32, 40.47s/it]                                                   1%|          | 2/240 [01:21<2:40:32, 40.47s/it]  1%|▏         | 3/240 [01:58<2:33:40, 38.90s/it]                                                   1%|▏         | 3/240 [01:58<2:33:40, 38.90s/it]  2%|▏         | 4/240 [02:38<2:34:35, 39.30s/it]                                                   2%|▏         | 4/240 [02:38<2:34:35, 39.30s/it]  2%|▏         | 5/240 [03:18<2:35:11, 39.62s/it]                                                   2%|▏         | 5/240 [03:18<2:35:11, 39.62s/it]  2%|▎         | 6/240 [03:48<2:22:03, 36.43s/it]                                                   2%|▎         | 6/240 [03:48<2:22:03, 36.43s/it]  3%|▎         | 7/240 [04:28<2:26:08, 37.63s/it]                                                   3%|▎         | 7/240 [04:28<2:26:08, 37.63s/it]  3%|▎         | 8/240 [05:08<2:28:20, 38.36s/it]                                                   3%|▎         | 8/240 [05:08<2:28:20, 38.36s/it]  4%|▍         | 9/240 [05:48<2:29:46, 38.90s/it]                                                   4%|▍         | 9/240 [05:48<2:29:46, 38.90s/it]  4%|▍         | 10/240 [06:28<2:30:11, 39.18s/it]                                                    4%|▍         | 10/240 [06:28<2:30:11, 39.18s/it]  5%|▍         | 11/240 [07:09<2:31:11, 39.61s/it]                                                    5%|▍         | 11/240 [07:09<2:31:11, 39.61s/it]  5%|▌         | 12/240 [07:39<2:19:28, 36.70s/it]                                                    5%|▌         | 12/240 [07:39<2:19:28, 36.70s/it]  5%|▌         | 13/240 [08:19<2:22:47, 37.74s/it]                                                    5%|▌         | 13/240 [08:19<2:22:47, 37.74s/it]  6%|▌         | 14/240 [08:59<2:24:22, 38.33s/it]                                                    6%|▌         | 14/240 [08:59<2:24:22, 38.33s/it]  6%|▋         | 15/240 [09:38<2:25:30, 38.80s/it]                                                    6%|▋         | 15/240 [09:38<2:25:30, 38.80s/it]  7%|▋         | 16/240 [10:19<2:26:31, 39.25s/it]                                                    7%|▋         | 16/240 [10:19<2:26:31, 39.25s/it]  7%|▋         | 17/240 [10:59<2:26:41, 39.47s/it]                                                    7%|▋         | 17/240 [10:59<2:26:41, 39.47s/it]  8%|▊         | 18/240 [11:29<2:15:38, 36.66s/it]                                                    8%|▊         | 18/240 [11:29<2:15:38, 36.66s/it]  8%|▊         | 19/240 [12:09<2:18:28, 37.60s/it]                                                    8%|▊         | 19/240 [12:09<2:18:28, 37.60s/it]  8%|▊         | 20/240 [12:49<2:20:37, 38.35s/it]                                                    8%|▊         | 20/240 [12:49<2:20:37, 38.35s/it]  9%|▉         | 21/240 [13:29<2:21:39, 38.81s/it]                                                    9%|▉         | 21/240 [13:29<2:21:39, 38.81s/it]  9%|▉         | 22/240 [14:09<2:22:18, 39.17s/it]                                                    9%|▉         | 22/240 [14:09<2:22:18, 39.17s/it] 10%|▉         | 23/240 [14:49<2:22:29, 39.40s/it]                                                   10%|▉         | 23/240 [14:49<2:22:29, 39.40s/it] 10%|█         | 24/240 [15:19<2:11:58, 36.66s/it]                                                   10%|█         | 24/240 [15:19<2:11:58, 36.66s/it] 10%|█         | 25/240 [15:59<2:15:07, 37.71s/it]                                                   10%|█         | 25/240 [15:59<2:15:07, 37.71s/it] 11%|█         | 26/240 [16:39<2:16:38, 38.31s/it]                                                   11%|█         | 26/240 [16:39<2:16:38, 38.31s/it] 11%|█▏        | 27/240 [17:18<2:17:02, 38.60s/it]                                                   11%|█▏        | 27/240 [17:18<2:17:02, 38.60s/it] 12%|█▏        | 28/240 [17:58<2:18:06, 39.09s/it]                                                   12%|█▏        | 28/240 [17:58<2:18:06, 39.09s/it] 12%|█▏        | 29/240 [18:38<2:18:33, 39.40s/it]                                                   12%|█▏        | 29/240 [18:38<2:18:33, 39.40s/it] 12%|█▎        | 30/240 [19:08<2:08:13, 36.64s/it]                                                   12%|█▎        | 30/240 [19:08<2:08:13, 36.64s/it] 13%|█▎        | 31/240 [19:48<2:11:09, 37.65s/it]                                                   13%|█▎        | 31/240 [19:48<2:11:09, 37.65s/it] 13%|█▎        | 32/240 [20:28<2:12:56, 38.35s/it]                                                   13%|█▎        | 32/240 [20:28<2:12:56, 38.35s/it] 14%|█▍        | 33/240 [21:08<2:13:59, 38.84s/it]                                                   14%|█▍        | 33/240 [21:08<2:13:59, 38.84s/it] 14%|█▍        | 34/240 [21:48<2:14:28, 39.17s/it]                                                   14%|█▍        | 34/240 [21:48<2:14:28, 39.17s/it] 15%|█▍        | 35/240 [22:28<2:14:32, 39.38s/it]                                                   15%|█▍        | 35/240 [22:28<2:14:32, 39.38s/it] 15%|█▌        | 36/240 [22:58<2:04:29, 36.62s/it]                                                   15%|█▌        | 36/240 [22:58<2:04:29, 36.62s/it] 15%|█▌        | 37/240 [23:39<2:07:40, 37.74s/it]                                                   15%|█▌        | 37/240 [23:39<2:07:40, 37.74s/it] 16%|█▌        | 38/240 [24:19<2:09:25, 38.44s/it]                                                   16%|█▌        | 38/240 [24:19<2:09:25, 38.44s/it] 16%|█▋        | 39/240 [24:59<2:10:11, 38.86s/it]                                                   16%|█▋        | 39/240 [24:59<2:10:11, 38.86s/it] 17%|█▋        | 40/240 [25:39<2:10:39, 39.20s/it]                                                   17%|█▋        | 40/240 [25:39<2:10:39, 39.20s/it] 17%|█▋        | 41/240 [26:19<2:10:50, 39.45s/it]                                                   17%|█▋        | 41/240 [26:19<2:10:50, 39.45s/it] 18%|█▊        | 42/240 [26:49<2:00:52, 36.63s/it]                                                   18%|█▊        | 42/240 [26:49<2:00:52, 36.63s/it] 18%|█▊        | 43/240 [27:29<2:03:40, 37.67s/it]                                                   18%|█▊        | 43/240 [27:29<2:03:40, 37.67s/it] 18%|█▊        | 44/240 [28:09<2:05:20, 38.37s/it]                                                   18%|█▊        | 44/240 [28:09<2:05:20, 38.37s/it] 19%|█▉        | 45/240 [28:49<2:06:33, 38.94s/it]                                                   19%|█▉        | 45/240 [28:49<2:06:33, 38.94s/it] 19%|█▉        | 46/240 [29:29<2:07:08, 39.32s/it]                                                   19%|█▉        | 46/240 [29:29<2:07:08, 39.32s/it] 20%|█▉        | 47/240 [30:09<2:07:12, 39.55s/it]                                                   20%|█▉        | 47/240 [30:09<2:07:12, 39.55s/it] 20%|██        | 48/240 [30:40<1:57:29, 36.72s/it]                                                   20%|██        | 48/240 [30:40<1:57:29, 36.72s/it] 20%|██        | 49/240 [31:20<2:00:11, 37.75s/it]                                                   20%|██        | 49/240 [31:20<2:00:11, 37.75s/it] 21%|██        | 50/240 [32:00<2:01:31, 38.38s/it]                                                   21%|██        | 50/240 [32:00<2:01:31, 38.38s/it] 21%|██▏       | 51/240 [32:40<2:02:47, 38.98s/it]                                                   21%|██▏       | 51/240 [32:40<2:02:47, 38.98s/it] 22%|██▏       | 52/240 [33:20<2:02:56, 39.23s/it]                                                   22%|██▏       | 52/240 [33:20<2:02:56, 39.23s/it] 22%|██▏       | 53/240 [33:59<2:02:29, 39.30s/it]                                                   22%|██▏       | 53/240 [33:59<2:02:29, 39.30s/it] 22%|██▎       | 54/240 [34:29<1:53:26, 36.59s/it]                                                   22%|██▎       | 54/240 [34:29<1:53:26, 36.59s/it] 23%|██▎       | 55/240 [35:10<1:56:03, 37.64s/it]                                                   23%|██▎       | 55/240 [35:10<1:56:03, 37.64s/it] 23%|██▎       | 56/240 [35:50<1:57:34, 38.34s/it]                                                   23%|██▎       | 56/240 [35:50<1:57:34, 38.34s/it] 24%|██▍       | 57/240 [36:30<1:58:48, 38.95s/it]                                                   24%|██▍       | 57/240 [36:30<1:58:48, 38.95s/it] 24%|██▍       | 58/240 [37:10<1:59:07, 39.27s/it]                                                   24%|██▍       | 58/240 [37:10<1:59:07, 39.27s/it] 25%|██▍       | 59/240 [37:50<1:59:07, 39.49s/it]                                                   25%|██▍       | 59/240 [37:50<1:59:07, 39.49s/it] 25%|██▌       | 60/240 [38:20<1:49:56, 36.64s/it]                                                   25%|██▌       | 60/240 [38:20<1:49:56, 36.64s/it] 25%|██▌       | 61/240 [39:00<1:52:13, 37.62s/it]                                                   25%|██▌       | 61/240 [39:00<1:52:13, 37.62s/it] 26%|██▌       | 62/240 [39:40<1:53:45, 38.34s/it]                                                   26%|██▌       | 62/240 [39:40<1:53:45, 38.34s/it] 26%|██▋       | 63/240 [40:21<1:55:21, 39.10s/it]                                                   26%|██▋       | 63/240 [40:21<1:55:21, 39.10s/it] 27%|██▋       | 64/240 [41:01<1:55:20, 39.32s/it]                                                   27%|██▋       | 64/240 [41:01<1:55:20, 39.32s/it] 27%|██▋       | 65/240 [41:41<1:55:15, 39.52s/it]                                                   27%|██▋       | 65/240 [41:41<1:55:15, 39.52s/it] 28%|██▊       | 66/240 [42:11<1:46:21, 36.67s/it]                                                   28%|██▊       | 66/240 [42:11<1:46:21, 36.67s/it] 28%|██▊       | 67/240 [42:51<1:48:46, 37.73s/it]                                                   28%|██▊       | 67/240 [42:51<1:48:46, 37.73s/it] 28%|██▊       | 68/240 [43:31<1:50:07, 38.42s/it]                                                   28%|██▊       | 68/240 [43:31<1:50:07, 38.42s/it] 29%|██▉       | 69/240 [44:11<1:50:39, 38.83s/it]                                                   29%|██▉       | 69/240 [44:11<1:50:39, 38.83s/it] 29%|██▉       | 70/240 [44:50<1:50:52, 39.13s/it]                                                   29%|██▉       | 70/240 [44:50<1:50:52, 39.13s/it] 30%|██▉       | 71/240 [45:30<1:50:59, 39.40s/it]                                                   30%|██▉       | 71/240 [45:30<1:50:59, 39.40s/it] 30%|███       | 72/240 [46:01<1:42:31, 36.62s/it]                                                   30%|███       | 72/240 [46:01<1:42:31, 36.62s/it] 30%|███       | 73/240 [46:41<1:44:44, 37.63s/it]                                                   30%|███       | 73/240 [46:41<1:44:44, 37.63s/it] 31%|███       | 74/240 [47:21<1:46:15, 38.41s/it]                                                   31%|███       | 74/240 [47:21<1:46:15, 38.41s/it] 31%|███▏      | 75/240 [48:00<1:46:34, 38.76s/it]                                                   31%|███▏      | 75/240 [48:00<1:46:34, 38.76s/it] 32%|███▏      | 76/240 [48:41<1:47:08, 39.20s/it]                                                   32%|███▏      | 76/240 [48:41<1:47:08, 39.20s/it] 32%|███▏      | 77/240 [49:20<1:47:02, 39.40s/it]                                                   32%|███▏      | 77/240 [49:20<1:47:02, 39.40s/it] 32%|███▎      | 78/240 [49:50<1:38:44, 36.57s/it]                                                   32%|███▎      | 78/240 [49:50<1:38:44, 36.57s/it] 33%|███▎      | 79/240 [50:30<1:40:36, 37.50s/it]                                                   33%|███▎      | 79/240 [50:30<1:40:36, 37.50s/it] 33%|███▎      | 80/240 [51:10<1:42:15, 38.35s/it]                                                   33%|███▎      | 80/240 [51:10<1:42:15, 38.35s/it] 34%|███▍      | 81/240 [51:51<1:43:37, 39.10s/it]                                                   34%|███▍      | 81/240 [51:51<1:43:37, 39.10s/it] 34%|███▍      | 82/240 [52:31<1:43:37, 39.35s/it]                                                   34%|███▍      | 82/240 [52:31<1:43:37, 39.35s/it] 35%|███▍      | 83/240 [53:11<1:43:29, 39.55s/it]                                                   35%|███▍      | 83/240 [53:11<1:43:29, 39.55s/it] 35%|███▌      | 84/240 [53:41<1:35:25, 36.70s/it]                                                   35%|███▌      | 84/240 [53:41<1:35:25, 36.70s/it] 35%|███▌      | 85/240 [54:21<1:37:08, 37.60s/it]                                                   35%|███▌      | 85/240 [54:21<1:37:08, 37.60s/it] 36%|███▌      | 86/240 [55:01<1:38:24, 38.34s/it]                                                   36%|███▌      | 86/240 [55:01<1:38:24, 38.34s/it] 36%|███▋      | 87/240 [55:41<1:39:07, 38.87s/it]                                                   36%|███▋      | 87/240 [55:41<1:39:07, 38.87s/it] 37%|███▋      | 88/240 [56:21<1:39:23, 39.23s/it]                                                   37%|███▋      | 88/240 [56:21<1:39:23, 39.23s/it] 37%|███▋      | 89/240 [57:01<1:39:17, 39.45s/it]                                                   37%|███▋      | 89/240 [57:01<1:39:17, 39.45s/it] 38%|███▊      | 90/240 [57:31<1:31:35, 36.63s/it]                                                   38%|███▊      | 90/240 [57:31<1:31:35, 36.63s/it] 38%|███▊      | 91/240 [58:11<1:33:20, 37.59s/it]                                                   38%|███▊      | 91/240 [58:11<1:33:20, 37.59s/it] 38%|███▊      | 92/240 [58:51<1:34:32, 38.33s/it]                                                   38%|███▊      | 92/240 [58:51<1:34:32, 38.33s/it] 39%|███▉      | 93/240 [59:31<1:35:16, 38.89s/it]                                                   39%|███▉      | 93/240 [59:31<1:35:16, 38.89s/it] 39%|███▉      | 94/240 [1:00:08<1:33:19, 38.35s/it]                                                     39%|███▉      | 94/240 [1:00:08<1:33:19, 38.35s/it] 40%|███▉      | 95/240 [1:00:49<1:33:59, 38.89s/it]                                                     40%|███▉      | 95/240 [1:00:49<1:33:59, 38.89s/it] 40%|████      | 96/240 [1:01:19<1:27:01, 36.26s/it]                                                     40%|████      | 96/240 [1:01:19<1:27:01, 36.26s/it] 40%|████      | 97/240 [1:02:00<1:29:40, 37.63s/it]                                                     40%|████      | 97/240 [1:02:00<1:29:40, 37.63s/it] 41%|████      | 98/240 [1:02:40<1:31:12, 38.54s/it]                                                     41%|████      | 98/240 [1:02:40<1:31:12, 38.54s/it] 41%|████▏     | 99/240 [1:03:20<1:31:33, 38.96s/it]                                                     41%|████▏     | 99/240 [1:03:20<1:31:33, 38.96s/it] 42%|████▏     | 100/240 [1:04:00<1:31:42, 39.30s/it]                                                      42%|████▏     | 100/240 [1:04:00<1:31:42, 39.30s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-100
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-100/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-100/special_tokens_map.json
 42%|████▏     | 101/240 [1:05:00<1:45:14, 45.43s/it]                                                      42%|████▏     | 101/240 [1:05:00<1:45:14, 45.43s/it] 42%|████▎     | 102/240 [1:05:30<1:33:53, 40.82s/it]                                                      42%|████▎     | 102/240 [1:05:30<1:33:53, 40.82s/it] 43%|████▎     | 103/240 [1:06:10<1:32:45, 40.62s/it]                                                      43%|████▎     | 103/240 [1:06:10<1:32:45, 40.62s/it] 43%|████▎     | 104/240 [1:06:50<1:31:42, 40.46s/it]                                                      43%|████▎     | 104/240 [1:06:50<1:31:42, 40.46s/it] 44%|████▍     | 105/240 [1:07:30<1:30:45, 40.34s/it]                                                      44%|████▍     | 105/240 [1:07:30<1:30:45, 40.34s/it] 44%|████▍     | 106/240 [1:08:10<1:29:53, 40.25s/it]                                                      44%|████▍     | 106/240 [1:08:10<1:29:53, 40.25s/it] 45%|████▍     | 107/240 [1:08:50<1:28:50, 40.08s/it]                                                      45%|████▍     | 107/240 [1:08:50<1:28:50, 40.08s/it] 45%|████▌     | 108/240 [1:09:20<1:21:19, 36.97s/it]                                                      45%|████▌     | 108/240 [1:09:20<1:21:19, 36.97s/it] 45%|████▌     | 109/240 [1:10:00<1:22:41, 37.87s/it]                                                      45%|████▌     | 109/240 [1:10:00<1:22:41, 37.87s/it] 46%|████▌     | 110/240 [1:10:40<1:23:26, 38.51s/it]                                                      46%|████▌     | 110/240 [1:10:40<1:23:26, 38.51s/it] 46%|████▋     | 111/240 [1:11:20<1:23:57, 39.05s/it]                                                      46%|████▋     | 111/240 [1:11:20<1:23:57, 39.05s/it] 47%|████▋     | 112/240 [1:12:00<1:23:39, 39.22s/it]                                                      47%|████▋     | 112/240 [1:12:00<1:23:39, 39.22s/it] 47%|████▋     | 113/240 [1:12:40<1:23:28, 39.43s/it]                                                      47%|████▋     | 113/240 [1:12:40<1:23:28, 39.43s/it] 48%|████▊     | 114/240 [1:13:11<1:17:29, 36.90s/it]                                                      48%|████▊     | 114/240 [1:13:11<1:17:29, 36.90s/it] 48%|████▊     | 115/240 [1:13:51<1:19:08, 37.98s/it]                                                      48%|████▊     | 115/240 [1:13:51<1:19:08, 37.98s/it] 48%|████▊     | 116/240 [1:14:31<1:19:44, 38.58s/it]                                                      48%|████▊     | 116/240 [1:14:31<1:19:44, 38.58s/it] 49%|████▉     | 117/240 [1:15:11<1:19:44, 38.90s/it]                                                      49%|████▉     | 117/240 [1:15:11<1:19:44, 38.90s/it] 49%|████▉     | 118/240 [1:15:51<1:19:43, 39.21s/it]                                                      49%|████▉     | 118/240 [1:15:51<1:19:43, 39.21s/it] 50%|████▉     | 119/240 [1:16:31<1:19:41, 39.52s/it]                                                      50%|████▉     | 119/240 [1:16:31<1:19:41, 39.52s/it] 50%|█████     | 120/240 [1:17:01<1:13:38, 36.82s/it]                                                      50%|█████     | 120/240 [1:17:01<1:13:38, 36.82s/it] 50%|█████     | 121/240 [1:17:41<1:14:54, 37.77s/it]                                                      50%|█████     | 121/240 [1:17:41<1:14:54, 37.77s/it] 51%|█████     | 122/240 [1:18:21<1:15:33, 38.42s/it]                                                      51%|█████     | 122/240 [1:18:21<1:15:33, 38.42s/it] 51%|█████▏    | 123/240 [1:19:02<1:15:57, 38.96s/it]                                                      51%|█████▏    | 123/240 [1:19:02<1:15:57, 38.96s/it] 52%|█████▏    | 124/240 [1:19:41<1:15:42, 39.16s/it]                                                      52%|█████▏    | 124/240 [1:19:41<1:15:42, 39.16s/it] 52%|█████▏    | 125/240 [1:20:21<1:15:31, 39.41s/it]                                                      52%|█████▏    | 125/240 [1:20:21<1:15:31, 39.41s/it] 52%|█████▎    | 126/240 [1:20:51<1:09:34, 36.62s/it]                                                      52%|█████▎    | 126/240 [1:20:51<1:09:34, 36.62s/it] 53%|█████▎    | 127/240 [1:21:31<1:10:53, 37.64s/it]                                                      53%|█████▎    | 127/240 [1:21:31<1:10:53, 37.64s/it] 53%|█████▎    | 128/240 [1:22:11<1:11:22, 38.24s/it]                                                      53%|█████▎    | 128/240 [1:22:11<1:11:22, 38.24s/it] 54%|█████▍    | 129/240 [1:22:51<1:11:42, 38.76s/it]                                                      54%|█████▍    | 129/240 [1:22:51<1:11:42, 38.76s/it] 54%|█████▍    | 130/240 [1:23:31<1:11:46, 39.15s/it]                                                      54%|█████▍    | 130/240 [1:23:31<1:11:46, 39.15s/it] 55%|█████▍    | 131/240 [1:24:11<1:11:49, 39.53s/it]                                                      55%|█████▍    | 131/240 [1:24:11<1:11:49, 39.53s/it] 55%|█████▌    | 132/240 [1:24:42<1:06:17, 36.83s/it]                                                      55%|█████▌    | 132/240 [1:24:42<1:06:17, 36.83s/it] 55%|█████▌    | 133/240 [1:25:22<1:07:25, 37.81s/it]                                                      55%|█████▌    | 133/240 [1:25:22<1:07:25, 37.81s/it] 56%|█████▌    | 134/240 [1:26:02<1:07:49, 38.39s/it]                                                      56%|█████▌    | 134/240 [1:26:02<1:07:49, 38.39s/it] 56%|█████▋    | 135/240 [1:26:42<1:07:58, 38.84s/it]                                                      56%|█████▋    | 135/240 [1:26:42<1:07:58, 38.84s/it] 57%|█████▋    | 136/240 [1:27:22<1:07:54, 39.18s/it]                                                      57%|█████▋    | 136/240 [1:27:22<1:07:54, 39.18s/it] 57%|█████▋    | 137/240 [1:28:02<1:07:54, 39.56s/it]                                                      57%|█████▋    | 137/240 [1:28:02<1:07:54, 39.56s/it] 57%|█████▊    | 138/240 [1:28:32<1:02:23, 36.70s/it]                                                      57%|█████▊    | 138/240 [1:28:32<1:02:23, 36.70s/it] 58%|█████▊    | 139/240 [1:29:12<1:03:24, 37.67s/it]                                                      58%|█████▊    | 139/240 [1:29:12<1:03:24, 37.67s/it] 58%|█████▊    | 140/240 [1:29:52<1:04:02, 38.42s/it]                                                      58%|█████▊    | 140/240 [1:29:52<1:04:02, 38.42s/it] 59%|█████▉    | 141/240 [1:30:32<1:04:08, 38.87s/it]                                                      59%|█████▉    | 141/240 [1:30:32<1:04:08, 38.87s/it] 59%|█████▉    | 142/240 [1:31:12<1:04:05, 39.24s/it]                                                      59%|█████▉    | 142/240 [1:31:12<1:04:05, 39.24s/it] 60%|█████▉    | 143/240 [1:31:52<1:03:42, 39.40s/it]                                                      60%|█████▉    | 143/240 [1:31:52<1:03:42, 39.40s/it] 60%|██████    | 144/240 [1:32:22<58:34, 36.61s/it]                                                      60%|██████    | 144/240 [1:32:22<58:34, 36.61s/it] 60%|██████    | 145/240 [1:33:02<59:26, 37.54s/it]                                                    60%|██████    | 145/240 [1:33:02<59:26, 37.54s/it] 61%|██████    | 146/240 [1:33:42<59:50, 38.19s/it]                                                    61%|██████    | 146/240 [1:33:42<59:50, 38.19s/it] 61%|██████▏   | 147/240 [1:34:22<1:00:02, 38.74s/it]                                                      61%|██████▏   | 147/240 [1:34:22<1:00:02, 38.74s/it] 62%|██████▏   | 148/240 [1:35:03<1:00:42, 39.60s/it]                                                      62%|██████▏   | 148/240 [1:35:03<1:00:42, 39.60s/it] 62%|██████▏   | 149/240 [1:35:44<1:00:32, 39.92s/it]                                                      62%|██████▏   | 149/240 [1:35:44<1:00:32, 39.92s/it] 62%|██████▎   | 150/240 [1:36:14<55:39, 37.11s/it]                                                      62%|██████▎   | 150/240 [1:36:14<55:39, 37.11s/it] 63%|██████▎   | 151/240 [1:36:54<56:17, 37.95s/it]                                                    63%|██████▎   | 151/240 [1:36:54<56:17, 37.95s/it] 63%|██████▎   | 152/240 [1:37:34<56:38, 38.62s/it]                                                    63%|██████▎   | 152/240 [1:37:34<56:38, 38.62s/it] 64%|██████▍   | 153/240 [1:38:14<56:36, 39.04s/it]                                                    64%|██████▍   | 153/240 [1:38:14<56:36, 39.04s/it] 64%|██████▍   | 154/240 [1:38:54<56:21, 39.31s/it]                                                    64%|██████▍   | 154/240 [1:38:54<56:21, 39.31s/it] 65%|██████▍   | 155/240 [1:39:31<54:41, 38.60s/it]                                                    65%|██████▍   | 155/240 [1:39:31<54:41, 38.60s/it] 65%|██████▌   | 156/240 [1:40:01<50:26, 36.03s/it]                                                    65%|██████▌   | 156/240 [1:40:01<50:26, 36.03s/it] 65%|██████▌   | 157/240 [1:40:41<51:27, 37.20s/it]                                                    65%|██████▌   | 157/240 [1:40:41<51:27, 37.20s/it] 66%|██████▌   | 158/240 [1:41:22<52:04, 38.11s/it]                                                    66%|██████▌   | 158/240 [1:41:22<52:04, 38.11s/it] 66%|██████▋   | 159/240 [1:42:01<52:10, 38.65s/it]                                                    66%|██████▋   | 159/240 [1:42:02<52:10, 38.65s/it] 67%|██████▋   | 160/240 [1:42:42<52:06, 39.08s/it]                                                    67%|██████▋   | 160/240 [1:42:42<52:06, 39.08s/it] 67%|██████▋   | 161/240 [1:43:21<51:45, 39.31s/it]                                                    67%|██████▋   | 161/240 [1:43:21<51:45, 39.31s/it] 68%|██████▊   | 162/240 [1:43:52<47:38, 36.65s/it]                                                    68%|██████▊   | 162/240 [1:43:52<47:38, 36.65s/it] 68%|██████▊   | 163/240 [1:44:32<48:13, 37.57s/it]                                                    68%|██████▊   | 163/240 [1:44:32<48:13, 37.57s/it] 68%|██████▊   | 164/240 [1:45:12<48:32, 38.33s/it]                                                    68%|██████▊   | 164/240 [1:45:12<48:32, 38.33s/it] 69%|██████▉   | 165/240 [1:45:52<48:35, 38.87s/it]                                                    69%|██████▉   | 165/240 [1:45:52<48:35, 38.87s/it] 69%|██████▉   | 166/240 [1:46:33<48:49, 39.59s/it]                                                    69%|██████▉   | 166/240 [1:46:33<48:49, 39.59s/it] 70%|██████▉   | 167/240 [1:47:13<48:22, 39.76s/it]                                                    70%|██████▉   | 167/240 [1:47:13<48:22, 39.76s/it] 70%|███████   | 168/240 [1:47:43<43:56, 36.62s/it]                                                    70%|███████   | 168/240 [1:47:43<43:56, 36.62s/it] 70%|███████   | 169/240 [1:48:23<44:36, 37.70s/it]                                                    70%|███████   | 169/240 [1:48:23<44:36, 37.70s/it] 71%|███████   | 170/240 [1:49:03<44:45, 38.36s/it]                                                    71%|███████   | 170/240 [1:49:03<44:45, 38.36s/it] 71%|███████▏  | 171/240 [1:49:43<44:45, 38.92s/it]                                                    71%|███████▏  | 171/240 [1:49:43<44:45, 38.92s/it] 72%|███████▏  | 172/240 [1:50:23<44:26, 39.21s/it]                                                    72%|███████▏  | 172/240 [1:50:23<44:26, 39.21s/it] 72%|███████▏  | 173/240 [1:51:03<43:59, 39.40s/it]                                                    72%|███████▏  | 173/240 [1:51:03<43:59, 39.40s/it] 72%|███████▎  | 174/240 [1:51:33<40:15, 36.61s/it]                                                    72%|███████▎  | 174/240 [1:51:33<40:15, 36.61s/it] 73%|███████▎  | 175/240 [1:52:13<40:49, 37.69s/it]                                                    73%|███████▎  | 175/240 [1:52:13<40:49, 37.69s/it] 73%|███████▎  | 176/240 [1:52:53<40:55, 38.36s/it]                                                    73%|███████▎  | 176/240 [1:52:53<40:55, 38.36s/it] 74%|███████▍  | 177/240 [1:53:33<40:45, 38.82s/it]                                                    74%|███████▍  | 177/240 [1:53:33<40:45, 38.82s/it] 74%|███████▍  | 178/240 [1:54:13<40:27, 39.16s/it]                                                    74%|███████▍  | 178/240 [1:54:13<40:27, 39.16s/it] 75%|███████▍  | 179/240 [1:54:53<40:09, 39.51s/it]                                                    75%|███████▍  | 179/240 [1:54:53<40:09, 39.51s/it] 75%|███████▌  | 180/240 [1:55:20<35:52, 35.87s/it]                                                    75%|███████▌  | 180/240 [1:55:20<35:52, 35.87s/it] 75%|███████▌  | 181/240 [1:56:01<36:33, 37.18s/it]                                                    75%|███████▌  | 181/240 [1:56:01<36:33, 37.18s/it] 76%|███████▌  | 182/240 [1:56:40<36:40, 37.94s/it]                                                    76%|███████▌  | 182/240 [1:56:40<36:40, 37.94s/it] 76%|███████▋  | 183/240 [1:57:20<36:35, 38.52s/it]                                                    76%|███████▋  | 183/240 [1:57:20<36:35, 38.52s/it] 77%|███████▋  | 184/240 [1:58:01<36:39, 39.28s/it]                                                    77%|███████▋  | 184/240 [1:58:01<36:39, 39.28s/it] 77%|███████▋  | 185/240 [1:58:41<36:13, 39.52s/it]                                                    77%|███████▋  | 185/240 [1:58:41<36:13, 39.52s/it] 78%|███████▊  | 186/240 [1:59:11<33:01, 36.69s/it]                                                    78%|███████▊  | 186/240 [1:59:11<33:01, 36.69s/it] 78%|███████▊  | 187/240 [1:59:51<33:15, 37.65s/it]                                                    78%|███████▊  | 187/240 [1:59:51<33:15, 37.65s/it] 78%|███████▊  | 188/240 [2:00:32<33:17, 38.42s/it]                                                    78%|███████▊  | 188/240 [2:00:32<33:17, 38.42s/it] 79%|███████▉  | 189/240 [2:01:12<33:17, 39.16s/it]                                                    79%|███████▉  | 189/240 [2:01:12<33:17, 39.16s/it] 79%|███████▉  | 190/240 [2:01:52<32:40, 39.22s/it]                                                    79%|███████▉  | 190/240 [2:01:52<32:40, 39.22s/it] 80%|███████▉  | 191/240 [2:02:32<32:10, 39.40s/it]                                                    80%|███████▉  | 191/240 [2:02:32<32:10, 39.40s/it] 80%|████████  | 192/240 [2:03:02<29:18, 36.63s/it]                                                    80%|████████  | 192/240 [2:03:02<29:18, 36.63s/it] 80%|████████  | 193/240 [2:03:42<29:33, 37.73s/it]                                                    80%|████████  | 193/240 [2:03:42<29:33, 37.73s/it] 81%|████████  | 194/240 [2:04:22<29:31, 38.50s/it]                                                    81%|████████  | 194/240 [2:04:22<29:31, 38.50s/it] 81%|████████▏ | 195/240 [2:05:02<29:13, 38.96s/it]                                                    81%|████████▏ | 195/240 [2:05:02<29:13, 38.96s/it] 82%|████████▏ | 196/240 [2:05:42<28:48, 39.27s/it]                                                    82%|████████▏ | 196/240 [2:05:42<28:48, 39.27s/it] 82%|████████▏ | 197/240 [2:06:22<28:16, 39.45s/it]                                                    82%|████████▏ | 197/240 [2:06:22<28:16, 39.45s/it] 82%|████████▎ | 198/240 [2:06:52<25:38, 36.62s/it]                                                    82%|████████▎ | 198/240 [2:06:52<25:38, 36.62s/it] 83%|████████▎ | 199/240 [2:07:32<25:44, 37.68s/it]                                                    83%|████████▎ | 199/240 [2:07:32<25:44, 37.68s/it] 83%|████████▎ | 200/240 [2:08:12<25:26, 38.16s/it]                                                    83%|████████▎ | 200/240 [2:08:12<25:26, 38.16s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-200
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-200/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-200/special_tokens_map.json
 84%|████████▍ | 201/240 [2:09:16<29:52, 45.96s/it]                                                    84%|████████▍ | 201/240 [2:09:16<29:52, 45.96s/it] 84%|████████▍ | 202/240 [2:09:56<27:56, 44.13s/it]                                                    84%|████████▍ | 202/240 [2:09:56<27:56, 44.13s/it] 85%|████████▍ | 203/240 [2:10:35<26:23, 42.81s/it]                                                    85%|████████▍ | 203/240 [2:10:35<26:23, 42.81s/it] 85%|████████▌ | 204/240 [2:11:06<23:24, 39.01s/it]                                                    85%|████████▌ | 204/240 [2:11:06<23:24, 39.01s/it] 85%|████████▌ | 205/240 [2:11:46<22:55, 39.29s/it]                                                    85%|████████▌ | 205/240 [2:11:46<22:55, 39.29s/it] 86%|████████▌ | 206/240 [2:12:29<22:53, 40.39s/it]                                                    86%|████████▌ | 206/240 [2:12:29<22:53, 40.39s/it] 86%|████████▋ | 207/240 [2:13:09<22:10, 40.33s/it]                                                    86%|████████▋ | 207/240 [2:13:09<22:10, 40.33s/it] 87%|████████▋ | 208/240 [2:13:49<21:26, 40.21s/it]                                                    87%|████████▋ | 208/240 [2:13:49<21:26, 40.21s/it] 87%|████████▋ | 209/240 [2:14:29<20:43, 40.12s/it]                                                    87%|████████▋ | 209/240 [2:14:29<20:43, 40.12s/it] 88%|████████▊ | 210/240 [2:14:59<18:32, 37.08s/it]                                                    88%|████████▊ | 210/240 [2:14:59<18:32, 37.08s/it] 88%|████████▊ | 211/240 [2:15:39<18:20, 37.95s/it]                                                    88%|████████▊ | 211/240 [2:15:39<18:20, 37.95s/it] 88%|████████▊ | 212/240 [2:16:19<18:00, 38.58s/it]                                                    88%|████████▊ | 212/240 [2:16:19<18:00, 38.58s/it] 89%|████████▉ | 213/240 [2:16:59<17:32, 38.99s/it]                                                    89%|████████▉ | 213/240 [2:16:59<17:32, 38.99s/it] 89%|████████▉ | 214/240 [2:17:39<17:01, 39.30s/it]                                                    89%|████████▉ | 214/240 [2:17:39<17:01, 39.30s/it] 90%|████████▉ | 215/240 [2:18:18<16:26, 39.44s/it]                                                    90%|████████▉ | 215/240 [2:18:18<16:26, 39.44s/it] 90%|█████████ | 216/240 [2:18:48<14:38, 36.61s/it]                                                    90%|█████████ | 216/240 [2:18:48<14:38, 36.61s/it] 90%|█████████ | 217/240 [2:19:28<14:25, 37.64s/it]                                                    90%|█████████ | 217/240 [2:19:28<14:25, 37.64s/it] 91%|█████████ | 218/240 [2:20:09<14:06, 38.47s/it]                                                    91%|█████████ | 218/240 [2:20:09<14:06, 38.47s/it] 91%|█████████▏| 219/240 [2:20:49<13:40, 39.06s/it]                                                    91%|█████████▏| 219/240 [2:20:49<13:40, 39.06s/it] 92%|█████████▏| 220/240 [2:21:29<13:06, 39.34s/it]                                                    92%|█████████▏| 220/240 [2:21:29<13:06, 39.34s/it] 92%|█████████▏| 221/240 [2:22:10<12:33, 39.64s/it]                                                    92%|█████████▏| 221/240 [2:22:10<12:33, 39.64s/it] 92%|█████████▎| 222/240 [2:22:40<11:02, 36.79s/it]                                                    92%|█████████▎| 222/240 [2:22:40<11:02, 36.79s/it] 93%|█████████▎| 223/240 [2:23:19<10:40, 37.69s/it]                                                    93%|█████████▎| 223/240 [2:23:19<10:40, 37.69s/it] 93%|█████████▎| 224/240 [2:23:59<10:12, 38.31s/it]                                                    93%|█████████▎| 224/240 [2:23:59<10:12, 38.31s/it] 94%|█████████▍| 225/240 [2:24:39<09:42, 38.85s/it]                                                    94%|█████████▍| 225/240 [2:24:39<09:42, 38.85s/it] 94%|█████████▍| 226/240 [2:25:19<09:08, 39.19s/it]                                                    94%|█████████▍| 226/240 [2:25:19<09:08, 39.19s/it] 95%|█████████▍| 227/240 [2:25:59<08:32, 39.45s/it]                                                    95%|█████████▍| 227/240 [2:25:59<08:32, 39.45s/it] 95%|█████████▌| 228/240 [2:26:30<07:19, 36.66s/it]                                                    95%|█████████▌| 228/240 [2:26:30<07:19, 36.66s/it] 95%|█████████▌| 229/240 [2:27:09<06:53, 37.63s/it]                                                    95%|█████████▌| 229/240 [2:27:09<06:53, 37.63s/it] 96%|█████████▌| 230/240 [2:27:49<06:23, 38.31s/it]                                                    96%|█████████▌| 230/240 [2:27:49<06:23, 38.31s/it] 96%|█████████▋| 231/240 [2:28:29<05:48, 38.72s/it]                                                    96%|█████████▋| 231/240 [2:28:29<05:48, 38.72s/it] 97%|█████████▋| 232/240 [2:29:09<05:12, 39.11s/it]                                                    97%|█████████▋| 232/240 [2:29:09<05:12, 39.11s/it] 97%|█████████▋| 233/240 [2:29:49<04:35, 39.42s/it]                                                    97%|█████████▋| 233/240 [2:29:49<04:35, 39.42s/it] 98%|█████████▊| 234/240 [2:30:19<03:39, 36.62s/it]                                                    98%|█████████▊| 234/240 [2:30:19<03:39, 36.62s/it] 98%|█████████▊| 235/240 [2:31:00<03:09, 37.93s/it]                                                    98%|█████████▊| 235/240 [2:31:00<03:09, 37.93s/it] 98%|█████████▊| 236/240 [2:31:40<02:34, 38.53s/it]                                                    98%|█████████▊| 236/240 [2:31:40<02:34, 38.53s/it] 99%|█████████▉| 237/240 [2:32:20<01:56, 38.99s/it]                                                    99%|█████████▉| 237/240 [2:32:20<01:56, 38.99s/it] 99%|█████████▉| 238/240 [2:33:00<01:18, 39.24s/it]                                                    99%|█████████▉| 238/240 [2:33:00<01:18, 39.24s/it]100%|█████████▉| 239/240 [2:33:40<00:39, 39.48s/it]                                                   100%|█████████▉| 239/240 [2:33:40<00:39, 39.48s/it]100%|██████████| 240/240 [2:34:10<00:00, 36.66s/it]                                                   100%|██████████| 240/240 [2:34:10<00:00, 36.66s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-240
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-240/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/checkpoint-240/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 240/240 [2:34:36<00:00, 36.66s/it]100%|██████████| 240/240 [2:34:36<00:00, 38.65s/it]
Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/special_tokens_map.json
tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_8870_1_1_4_0.01_40/special_tokens_map.json
[rank4]:[W1130 16:48:18.193704311 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank7]:[W1130 16:48:18.251663193 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank1]:[W1130 16:48:18.274020153 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank6]:[W1130 16:48:18.286462566 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank3]:[W1130 16:48:18.300691584 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank5]:[W1130 16:48:18.318805834 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank2]:[W1130 16:48:18.320666330 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank0]:[W1130 16:48:20.634217976 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1130 16:48:30.486415154 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1130 16:48:32.336870786 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
run_train.sh: error reading input file: Stale file handle
