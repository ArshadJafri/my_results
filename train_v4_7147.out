Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-12-01 16:28:59,730] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-12-01 16:28:59,730] [INFO] [runner.py:630:main] cmd = /home/axs7716/anaconda3/envs/arsh_env/bin/python3.12 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --log_level=info Finetuning_V5.py 0.0001 4092 3 3 7 0.01 120
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-12-01 16:29:12,964] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-12-01 16:29:12,964] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-12-01 16:29:12,964] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-12-01 16:29:12,965] [INFO] [launch.py:180:main] dist_world_size=8
[2025-12-01 16:29:12,965] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-12-01 16:29:12,966] [INFO] [launch.py:272:main] process 2718858 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=0', '0.0001', '4092', '3', '3', '7', '0.01', '120']
[2025-12-01 16:29:12,967] [INFO] [launch.py:272:main] process 2718859 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=1', '0.0001', '4092', '3', '3', '7', '0.01', '120']
[2025-12-01 16:29:12,968] [INFO] [launch.py:272:main] process 2718860 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=2', '0.0001', '4092', '3', '3', '7', '0.01', '120']
[2025-12-01 16:29:12,969] [INFO] [launch.py:272:main] process 2718861 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=3', '0.0001', '4092', '3', '3', '7', '0.01', '120']
[2025-12-01 16:29:12,970] [INFO] [launch.py:272:main] process 2718862 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=4', '0.0001', '4092', '3', '3', '7', '0.01', '120']
[2025-12-01 16:29:12,971] [INFO] [launch.py:272:main] process 2718863 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=5', '0.0001', '4092', '3', '3', '7', '0.01', '120']
[2025-12-01 16:29:12,972] [INFO] [launch.py:272:main] process 2718864 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=6', '0.0001', '4092', '3', '3', '7', '0.01', '120']
[2025-12-01 16:29:12,973] [INFO] [launch.py:272:main] process 2718865 spawned with command: ['/home/axs7716/anaconda3/envs/arsh_env/bin/python3.12', '-u', 'Finetuning_V5.py', '--local_rank=7', '0.0001', '4092', '3', '3', '7', '0.01', '120']
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/axs7716/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

============================================================
ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERS
Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter
Effective batch size (per GPU): 21
Epochs: 120
Learning rate: 0.0001
============================================================


============================================================
ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERS
Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter
Effective batch size (per GPU): 21
Epochs: 120
Learning rate: 0.0001
============================================================


============================================================
ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERS
Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter
Effective batch size (per GPU): 21
Epochs: 120
Learning rate: 0.0001
============================================================


============================================================
ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERS
Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter
Effective batch size (per GPU): 21
Epochs: 120
Learning rate: 0.0001
============================================================


============================================================
ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERS
Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter

============================================================Effective batch size (per GPU): 21

ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERSEpochs: 120

Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter
Learning rate: 0.0001Effective batch size (per GPU): 21

============================================================
Epochs: 120

Learning rate: 0.0001
============================================================


============================================================
ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERS
Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter
Effective batch size (per GPU): 21
Epochs: 120
Learning rate: 0.0001
============================================================


============================================================
ðŸš€ STARTING TRAINING WITH ORIGINAL CENTAUR PARAMETERS
Base model: marcelbinz/Llama-3.1-Centaur-70B-adapter
Effective batch size (per GPU): 21
Epochs: 120
Learning rate: 0.0001
============================================================

Loading tokenizer...
Loading tokenizer...
Loading tokenizer...
Loading tokenizer...
Loading tokenizer...
Loading tokenizer...
Loading tokenizer...
Loading tokenizer...
Tokenizing train dataset...
Tokenizing train dataset...
Tokenizing train dataset...
Tokenizing train dataset...
Tokenizing train dataset...
Tokenizing train dataset...
Tokenizing train dataset...
Tokenizing train dataset...
Tokenizing eval dataset...
Tokenizing eval dataset...
Tokenizing eval dataset...
Tokenizing eval dataset...
Tokenizing eval dataset...
Tokenizing eval dataset...
Tokenizing eval dataset...
Tokenizing eval dataset...
Loading model...
Loading model...
Loading model...
Loading model...
Loading model...
Loading model...
Loading model...
Loading model...
Preparing model for k-bit training...
Configuring LoRA...
Preparing model for k-bit training...
Configuring LoRA...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
Preparing model for k-bit training...
Preparing model for k-bit training...
Configuring LoRA...
Configuring LoRA...
Preparing model for k-bit training...
Configuring LoRA...
Preparing model for k-bit training...
Preparing model for k-bit training...
Preparing model for k-bit training...
Configuring LoRA...
Configuring LoRA...
Configuring LoRA...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((8192,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)
    )
  )
)
trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465

ðŸš€ Starting training...
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Parameter Offload - Persistent parameters statistics: param_count = 1041, numel = 49815552
[2025-12-01 16:33:10,140] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-01 16:33:10,141] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-01 16:33:10,141] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-01 16:33:10,141] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-01 16:33:10,142] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-01 16:33:10,142] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-01 16:33:10,143] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-12-01 16:33:10,936] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
{'loss': 0.1961, 'grad_norm': 0.09493500739336014, 'learning_rate': 0.0, 'epoch': 0.88}
{'loss': 0.1926, 'grad_norm': 0.20905379951000214, 'learning_rate': 3.0102999566398115e-05, 'epoch': 1.0}
{'loss': 0.1996, 'grad_norm': 0.10625384747982025, 'learning_rate': 4.771212547196624e-05, 'epoch': 1.88}
{'loss': 0.234, 'grad_norm': 0.14679113030433655, 'learning_rate': 6.020599913279623e-05, 'epoch': 2.0}
{'loss': 0.1912, 'grad_norm': 0.11115482449531555, 'learning_rate': 6.989700043360187e-05, 'epoch': 2.88}
{'loss': 0.2601, 'grad_norm': 0.19218184053897858, 'learning_rate': 7.781512503836436e-05, 'epoch': 3.0}
{'loss': 0.1786, 'grad_norm': 0.11330313980579376, 'learning_rate': 8.450980400142567e-05, 'epoch': 3.88}
{'loss': 0.1503, 'grad_norm': 0.17110419273376465, 'learning_rate': 9.030899869919434e-05, 'epoch': 4.0}
{'loss': 0.1598, 'grad_norm': 0.1920377016067505, 'learning_rate': 9.542425094393248e-05, 'epoch': 4.88}
{'loss': 0.2272, 'grad_norm': 0.4537360966205597, 'learning_rate': 9.999999999999999e-05, 'epoch': 5.0}
{'loss': 0.1616, 'grad_norm': 0.3397439122200012, 'learning_rate': 0.0001, 'epoch': 5.88}
{'loss': 0.1758, 'grad_norm': 0.17282140254974365, 'learning_rate': 9.956521739130435e-05, 'epoch': 6.0}
{'loss': 0.1531, 'grad_norm': 0.09090474247932434, 'learning_rate': 9.91304347826087e-05, 'epoch': 6.88}
{'loss': 0.1071, 'grad_norm': 0.09436066448688507, 'learning_rate': 9.869565217391305e-05, 'epoch': 7.0}
{'loss': 0.1489, 'grad_norm': 0.0803586095571518, 'learning_rate': 9.82608695652174e-05, 'epoch': 7.88}
{'loss': 0.147, 'grad_norm': 0.1330277919769287, 'learning_rate': 9.782608695652174e-05, 'epoch': 8.0}
{'loss': 0.1468, 'grad_norm': 0.07912874966859818, 'learning_rate': 9.739130434782609e-05, 'epoch': 8.88}
{'loss': 0.1193, 'grad_norm': 0.1039164587855339, 'learning_rate': 9.695652173913044e-05, 'epoch': 9.0}
{'loss': 0.1444, 'grad_norm': 0.05914803221821785, 'learning_rate': 9.652173913043479e-05, 'epoch': 9.88}
{'loss': 0.1177, 'grad_norm': 0.13809038698673248, 'learning_rate': 9.608695652173914e-05, 'epoch': 10.0}
{'loss': 0.1362, 'grad_norm': 0.0948038250207901, 'learning_rate': 9.565217391304348e-05, 'epoch': 10.88}
{'loss': 0.1319, 'grad_norm': 0.1608584225177765, 'learning_rate': 9.521739130434783e-05, 'epoch': 11.0}
{'loss': 0.1356, 'grad_norm': 0.059028711169958115, 'learning_rate': 9.478260869565218e-05, 'epoch': 11.88}
{'loss': 0.1368, 'grad_norm': 0.17237794399261475, 'learning_rate': 9.434782608695653e-05, 'epoch': 12.0}
{'loss': 0.1381, 'grad_norm': 0.08209731429815292, 'learning_rate': 9.391304347826087e-05, 'epoch': 12.88}
{'loss': 0.0836, 'grad_norm': 0.0879979133605957, 'learning_rate': 9.347826086956522e-05, 'epoch': 13.0}
{'loss': 0.1286, 'grad_norm': 0.0718340203166008, 'learning_rate': 9.304347826086957e-05, 'epoch': 13.88}
{'loss': 0.1859, 'grad_norm': 0.13956455886363983, 'learning_rate': 9.260869565217392e-05, 'epoch': 14.0}
{'loss': 0.1278, 'grad_norm': 0.07634548842906952, 'learning_rate': 9.217391304347827e-05, 'epoch': 14.88}
{'loss': 0.1683, 'grad_norm': 0.14146503806114197, 'learning_rate': 9.173913043478261e-05, 'epoch': 15.0}
{'loss': 0.1271, 'grad_norm': 0.08319215476512909, 'learning_rate': 9.130434782608696e-05, 'epoch': 15.88}
{'loss': 0.13, 'grad_norm': 0.09863106161355972, 'learning_rate': 9.086956521739131e-05, 'epoch': 16.0}
{'loss': 0.123, 'grad_norm': 0.08795441687107086, 'learning_rate': 9.043478260869566e-05, 'epoch': 16.88}
{'loss': 0.149, 'grad_norm': 0.14410637319087982, 'learning_rate': 9e-05, 'epoch': 17.0}
{'loss': 0.1262, 'grad_norm': 0.0718708261847496, 'learning_rate': 8.956521739130435e-05, 'epoch': 17.88}
{'loss': 0.115, 'grad_norm': 0.24184608459472656, 'learning_rate': 8.91304347826087e-05, 'epoch': 18.0}
{'loss': 0.1244, 'grad_norm': 0.083210289478302, 'learning_rate': 8.869565217391305e-05, 'epoch': 18.88}
{'loss': 0.1154, 'grad_norm': 0.47366875410079956, 'learning_rate': 8.82608695652174e-05, 'epoch': 19.0}
{'loss': 0.1233, 'grad_norm': 0.058194562792778015, 'learning_rate': 8.782608695652174e-05, 'epoch': 19.88}
{'loss': 0.127, 'grad_norm': 0.2000868320465088, 'learning_rate': 8.739130434782609e-05, 'epoch': 20.0}
{'loss': 0.1228, 'grad_norm': 0.07232818752527237, 'learning_rate': 8.695652173913044e-05, 'epoch': 20.88}
{'loss': 0.1419, 'grad_norm': 0.14396068453788757, 'learning_rate': 8.652173913043479e-05, 'epoch': 21.0}
{'loss': 0.1278, 'grad_norm': 0.06380149722099304, 'learning_rate': 8.608695652173914e-05, 'epoch': 21.88}
{'loss': 0.1087, 'grad_norm': 0.13022585213184357, 'learning_rate': 8.565217391304348e-05, 'epoch': 22.0}
{'loss': 0.1274, 'grad_norm': 0.10545800626277924, 'learning_rate': 8.521739130434783e-05, 'epoch': 22.88}
{'loss': 0.1125, 'grad_norm': 0.12194037437438965, 'learning_rate': 8.478260869565218e-05, 'epoch': 23.0}
{'loss': 0.1249, 'grad_norm': 0.11074714362621307, 'learning_rate': 8.434782608695653e-05, 'epoch': 23.88}
{'loss': 0.1137, 'grad_norm': 0.12392047047615051, 'learning_rate': 8.391304347826088e-05, 'epoch': 24.0}
{'loss': 0.1182, 'grad_norm': 0.1451348513364792, 'learning_rate': 8.347826086956521e-05, 'epoch': 24.88}
{'loss': 0.1402, 'grad_norm': 0.2345655858516693, 'learning_rate': 8.304347826086957e-05, 'epoch': 25.0}
{'loss': 0.1204, 'grad_norm': 0.07617977261543274, 'learning_rate': 8.260869565217392e-05, 'epoch': 25.88}
{'loss': 0.1243, 'grad_norm': 0.33757680654525757, 'learning_rate': 8.217391304347827e-05, 'epoch': 26.0}
{'loss': 0.1141, 'grad_norm': 0.07516543567180634, 'learning_rate': 8.173913043478262e-05, 'epoch': 26.88}
{'loss': 0.1648, 'grad_norm': 0.22174856066703796, 'learning_rate': 8.130434782608696e-05, 'epoch': 27.0}
{'loss': 0.1144, 'grad_norm': 0.09736449271440506, 'learning_rate': 8.086956521739131e-05, 'epoch': 27.88}
{'loss': 0.1442, 'grad_norm': 0.21648001670837402, 'learning_rate': 8.043478260869566e-05, 'epoch': 28.0}
{'loss': 0.1127, 'grad_norm': 0.06960522383451462, 'learning_rate': 8e-05, 'epoch': 28.88}
{'loss': 0.1401, 'grad_norm': 0.3803006410598755, 'learning_rate': 7.956521739130434e-05, 'epoch': 29.0}
{'loss': 0.1178, 'grad_norm': 0.11698950827121735, 'learning_rate': 7.91304347826087e-05, 'epoch': 29.88}
{'loss': 0.0806, 'grad_norm': 0.15600794553756714, 'learning_rate': 7.869565217391305e-05, 'epoch': 30.0}
{'loss': 0.1153, 'grad_norm': 0.10825799405574799, 'learning_rate': 7.82608695652174e-05, 'epoch': 30.88}
{'loss': 0.1046, 'grad_norm': 0.44559481739997864, 'learning_rate': 7.782608695652173e-05, 'epoch': 31.0}
{'loss': 0.1088, 'grad_norm': 0.26130664348602295, 'learning_rate': 7.73913043478261e-05, 'epoch': 31.88}
{'loss': 0.1113, 'grad_norm': 3.32611346244812, 'learning_rate': 7.695652173913044e-05, 'epoch': 32.0}
{'loss': 0.1108, 'grad_norm': 0.14438793063163757, 'learning_rate': 7.652173913043479e-05, 'epoch': 32.88}
{'loss': 0.0843, 'grad_norm': 0.2705294191837311, 'learning_rate': 7.608695652173914e-05, 'epoch': 33.0}
{'loss': 0.1062, 'grad_norm': 0.19524337351322174, 'learning_rate': 7.565217391304347e-05, 'epoch': 33.88}
{'loss': 0.1047, 'grad_norm': 0.2903197109699249, 'learning_rate': 7.521739130434783e-05, 'epoch': 34.0}
{'loss': 0.1047, 'grad_norm': 0.13142095506191254, 'learning_rate': 7.478260869565218e-05, 'epoch': 34.88}
{'loss': 0.0761, 'grad_norm': 0.2697310745716095, 'learning_rate': 7.434782608695653e-05, 'epoch': 35.0}
{'loss': 0.0965, 'grad_norm': 0.2121906876564026, 'learning_rate': 7.391304347826086e-05, 'epoch': 35.88}
{'loss': 0.1254, 'grad_norm': 0.3239136040210724, 'learning_rate': 7.347826086956522e-05, 'epoch': 36.0}
{'loss': 0.1026, 'grad_norm': 0.5506059527397156, 'learning_rate': 7.304347826086957e-05, 'epoch': 36.88}
{'loss': 0.0821, 'grad_norm': 0.2981710731983185, 'learning_rate': 7.260869565217392e-05, 'epoch': 37.0}
{'loss': 0.0981, 'grad_norm': 0.505107581615448, 'learning_rate': 7.217391304347827e-05, 'epoch': 37.88}
{'loss': 0.1252, 'grad_norm': 0.4590071141719818, 'learning_rate': 7.17391304347826e-05, 'epoch': 38.0}
{'loss': 0.0933, 'grad_norm': 0.39927995204925537, 'learning_rate': 7.130434782608696e-05, 'epoch': 38.88}
{'loss': 0.1068, 'grad_norm': 0.5103194117546082, 'learning_rate': 7.086956521739131e-05, 'epoch': 39.0}
{'loss': 0.0879, 'grad_norm': 0.2564444839954376, 'learning_rate': 7.043478260869566e-05, 'epoch': 39.88}
{'loss': 0.088, 'grad_norm': 0.4337961971759796, 'learning_rate': 7e-05, 'epoch': 40.0}
{'loss': 0.0872, 'grad_norm': 0.33543339371681213, 'learning_rate': 6.956521739130436e-05, 'epoch': 40.88}
{'loss': 0.067, 'grad_norm': 0.431728720664978, 'learning_rate': 6.91304347826087e-05, 'epoch': 41.0}
{'loss': 0.0756, 'grad_norm': 0.3839477300643921, 'learning_rate': 6.869565217391305e-05, 'epoch': 41.88}
{'loss': 0.0684, 'grad_norm': 0.4598580002784729, 'learning_rate': 6.826086956521739e-05, 'epoch': 42.0}
{'loss': 0.0709, 'grad_norm': 0.23850810527801514, 'learning_rate': 6.782608695652173e-05, 'epoch': 42.88}
{'loss': 0.0518, 'grad_norm': 0.32811957597732544, 'learning_rate': 6.73913043478261e-05, 'epoch': 43.0}
{'loss': 0.059, 'grad_norm': 0.18179203569889069, 'learning_rate': 6.695652173913044e-05, 'epoch': 43.88}
{'loss': 0.0711, 'grad_norm': 0.37446677684783936, 'learning_rate': 6.652173913043479e-05, 'epoch': 44.0}
{'loss': 0.0564, 'grad_norm': 0.2396809309720993, 'learning_rate': 6.608695652173912e-05, 'epoch': 44.88}
{'loss': 0.0396, 'grad_norm': 0.28988105058670044, 'learning_rate': 6.565217391304349e-05, 'epoch': 45.0}
{'loss': 0.0485, 'grad_norm': 0.24327829480171204, 'learning_rate': 6.521739130434783e-05, 'epoch': 45.88}
{'loss': 0.0332, 'grad_norm': 0.3316682279109955, 'learning_rate': 6.478260869565218e-05, 'epoch': 46.0}
{'loss': 0.0391, 'grad_norm': 0.3122806251049042, 'learning_rate': 6.434782608695652e-05, 'epoch': 46.88}
{'loss': 0.0427, 'grad_norm': 0.4711802303791046, 'learning_rate': 6.391304347826086e-05, 'epoch': 47.0}
{'loss': 0.0326, 'grad_norm': 0.26639705896377563, 'learning_rate': 6.347826086956523e-05, 'epoch': 47.88}
{'loss': 0.0284, 'grad_norm': 0.26327797770500183, 'learning_rate': 6.304347826086957e-05, 'epoch': 48.0}
{'loss': 0.0302, 'grad_norm': 0.26127082109451294, 'learning_rate': 6.260869565217392e-05, 'epoch': 48.88}
{'loss': 0.0234, 'grad_norm': 0.2630583941936493, 'learning_rate': 6.217391304347826e-05, 'epoch': 49.0}
{'loss': 0.0232, 'grad_norm': 0.20344384014606476, 'learning_rate': 6.173913043478262e-05, 'epoch': 49.88}
{'loss': 0.0294, 'grad_norm': 0.35717999935150146, 'learning_rate': 6.130434782608696e-05, 'epoch': 50.0}
{'eval_loss': 0.27616575360298157, 'eval_runtime': 14.4741, 'eval_samples_per_second': 4.214, 'eval_steps_per_second': 0.207, 'epoch': 50.0}
{'loss': 0.021, 'grad_norm': 0.19900667667388916, 'learning_rate': 6.086956521739131e-05, 'epoch': 50.88}
{'loss': 0.0178, 'grad_norm': 0.5378164649009705, 'learning_rate': 6.0434782608695654e-05, 'epoch': 51.0}
{'loss': 0.0156, 'grad_norm': 0.0788610503077507, 'learning_rate': 6e-05, 'epoch': 51.88}
{'loss': 0.0261, 'grad_norm': 0.3557453751564026, 'learning_rate': 5.956521739130435e-05, 'epoch': 52.0}
{'loss': 0.0157, 'grad_norm': 0.13699369132518768, 'learning_rate': 5.9130434782608704e-05, 'epoch': 52.88}
{'loss': 0.0137, 'grad_norm': 0.16423006355762482, 'learning_rate': 5.869565217391305e-05, 'epoch': 53.0}
{'loss': 0.0139, 'grad_norm': 0.24934597313404083, 'learning_rate': 5.826086956521739e-05, 'epoch': 53.88}
{'loss': 0.0147, 'grad_norm': 0.4688236117362976, 'learning_rate': 5.782608695652174e-05, 'epoch': 54.0}
{'loss': 0.0125, 'grad_norm': 0.19613470137119293, 'learning_rate': 5.739130434782609e-05, 'epoch': 54.88}
{'loss': 0.0127, 'grad_norm': 0.5613853335380554, 'learning_rate': 5.695652173913044e-05, 'epoch': 55.0}
{'loss': 0.0105, 'grad_norm': 0.19328196346759796, 'learning_rate': 5.652173913043478e-05, 'epoch': 55.88}
{'loss': 0.0061, 'grad_norm': 0.2140585482120514, 'learning_rate': 5.608695652173913e-05, 'epoch': 56.0}
{'loss': 0.0101, 'grad_norm': 0.257788747549057, 'learning_rate': 5.565217391304348e-05, 'epoch': 56.88}
{'loss': 0.0082, 'grad_norm': 0.2887912094593048, 'learning_rate': 5.5217391304347835e-05, 'epoch': 57.0}
{'loss': 0.0088, 'grad_norm': 0.15451782941818237, 'learning_rate': 5.478260869565217e-05, 'epoch': 57.88}
{'loss': 0.0082, 'grad_norm': 0.2825792729854584, 'learning_rate': 5.4347826086956524e-05, 'epoch': 58.0}
{'loss': 0.0083, 'grad_norm': 0.18696072697639465, 'learning_rate': 5.391304347826087e-05, 'epoch': 58.88}
{'loss': 0.0071, 'grad_norm': 0.28762540221214294, 'learning_rate': 5.347826086956522e-05, 'epoch': 59.0}
{'loss': 0.0077, 'grad_norm': 0.17428140342235565, 'learning_rate': 5.3043478260869574e-05, 'epoch': 59.88}
{'loss': 0.0042, 'grad_norm': 0.05681639909744263, 'learning_rate': 5.260869565217391e-05, 'epoch': 60.0}
{'loss': 0.0065, 'grad_norm': 0.08475183695554733, 'learning_rate': 5.217391304347826e-05, 'epoch': 60.88}
{'loss': 0.0076, 'grad_norm': 0.19718989729881287, 'learning_rate': 5.173913043478261e-05, 'epoch': 61.0}
{'loss': 0.0066, 'grad_norm': 0.20681598782539368, 'learning_rate': 5.1304347826086966e-05, 'epoch': 61.88}
{'loss': 0.0054, 'grad_norm': 0.05734999477863312, 'learning_rate': 5.08695652173913e-05, 'epoch': 62.0}
{'loss': 0.0053, 'grad_norm': 0.047938521951436996, 'learning_rate': 5.0434782608695655e-05, 'epoch': 62.88}
{'loss': 0.0049, 'grad_norm': 0.5166565775871277, 'learning_rate': 5e-05, 'epoch': 63.0}
{'loss': 0.005, 'grad_norm': 0.24868188798427582, 'learning_rate': 4.956521739130435e-05, 'epoch': 63.88}
{'loss': 0.0052, 'grad_norm': 0.049406833946704865, 'learning_rate': 4.91304347826087e-05, 'epoch': 64.0}
{'loss': 0.0049, 'grad_norm': 0.4606536328792572, 'learning_rate': 4.8695652173913046e-05, 'epoch': 64.88}
{'loss': 0.0032, 'grad_norm': 0.13834437727928162, 'learning_rate': 4.8260869565217394e-05, 'epoch': 65.0}
{'loss': 0.0041, 'grad_norm': 0.1153917983174324, 'learning_rate': 4.782608695652174e-05, 'epoch': 65.88}
{'loss': 0.0048, 'grad_norm': 0.0836620181798935, 'learning_rate': 4.739130434782609e-05, 'epoch': 66.0}
{'loss': 0.0035, 'grad_norm': 0.053142257034778595, 'learning_rate': 4.695652173913044e-05, 'epoch': 66.88}
{'loss': 0.0068, 'grad_norm': 0.4845718443393707, 'learning_rate': 4.6521739130434785e-05, 'epoch': 67.0}
{'loss': 0.0033, 'grad_norm': 0.051676537841558456, 'learning_rate': 4.608695652173913e-05, 'epoch': 67.88}
{'loss': 0.0019, 'grad_norm': 0.058481525629758835, 'learning_rate': 4.565217391304348e-05, 'epoch': 68.0}
{'loss': 0.0034, 'grad_norm': 0.08463426679372787, 'learning_rate': 4.521739130434783e-05, 'epoch': 68.88}
{'loss': 0.0023, 'grad_norm': 0.13796326518058777, 'learning_rate': 4.478260869565218e-05, 'epoch': 69.0}
{'loss': 0.0031, 'grad_norm': 0.0889427438378334, 'learning_rate': 4.4347826086956525e-05, 'epoch': 69.88}
{'loss': 0.0016, 'grad_norm': 0.09067434817552567, 'learning_rate': 4.391304347826087e-05, 'epoch': 70.0}
{'loss': 0.0017, 'grad_norm': 0.08282621949911118, 'learning_rate': 4.347826086956522e-05, 'epoch': 70.88}
{'loss': 0.0015, 'grad_norm': 0.0907311961054802, 'learning_rate': 4.304347826086957e-05, 'epoch': 71.0}
{'loss': 0.0013, 'grad_norm': 0.06852951645851135, 'learning_rate': 4.2608695652173916e-05, 'epoch': 71.88}
{'loss': 0.0006, 'grad_norm': 0.03912285715341568, 'learning_rate': 4.2173913043478264e-05, 'epoch': 72.0}
{'loss': 0.001, 'grad_norm': 0.040981072932481766, 'learning_rate': 4.1739130434782605e-05, 'epoch': 72.88}
{'loss': 0.0021, 'grad_norm': 0.08384090662002563, 'learning_rate': 4.130434782608696e-05, 'epoch': 73.0}
{'loss': 0.0006, 'grad_norm': 0.020515281707048416, 'learning_rate': 4.086956521739131e-05, 'epoch': 73.88}
{'loss': 0.0004, 'grad_norm': 0.03432730585336685, 'learning_rate': 4.0434782608695655e-05, 'epoch': 74.0}
{'loss': 0.0005, 'grad_norm': 0.023685066029429436, 'learning_rate': 4e-05, 'epoch': 74.88}
{'loss': 0.0002, 'grad_norm': 0.10248688608407974, 'learning_rate': 3.956521739130435e-05, 'epoch': 75.0}
{'loss': 0.0006, 'grad_norm': 0.03883909806609154, 'learning_rate': 3.91304347826087e-05, 'epoch': 75.88}
{'loss': 0.0007, 'grad_norm': 0.06351843476295471, 'learning_rate': 3.869565217391305e-05, 'epoch': 76.0}
{'loss': 0.0004, 'grad_norm': 0.01861829310655594, 'learning_rate': 3.8260869565217395e-05, 'epoch': 76.88}
{'loss': 0.0002, 'grad_norm': 0.019936056807637215, 'learning_rate': 3.7826086956521736e-05, 'epoch': 77.0}
{'loss': 0.0001, 'grad_norm': 0.006279464345425367, 'learning_rate': 3.739130434782609e-05, 'epoch': 77.88}
{'loss': 0.0001, 'grad_norm': 0.024939458817243576, 'learning_rate': 3.695652173913043e-05, 'epoch': 78.0}
{'loss': 0.0001, 'grad_norm': 0.02555079385638237, 'learning_rate': 3.6521739130434786e-05, 'epoch': 78.88}
{'loss': 0.0001, 'grad_norm': 0.0067434185184538364, 'learning_rate': 3.6086956521739134e-05, 'epoch': 79.0}
{'loss': 0.0001, 'grad_norm': 0.007015773095190525, 'learning_rate': 3.565217391304348e-05, 'epoch': 79.88}
{'loss': 0.0001, 'grad_norm': 0.023291299119591713, 'learning_rate': 3.521739130434783e-05, 'epoch': 80.0}
{'loss': 0.0001, 'grad_norm': 0.007705835159868002, 'learning_rate': 3.478260869565218e-05, 'epoch': 80.88}
{'loss': 0.0, 'grad_norm': 0.009486564435064793, 'learning_rate': 3.4347826086956526e-05, 'epoch': 81.0}
{'loss': 0.0, 'grad_norm': 0.010303783230483532, 'learning_rate': 3.3913043478260867e-05, 'epoch': 81.88}
{'loss': 0.0, 'grad_norm': 0.001754940953105688, 'learning_rate': 3.347826086956522e-05, 'epoch': 82.0}
{'loss': 0.0, 'grad_norm': 0.002098354045301676, 'learning_rate': 3.304347826086956e-05, 'epoch': 82.88}
{'loss': 0.0, 'grad_norm': 0.005067596212029457, 'learning_rate': 3.260869565217392e-05, 'epoch': 83.0}
{'loss': 0.0, 'grad_norm': 0.009945479221642017, 'learning_rate': 3.217391304347826e-05, 'epoch': 83.88}
{'loss': 0.0, 'grad_norm': 0.004777831491082907, 'learning_rate': 3.173913043478261e-05, 'epoch': 84.0}
{'loss': 0.0, 'grad_norm': 0.0034823615569621325, 'learning_rate': 3.130434782608696e-05, 'epoch': 84.88}
{'loss': 0.0, 'grad_norm': 0.0017694365233182907, 'learning_rate': 3.086956521739131e-05, 'epoch': 85.0}
{'loss': 0.0, 'grad_norm': 0.006096467841416597, 'learning_rate': 3.0434782608695656e-05, 'epoch': 85.88}
{'loss': 0.0, 'grad_norm': 0.002593852113932371, 'learning_rate': 3e-05, 'epoch': 86.0}
{'loss': 0.0, 'grad_norm': 0.0019472639542073011, 'learning_rate': 2.9565217391304352e-05, 'epoch': 86.88}
{'loss': 0.0, 'grad_norm': 0.0011919605312868953, 'learning_rate': 2.9130434782608696e-05, 'epoch': 87.0}
{'loss': 0.0, 'grad_norm': 0.0014921561814844608, 'learning_rate': 2.8695652173913044e-05, 'epoch': 87.88}
{'loss': 0.0, 'grad_norm': 0.0024503383319824934, 'learning_rate': 2.826086956521739e-05, 'epoch': 88.0}
{'loss': 0.0, 'grad_norm': 0.0009284549159929156, 'learning_rate': 2.782608695652174e-05, 'epoch': 88.88}
{'loss': 0.0, 'grad_norm': 0.0020415280014276505, 'learning_rate': 2.7391304347826085e-05, 'epoch': 89.0}
{'loss': 0.0, 'grad_norm': 0.0010269187623634934, 'learning_rate': 2.6956521739130436e-05, 'epoch': 89.88}
{'loss': 0.0, 'grad_norm': 0.0014027566649019718, 'learning_rate': 2.6521739130434787e-05, 'epoch': 90.0}
{'loss': 0.0, 'grad_norm': 0.0008743895450606942, 'learning_rate': 2.608695652173913e-05, 'epoch': 90.88}
{'loss': 0.0, 'grad_norm': 0.00038808854878880084, 'learning_rate': 2.5652173913043483e-05, 'epoch': 91.0}
{'loss': 0.0, 'grad_norm': 0.0005749269621446729, 'learning_rate': 2.5217391304347827e-05, 'epoch': 91.88}
{'loss': 0.0, 'grad_norm': 0.0009118059533648193, 'learning_rate': 2.4782608695652175e-05, 'epoch': 92.0}
{'loss': 0.0, 'grad_norm': 0.0004389614041429013, 'learning_rate': 2.4347826086956523e-05, 'epoch': 92.88}
{'loss': 0.0, 'grad_norm': 0.004027628339827061, 'learning_rate': 2.391304347826087e-05, 'epoch': 93.0}
{'loss': 0.0, 'grad_norm': 0.0004896558239124715, 'learning_rate': 2.347826086956522e-05, 'epoch': 93.88}
{'loss': 0.0, 'grad_norm': 0.0018629803089424968, 'learning_rate': 2.3043478260869567e-05, 'epoch': 94.0}
{'loss': 0.0, 'grad_norm': 0.0005314217414706945, 'learning_rate': 2.2608695652173914e-05, 'epoch': 94.88}
{'loss': 0.0, 'grad_norm': 0.0008154144161380827, 'learning_rate': 2.2173913043478262e-05, 'epoch': 95.0}
{'loss': 0.0, 'grad_norm': 0.0006789430044591427, 'learning_rate': 2.173913043478261e-05, 'epoch': 95.88}
{'loss': 0.0, 'grad_norm': 0.0008584159659221768, 'learning_rate': 2.1304347826086958e-05, 'epoch': 96.0}
{'loss': 0.0, 'grad_norm': 0.0005070502520538867, 'learning_rate': 2.0869565217391303e-05, 'epoch': 96.88}
{'loss': 0.0, 'grad_norm': 0.0005356569890864193, 'learning_rate': 2.0434782608695654e-05, 'epoch': 97.0}
{'loss': 0.0, 'grad_norm': 0.00049642997328192, 'learning_rate': 2e-05, 'epoch': 97.88}
{'loss': 0.0, 'grad_norm': 0.0005777889164164662, 'learning_rate': 1.956521739130435e-05, 'epoch': 98.0}
{'loss': 0.0, 'grad_norm': 0.00038558494998142123, 'learning_rate': 1.9130434782608697e-05, 'epoch': 98.88}
{'loss': 0.0, 'grad_norm': 0.0004891115240752697, 'learning_rate': 1.8695652173913045e-05, 'epoch': 99.0}
{'loss': 0.0, 'grad_norm': 0.00038280434091575444, 'learning_rate': 1.8260869565217393e-05, 'epoch': 99.88}
{'loss': 0.0, 'grad_norm': 0.0014692987315356731, 'learning_rate': 1.782608695652174e-05, 'epoch': 100.0}
{'eval_loss': 0.6580808162689209, 'eval_runtime': 14.4803, 'eval_samples_per_second': 4.213, 'eval_steps_per_second': 0.207, 'epoch': 100.0}
{'loss': 0.0, 'grad_norm': 0.0003747382725123316, 'learning_rate': 1.739130434782609e-05, 'epoch': 100.88}
{'loss': 0.0, 'grad_norm': 0.0005895541398786008, 'learning_rate': 1.6956521739130433e-05, 'epoch': 101.0}
{'loss': 0.0, 'grad_norm': 0.00046854131505824625, 'learning_rate': 1.652173913043478e-05, 'epoch': 101.88}
{'loss': 0.0, 'grad_norm': 0.0010638064704835415, 'learning_rate': 1.608695652173913e-05, 'epoch': 102.0}
{'loss': 0.0, 'grad_norm': 0.00042640906758606434, 'learning_rate': 1.565217391304348e-05, 'epoch': 102.88}
{'loss': 0.0, 'grad_norm': 0.000589178001973778, 'learning_rate': 1.5217391304347828e-05, 'epoch': 103.0}
{'loss': 0.0, 'grad_norm': 0.0002949210465885699, 'learning_rate': 1.4782608695652176e-05, 'epoch': 103.88}
{'loss': 0.0, 'grad_norm': 0.0010559633374214172, 'learning_rate': 1.4347826086956522e-05, 'epoch': 104.0}
{'loss': 0.0, 'grad_norm': 0.0006210787687450647, 'learning_rate': 1.391304347826087e-05, 'epoch': 104.88}
{'loss': 0.0, 'grad_norm': 0.0017317233141511679, 'learning_rate': 1.3478260869565218e-05, 'epoch': 105.0}
{'loss': 0.0, 'grad_norm': 0.0004954335745424032, 'learning_rate': 1.3043478260869566e-05, 'epoch': 105.88}
{'loss': 0.0, 'grad_norm': 0.0006421405705623329, 'learning_rate': 1.2608695652173914e-05, 'epoch': 106.0}
{'loss': 0.0, 'grad_norm': 0.00029281049501150846, 'learning_rate': 1.2173913043478261e-05, 'epoch': 106.88}
{'loss': 0.0, 'grad_norm': 0.0014980240957811475, 'learning_rate': 1.173913043478261e-05, 'epoch': 107.0}
{'loss': 0.0, 'grad_norm': 0.0002785834367386997, 'learning_rate': 1.1304347826086957e-05, 'epoch': 107.88}
{'loss': 0.0, 'grad_norm': 0.0004235480446368456, 'learning_rate': 1.0869565217391305e-05, 'epoch': 108.0}
{'loss': 0.0, 'grad_norm': 0.0002637866127770394, 'learning_rate': 1.0434782608695651e-05, 'epoch': 108.88}
{'loss': 0.0, 'grad_norm': 0.000680705881677568, 'learning_rate': 1e-05, 'epoch': 109.0}
{'loss': 0.0, 'grad_norm': 0.00038517973734997213, 'learning_rate': 9.565217391304349e-06, 'epoch': 109.88}
{'loss': 0.0, 'grad_norm': 0.0022121514193713665, 'learning_rate': 9.130434782608697e-06, 'epoch': 110.0}
{'loss': 0.0, 'grad_norm': 0.0036549081560224295, 'learning_rate': 8.695652173913044e-06, 'epoch': 110.88}
{'loss': 0.0, 'grad_norm': 0.0004932321608066559, 'learning_rate': 8.26086956521739e-06, 'epoch': 111.0}
{'loss': 0.0, 'grad_norm': 0.000990509637631476, 'learning_rate': 7.82608695652174e-06, 'epoch': 111.88}
{'loss': 0.0, 'grad_norm': 0.0007917767507024109, 'learning_rate': 7.391304347826088e-06, 'epoch': 112.0}
{'loss': 0.0, 'grad_norm': 0.00030788485310040414, 'learning_rate': 6.956521739130435e-06, 'epoch': 112.88}
{'loss': 0.0, 'grad_norm': 0.0005363399395719171, 'learning_rate': 6.521739130434783e-06, 'epoch': 113.0}
{'loss': 0.0, 'grad_norm': 0.0003840311255771667, 'learning_rate': 6.086956521739131e-06, 'epoch': 113.88}
{'loss': 0.0, 'grad_norm': 0.0006426501786336303, 'learning_rate': 5.652173913043479e-06, 'epoch': 114.0}
{'loss': 0.0, 'grad_norm': 0.0004969416186213493, 'learning_rate': 5.217391304347826e-06, 'epoch': 114.88}
{'loss': 0.0, 'grad_norm': 0.0004568318836390972, 'learning_rate': 4.782608695652174e-06, 'epoch': 115.0}
{'loss': 0.0, 'grad_norm': 0.0003793009091168642, 'learning_rate': 4.347826086956522e-06, 'epoch': 115.88}
{'loss': 0.0, 'grad_norm': 0.00031298716203309596, 'learning_rate': 3.91304347826087e-06, 'epoch': 116.0}
{'loss': 0.0, 'grad_norm': 0.0002953401708509773, 'learning_rate': 3.4782608695652175e-06, 'epoch': 116.88}
{'loss': 0.0, 'grad_norm': 0.0005045499419793487, 'learning_rate': 3.0434782608695654e-06, 'epoch': 117.0}
{'loss': 0.0, 'grad_norm': 0.00032098102383315563, 'learning_rate': 2.608695652173913e-06, 'epoch': 117.88}
{'loss': 0.0, 'grad_norm': 0.00042286558891646564, 'learning_rate': 2.173913043478261e-06, 'epoch': 118.0}
{'loss': 0.0, 'grad_norm': 0.0002936659730039537, 'learning_rate': 1.7391304347826088e-06, 'epoch': 118.88}
{'loss': 0.0, 'grad_norm': 0.0005251080729067326, 'learning_rate': 1.3043478260869564e-06, 'epoch': 119.0}
{'loss': 0.0, 'grad_norm': 0.0002728002436924726, 'learning_rate': 8.695652173913044e-07, 'epoch': 119.88}
{'loss': 0.0, 'grad_norm': 0.0005082686548121274, 'learning_rate': 4.347826086956522e-07, 'epoch': 120.0}

ðŸ’¾ Saving model with the current parameters
ðŸ’¾ Saving model with the current parameters
ðŸ’¾ Saving model with the current parameters

ðŸ’¾ Saving model with the current parameters
ðŸ’¾ Saving model with the current parameters



ðŸ’¾ Saving model with the current parameters


ðŸ’¾ Saving model with the current parameters
{'train_runtime': 15211.7499, 'train_samples_per_second': 1.428, 'train_steps_per_second': 0.016, 'train_loss': 0.049468775277936555, 'epoch': 120.0}

ðŸ’¾ Saving model with the current parameters
âœ“ Training complete!
âœ“ Training complete!
âœ“ Training complete!
âœ“ Training complete!
âœ“ Training complete!
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
âœ“ Training complete!
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
âœ“ Training complete!
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
Saved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpg
Saved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpgSaved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpg

Saved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpg
Saved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpg
Saved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpg
Saved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpg
âœ“ Training complete!
Centaur full build run - Training completed!
Contents of '/scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120':
runs
checkpoint-100
checkpoint-200
checkpoint-240
tokenizer_config.json
special_tokens_map.json
README.md
tokenizer.json
adapter_model.safetensors
adapter_config.json
training_args.bin
Error: trainer_state.json not found at /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/trainer_state.json
Found trainer_state.json in: /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0001_4092_3_3_7_0.01_120/checkpoint-100/trainer_state.json
Saved combined loss plot to loss_plot_0.0001_4092_3_3_7_0.01_120.jpg
[2025-12-01 20:47:17,897] [INFO] [launch.py:367:main] Process 2718863 exits successfully.
[2025-12-01 20:47:18,898] [INFO] [launch.py:367:main] Process 2718864 exits successfully.
[2025-12-01 20:47:19,899] [INFO] [launch.py:367:main] Process 2718862 exits successfully.
[2025-12-01 20:47:20,900] [INFO] [launch.py:367:main] Process 2718861 exits successfully.
[2025-12-01 20:47:21,900] [INFO] [launch.py:367:main] Process 2718859 exits successfully.
[2025-12-01 20:47:22,901] [INFO] [launch.py:367:main] Process 2718865 exits successfully.
[2025-12-01 20:47:23,902] [INFO] [launch.py:367:main] Process 2718860 exits successfully.
[2025-12-01 20:47:24,902] [INFO] [launch.py:367:main] Process 2718858 exits successfully.
