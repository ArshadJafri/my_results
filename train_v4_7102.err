2025-11-30 18:08:21.555871: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:21.607520: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:24.301072: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:35.905369: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:35.954882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:38.189067: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.275132: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.325897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:46.396797: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.448743: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:46.457468: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.477897: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.495732: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.500795: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.508615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:46.521573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.521573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:46.530288: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:46.545781: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:46.545781: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:46.573523: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:46.573523: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-11-30 18:08:49.093275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:49.155150: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:49.259018: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:49.387953: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:49.419094: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:49.428143: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:49.486220: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 18:08:49.493194: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.95s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.96s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.94s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.94s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.94s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:19,  3.98s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:20,  4.04s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:04<00:20,  4.04s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.40s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.40s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.44s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:18,  4.51s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.50s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.50s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:17,  4.50s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:08<00:18,  4.55s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:14,  4.94s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:14,  4.91s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:14,  4.96s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:15,  5.05s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:15,  5.06s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:15,  5.08s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:15,  5.08s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:14<00:15,  5.13s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.33s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.33s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.33s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.35s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.35s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.38s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.38s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.44s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:25<00:05,  5.50s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:25<00:05,  5.51s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.52s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.53s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.53s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.55s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.55s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:26<00:05,  5.63s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.76s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.75s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.77s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.79s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.79s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.52s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.81s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.52s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:28<00:00,  4.80s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.93s/it]
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Using auto half precision backend
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
/home/axs7716/my_model/Finetuning_V4-deepseek.py:239: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 10. Using DeepSpeed's value.
***** Running training *****
  Num examples = 181
  Num Epochs = 115
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 160
  Gradient Accumulation steps = 10
  Total optimization steps = 230
  Number of trainable parameters = 103,546,880
  0%|          | 0/230 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/axs7716/anaconda3/envs/arsh_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 1/230 [05:42<21:46:07, 342.22s/it]                                                     0%|          | 1/230 [05:42<21:46:07, 342.22s/it]  1%|          | 2/230 [06:50<11:29:02, 181.33s/it]                                                     1%|          | 2/230 [06:50<11:29:02, 181.33s/it]  1%|▏         | 3/230 [12:33<16:05:13, 255.12s/it]                                                     1%|▏         | 3/230 [12:33<16:05:13, 255.12s/it]  2%|▏         | 4/230 [13:42<11:24:03, 181.61s/it]                                                     2%|▏         | 4/230 [13:42<11:24:03, 181.61s/it]  2%|▏         | 5/230 [19:25<14:59:22, 239.84s/it]                                                     2%|▏         | 5/230 [19:25<14:59:22, 239.84s/it]  3%|▎         | 6/230 [20:34<11:18:08, 181.64s/it]                                                     3%|▎         | 6/230 [20:34<11:18:08, 181.64s/it]  3%|▎         | 7/230 [26:15<14:28:57, 233.80s/it]                                                     3%|▎         | 7/230 [26:15<14:28:57, 233.80s/it]  3%|▎         | 8/230 [27:24<11:10:47, 181.29s/it]                                                     3%|▎         | 8/230 [27:24<11:10:47, 181.29s/it]  4%|▍         | 9/230 [33:06<14:12:37, 231.48s/it]                                                     4%|▍         | 9/230 [33:06<14:12:37, 231.48s/it]  4%|▍         | 10/230 [34:15<11:04:39, 181.27s/it]                                                      4%|▍         | 10/230 [34:15<11:04:39, 181.27s/it]  5%|▍         | 11/230 [39:56<13:59:59, 230.14s/it]                                                      5%|▍         | 11/230 [39:56<13:59:59, 230.14s/it]  5%|▌         | 12/230 [41:05<10:57:59, 181.10s/it]                                                      5%|▌         | 12/230 [41:05<10:57:59, 181.10s/it]  6%|▌         | 13/230 [46:45<13:49:05, 229.24s/it]                                                      6%|▌         | 13/230 [46:45<13:49:05, 229.24s/it]  6%|▌         | 14/230 [47:53<10:50:22, 180.66s/it]                                                      6%|▌         | 14/230 [47:53<10:50:22, 180.66s/it]  7%|▋         | 15/230 [53:34<13:40:52, 229.08s/it]                                                      7%|▋         | 15/230 [53:34<13:40:52, 229.08s/it]  7%|▋         | 16/230 [54:43<10:44:17, 180.64s/it]                                                      7%|▋         | 16/230 [54:43<10:44:17, 180.64s/it]  7%|▋         | 17/230 [1:00:24<13:32:56, 229.00s/it]                                                        7%|▋         | 17/230 [1:00:24<13:32:56, 229.00s/it]  8%|▊         | 18/230 [1:01:33<10:39:06, 180.88s/it]                                                        8%|▊         | 18/230 [1:01:33<10:39:06, 180.88s/it]  8%|▊         | 19/230 [1:07:13<13:24:46, 228.84s/it]                                                        8%|▊         | 19/230 [1:07:13<13:24:46, 228.84s/it]  9%|▊         | 20/230 [1:08:22<10:32:14, 180.64s/it]                                                        9%|▊         | 20/230 [1:08:22<10:32:14, 180.64s/it]  9%|▉         | 21/230 [1:14:03<13:17:09, 228.85s/it]                                                        9%|▉         | 21/230 [1:14:03<13:17:09, 228.85s/it] 10%|▉         | 22/230 [1:15:11<10:26:34, 180.74s/it]                                                       10%|▉         | 22/230 [1:15:11<10:26:34, 180.74s/it] 10%|█         | 23/230 [1:20:54<13:10:44, 229.20s/it]                                                       10%|█         | 23/230 [1:20:54<13:10:44, 229.20s/it] 10%|█         | 24/230 [1:22:03<10:22:17, 181.25s/it]                                                       10%|█         | 24/230 [1:22:03<10:22:17, 181.25s/it] 11%|█         | 25/230 [1:27:44<13:02:57, 229.16s/it]                                                       11%|█         | 25/230 [1:27:44<13:02:57, 229.16s/it] 11%|█▏        | 26/230 [1:28:53<10:15:15, 180.96s/it]                                                       11%|█▏        | 26/230 [1:28:53<10:15:15, 180.96s/it] 12%|█▏        | 27/230 [1:34:33<12:53:40, 228.67s/it]                                                       12%|█▏        | 27/230 [1:34:33<12:53:40, 228.67s/it] 12%|█▏        | 28/230 [1:35:41<10:08:19, 180.69s/it]                                                       12%|█▏        | 28/230 [1:35:41<10:08:19, 180.69s/it] 13%|█▎        | 29/230 [1:41:22<12:45:55, 228.64s/it]                                                       13%|█▎        | 29/230 [1:41:22<12:45:55, 228.64s/it] 13%|█▎        | 30/230 [1:42:31<10:02:13, 180.67s/it]                                                       13%|█▎        | 30/230 [1:42:31<10:02:13, 180.67s/it] 13%|█▎        | 31/230 [1:48:09<12:36:42, 228.15s/it]                                                       13%|█▎        | 31/230 [1:48:10<12:36:42, 228.15s/it] 14%|█▍        | 32/230 [1:49:18<9:55:03, 180.32s/it]                                                       14%|█▍        | 32/230 [1:49:18<9:55:03, 180.32s/it] 14%|█▍        | 33/230 [1:54:58<12:29:03, 228.14s/it]                                                       14%|█▍        | 33/230 [1:54:58<12:29:03, 228.14s/it] 15%|█▍        | 34/230 [1:56:06<9:48:39, 180.20s/it]                                                       15%|█▍        | 34/230 [1:56:06<9:48:39, 180.20s/it] 15%|█▌        | 35/230 [2:01:47<12:21:54, 228.28s/it]                                                       15%|█▌        | 35/230 [2:01:47<12:21:54, 228.28s/it] 16%|█▌        | 36/230 [2:02:55<9:42:52, 180.27s/it]                                                       16%|█▌        | 36/230 [2:02:55<9:42:52, 180.27s/it] 16%|█▌        | 37/230 [2:08:36<12:15:13, 228.57s/it]                                                       16%|█▌        | 37/230 [2:08:36<12:15:13, 228.57s/it] 17%|█▋        | 38/230 [2:09:45<9:37:55, 180.60s/it]                                                       17%|█▋        | 38/230 [2:09:45<9:37:55, 180.60s/it] 17%|█▋        | 39/230 [2:15:25<12:07:07, 228.42s/it]                                                       17%|█▋        | 39/230 [2:15:25<12:07:07, 228.42s/it] 17%|█▋        | 40/230 [2:16:34<9:31:35, 180.50s/it]                                                       17%|█▋        | 40/230 [2:16:34<9:31:35, 180.50s/it] 18%|█▊        | 41/230 [2:22:15<12:00:23, 228.70s/it]                                                       18%|█▊        | 41/230 [2:22:15<12:00:23, 228.70s/it] 18%|█▊        | 42/230 [2:23:23<9:26:03, 180.66s/it]                                                       18%|█▊        | 42/230 [2:23:23<9:26:03, 180.66s/it] 19%|█▊        | 43/230 [2:29:04<11:52:59, 228.77s/it]                                                       19%|█▊        | 43/230 [2:29:04<11:52:59, 228.77s/it] 19%|█▉        | 44/230 [2:30:13<9:19:58, 180.64s/it]                                                       19%|█▉        | 44/230 [2:30:13<9:19:58, 180.64s/it] 20%|█▉        | 45/230 [2:35:54<11:45:46, 228.90s/it]                                                       20%|█▉        | 45/230 [2:35:54<11:45:46, 228.90s/it] 20%|██        | 46/230 [2:37:03<9:14:14, 180.73s/it]                                                       20%|██        | 46/230 [2:37:03<9:14:14, 180.73s/it] 20%|██        | 47/230 [2:42:44<11:38:20, 228.97s/it]                                                       20%|██        | 47/230 [2:42:44<11:38:20, 228.97s/it] 21%|██        | 48/230 [2:43:52<9:08:04, 180.69s/it]                                                       21%|██        | 48/230 [2:43:52<9:08:04, 180.69s/it] 21%|██▏       | 49/230 [2:49:33<11:29:38, 228.61s/it]                                                       21%|██▏       | 49/230 [2:49:33<11:29:38, 228.61s/it] 22%|██▏       | 50/230 [2:50:41<9:02:09, 180.72s/it]                                                       22%|██▏       | 50/230 [2:50:42<9:02:09, 180.72s/it] 22%|██▏       | 51/230 [2:56:22<11:21:49, 228.54s/it]                                                       22%|██▏       | 51/230 [2:56:22<11:21:49, 228.54s/it] 23%|██▎       | 52/230 [2:57:31<8:55:57, 180.66s/it]                                                       23%|██▎       | 52/230 [2:57:31<8:55:57, 180.66s/it] 23%|██▎       | 53/230 [3:03:12<11:15:10, 228.87s/it]                                                       23%|██▎       | 53/230 [3:03:12<11:15:10, 228.87s/it] 23%|██▎       | 54/230 [3:04:23<8:52:05, 181.39s/it]                                                       23%|██▎       | 54/230 [3:04:23<8:52:05, 181.39s/it] 24%|██▍       | 55/230 [3:10:03<11:08:29, 229.19s/it]                                                       24%|██▍       | 55/230 [3:10:03<11:08:29, 229.19s/it] 24%|██▍       | 56/230 [3:11:12<8:45:05, 181.07s/it]                                                       24%|██▍       | 56/230 [3:11:12<8:45:05, 181.07s/it] 25%|██▍       | 57/230 [3:16:53<11:00:26, 229.05s/it]                                                       25%|██▍       | 57/230 [3:16:53<11:00:26, 229.05s/it] 25%|██▌       | 58/230 [3:18:02<8:38:32, 180.88s/it]                                                       25%|██▌       | 58/230 [3:18:02<8:38:32, 180.88s/it] 26%|██▌       | 59/230 [3:23:43<10:52:54, 229.09s/it]                                                       26%|██▌       | 59/230 [3:23:43<10:52:54, 229.09s/it] 26%|██▌       | 60/230 [3:24:51<8:31:53, 180.67s/it]                                                       26%|██▌       | 60/230 [3:24:51<8:31:53, 180.67s/it] 27%|██▋       | 61/230 [3:30:31<10:43:38, 228.51s/it]                                                       27%|██▋       | 61/230 [3:30:31<10:43:38, 228.51s/it] 27%|██▋       | 62/230 [3:31:39<8:24:55, 180.33s/it]                                                       27%|██▋       | 62/230 [3:31:39<8:24:55, 180.33s/it] 27%|██▋       | 63/230 [3:37:20<10:36:24, 228.65s/it]                                                       27%|██▋       | 63/230 [3:37:20<10:36:24, 228.65s/it] 28%|██▊       | 64/230 [3:38:29<8:20:02, 180.74s/it]                                                       28%|██▊       | 64/230 [3:38:29<8:20:02, 180.74s/it] 28%|██▊       | 65/230 [3:44:09<10:28:02, 228.38s/it]                                                       28%|██▊       | 65/230 [3:44:09<10:28:02, 228.38s/it] 29%|██▊       | 66/230 [3:45:16<8:12:27, 180.17s/it]                                                       29%|██▊       | 66/230 [3:45:16<8:12:27, 180.17s/it] 29%|██▉       | 67/230 [3:50:55<10:18:28, 227.66s/it]                                                       29%|██▉       | 67/230 [3:50:55<10:18:28, 227.66s/it] 30%|██▉       | 68/230 [3:52:04<8:05:54, 179.97s/it]                                                       30%|██▉       | 68/230 [3:52:04<8:05:54, 179.97s/it] 30%|███       | 69/230 [3:57:46<10:13:23, 228.59s/it]                                                       30%|███       | 69/230 [3:57:46<10:13:23, 228.59s/it] 30%|███       | 70/230 [3:58:54<8:01:21, 180.51s/it]                                                       30%|███       | 70/230 [3:58:54<8:01:21, 180.51s/it] 31%|███       | 71/230 [4:04:35<10:05:45, 228.59s/it]                                                       31%|███       | 71/230 [4:04:35<10:05:45, 228.59s/it] 31%|███▏      | 72/230 [4:05:43<7:55:05, 180.41s/it]                                                       31%|███▏      | 72/230 [4:05:43<7:55:05, 180.41s/it] 32%|███▏      | 73/230 [4:11:23<9:57:49, 228.47s/it]                                                      32%|███▏      | 73/230 [4:11:23<9:57:49, 228.47s/it] 32%|███▏      | 74/230 [4:12:32<7:49:20, 180.52s/it]                                                      32%|███▏      | 74/230 [4:12:32<7:49:20, 180.52s/it] 33%|███▎      | 75/230 [4:18:11<9:49:30, 228.20s/it]                                                      33%|███▎      | 75/230 [4:18:11<9:49:30, 228.20s/it] 33%|███▎      | 76/230 [4:19:20<7:42:28, 180.19s/it]                                                      33%|███▎      | 76/230 [4:19:20<7:42:28, 180.19s/it] 33%|███▎      | 77/230 [4:25:03<9:44:18, 229.14s/it]                                                      33%|███▎      | 77/230 [4:25:03<9:44:18, 229.14s/it] 34%|███▍      | 78/230 [4:26:12<7:38:45, 181.09s/it]                                                      34%|███▍      | 78/230 [4:26:12<7:38:45, 181.09s/it] 34%|███▍      | 79/230 [4:31:54<9:36:59, 229.27s/it]                                                      34%|███▍      | 79/230 [4:31:54<9:36:59, 229.27s/it] 35%|███▍      | 80/230 [4:33:03<7:33:09, 181.27s/it]                                                      35%|███▍      | 80/230 [4:33:03<7:33:09, 181.27s/it] 35%|███▌      | 81/230 [4:38:45<9:30:00, 229.53s/it]                                                      35%|███▌      | 81/230 [4:38:45<9:30:00, 229.53s/it] 36%|███▌      | 82/230 [4:39:53<7:26:32, 181.03s/it]                                                      36%|███▌      | 82/230 [4:39:53<7:26:32, 181.03s/it] 36%|███▌      | 83/230 [4:45:35<9:22:02, 229.40s/it]                                                      36%|███▌      | 83/230 [4:45:35<9:22:02, 229.40s/it] 37%|███▋      | 84/230 [4:46:44<7:20:48, 181.16s/it]                                                      37%|███▋      | 84/230 [4:46:44<7:20:48, 181.16s/it] 37%|███▋      | 85/230 [4:52:25<9:13:38, 229.09s/it]                                                      37%|███▋      | 85/230 [4:52:25<9:13:38, 229.09s/it] 37%|███▋      | 86/230 [4:53:32<7:13:36, 180.67s/it]                                                      37%|███▋      | 86/230 [4:53:32<7:13:36, 180.67s/it] 38%|███▊      | 87/230 [4:59:14<9:05:29, 228.88s/it]                                                      38%|███▊      | 87/230 [4:59:14<9:05:29, 228.88s/it] 38%|███▊      | 88/230 [5:00:22<7:07:49, 180.77s/it]                                                      38%|███▊      | 88/230 [5:00:22<7:07:49, 180.77s/it] 39%|███▊      | 89/230 [5:06:05<8:59:17, 229.49s/it]                                                      39%|███▊      | 89/230 [5:06:05<8:59:17, 229.49s/it] 39%|███▉      | 90/230 [5:07:13<7:02:26, 181.04s/it]                                                      39%|███▉      | 90/230 [5:07:13<7:02:26, 181.04s/it] 40%|███▉      | 91/230 [5:12:53<8:49:17, 228.47s/it]                                                      40%|███▉      | 91/230 [5:12:53<8:49:17, 228.47s/it] 40%|████      | 92/230 [5:14:01<6:55:10, 180.51s/it]                                                      40%|████      | 92/230 [5:14:01<6:55:10, 180.51s/it] 40%|████      | 93/230 [5:19:43<8:42:39, 228.90s/it]                                                      40%|████      | 93/230 [5:19:43<8:42:39, 228.90s/it] 41%|████      | 94/230 [5:20:52<6:49:54, 180.84s/it]                                                      41%|████      | 94/230 [5:20:52<6:49:54, 180.84s/it] 41%|████▏     | 95/230 [5:26:33<8:34:58, 228.88s/it]                                                      41%|████▏     | 95/230 [5:26:33<8:34:58, 228.88s/it] 42%|████▏     | 96/230 [5:27:41<6:43:49, 180.81s/it]                                                      42%|████▏     | 96/230 [5:27:41<6:43:49, 180.81s/it] 42%|████▏     | 97/230 [5:33:23<8:27:49, 229.10s/it]                                                      42%|████▏     | 97/230 [5:33:23<8:27:49, 229.10s/it] 43%|████▎     | 98/230 [5:34:31<6:37:27, 180.66s/it]                                                      43%|████▎     | 98/230 [5:34:31<6:37:27, 180.66s/it] 43%|████▎     | 99/230 [5:40:13<8:20:16, 229.13s/it]                                                      43%|████▎     | 99/230 [5:40:13<8:20:16, 229.13s/it] 43%|████▎     | 100/230 [5:41:21<6:31:59, 180.92s/it]                                                       43%|████▎     | 100/230 [5:41:21<6:31:59, 180.92s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-100
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-100/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-100/special_tokens_map.json
 44%|████▍     | 101/230 [5:47:25<8:27:00, 235.82s/it]                                                       44%|████▍     | 101/230 [5:47:25<8:27:00, 235.82s/it] 44%|████▍     | 102/230 [5:48:34<6:36:06, 185.68s/it]                                                       44%|████▍     | 102/230 [5:48:34<6:36:06, 185.68s/it] 45%|████▍     | 103/230 [5:54:17<8:12:40, 232.76s/it]                                                       45%|████▍     | 103/230 [5:54:17<8:12:40, 232.76s/it] 45%|████▌     | 104/230 [5:55:25<6:25:18, 183.48s/it]                                                       45%|████▌     | 104/230 [5:55:25<6:25:18, 183.48s/it] 46%|████▌     | 105/230 [6:01:05<7:59:57, 230.38s/it]                                                       46%|████▌     | 105/230 [6:01:05<7:59:57, 230.38s/it] 46%|████▌     | 106/230 [6:02:13<6:15:24, 181.65s/it]                                                       46%|████▌     | 106/230 [6:02:13<6:15:24, 181.65s/it] 47%|████▋     | 107/230 [6:07:53<7:50:04, 229.30s/it]                                                       47%|████▋     | 107/230 [6:07:53<7:50:04, 229.30s/it] 47%|████▋     | 108/230 [6:09:02<6:08:20, 181.15s/it]                                                       47%|████▋     | 108/230 [6:09:02<6:08:20, 181.15s/it] 47%|████▋     | 109/230 [6:14:44<7:42:28, 229.33s/it]                                                       47%|████▋     | 109/230 [6:14:44<7:42:28, 229.33s/it] 48%|████▊     | 110/230 [6:15:53<6:02:30, 181.26s/it]                                                       48%|████▊     | 110/230 [6:15:53<6:02:30, 181.26s/it] 48%|████▊     | 111/230 [6:21:34<7:34:28, 229.15s/it]                                                       48%|████▊     | 111/230 [6:21:34<7:34:28, 229.15s/it] 49%|████▊     | 112/230 [6:22:43<5:56:08, 181.09s/it]                                                       49%|████▊     | 112/230 [6:22:43<5:56:08, 181.09s/it] 49%|████▉     | 113/230 [6:28:25<7:27:32, 229.51s/it]                                                       49%|████▉     | 113/230 [6:28:25<7:27:32, 229.51s/it] 50%|████▉     | 114/230 [6:29:33<5:49:55, 180.99s/it]                                                       50%|████▉     | 114/230 [6:29:33<5:49:55, 180.99s/it] 50%|█████     | 115/230 [6:35:15<7:19:35, 229.35s/it]                                                       50%|█████     | 115/230 [6:35:15<7:19:35, 229.35s/it] 50%|█████     | 116/230 [6:36:24<5:44:13, 181.17s/it]                                                       50%|█████     | 116/230 [6:36:24<5:44:13, 181.17s/it] 51%|█████     | 117/230 [6:42:05<7:11:18, 229.01s/it]                                                       51%|█████     | 117/230 [6:42:05<7:11:18, 229.01s/it] 51%|█████▏    | 118/230 [6:43:12<5:37:06, 180.60s/it]                                                       51%|█████▏    | 118/230 [6:43:12<5:37:06, 180.60s/it] 52%|█████▏    | 119/230 [6:48:52<7:02:17, 228.27s/it]                                                       52%|█████▏    | 119/230 [6:48:52<7:02:17, 228.27s/it] 52%|█████▏    | 120/230 [6:50:01<5:31:09, 180.63s/it]                                                       52%|█████▏    | 120/230 [6:50:01<5:31:09, 180.63s/it] 53%|█████▎    | 121/230 [6:55:42<6:55:38, 228.80s/it]                                                       53%|█████▎    | 121/230 [6:55:42<6:55:38, 228.80s/it] 53%|█████▎    | 122/230 [6:56:50<5:24:58, 180.54s/it]                                                       53%|█████▎    | 122/230 [6:56:50<5:24:58, 180.54s/it] 53%|█████▎    | 123/230 [7:02:31<6:47:43, 228.63s/it]                                                       53%|█████▎    | 123/230 [7:02:31<6:47:43, 228.63s/it] 54%|█████▍    | 124/230 [7:03:42<5:20:13, 181.26s/it]                                                       54%|█████▍    | 124/230 [7:03:42<5:20:13, 181.26s/it] 54%|█████▍    | 125/230 [7:09:23<6:41:10, 229.24s/it]                                                       54%|█████▍    | 125/230 [7:09:23<6:41:10, 229.24s/it] 55%|█████▍    | 126/230 [7:10:32<5:13:44, 181.00s/it]                                                       55%|█████▍    | 126/230 [7:10:32<5:13:44, 181.00s/it] 55%|█████▌    | 127/230 [7:16:13<6:33:33, 229.26s/it]                                                       55%|█████▌    | 127/230 [7:16:13<6:33:33, 229.26s/it] 56%|█████▌    | 128/230 [7:17:22<5:07:39, 180.97s/it]                                                       56%|█████▌    | 128/230 [7:17:22<5:07:39, 180.97s/it] 56%|█████▌    | 129/230 [7:23:02<6:25:12, 228.84s/it]                                                       56%|█████▌    | 129/230 [7:23:02<6:25:12, 228.84s/it] 57%|█████▋    | 130/230 [7:24:10<5:01:04, 180.64s/it]                                                       57%|█████▋    | 130/230 [7:24:10<5:01:04, 180.64s/it] 57%|█████▋    | 131/230 [7:29:52<6:17:40, 228.89s/it]                                                       57%|█████▋    | 131/230 [7:29:52<6:17:40, 228.89s/it] 57%|█████▋    | 132/230 [7:31:00<4:55:02, 180.64s/it]                                                       57%|█████▋    | 132/230 [7:31:00<4:55:02, 180.64s/it] 58%|█████▊    | 133/230 [7:36:41<6:09:35, 228.61s/it]                                                       58%|█████▊    | 133/230 [7:36:41<6:09:35, 228.61s/it] 58%|█████▊    | 134/230 [7:37:49<4:48:43, 180.45s/it]                                                       58%|█████▊    | 134/230 [7:37:49<4:48:43, 180.45s/it] 59%|█████▊    | 135/230 [7:43:28<6:01:23, 228.24s/it]                                                       59%|█████▊    | 135/230 [7:43:28<6:01:23, 228.24s/it] 59%|█████▉    | 136/230 [7:44:37<4:42:39, 180.42s/it]                                                       59%|█████▉    | 136/230 [7:44:37<4:42:39, 180.42s/it] 60%|█████▉    | 137/230 [7:50:19<5:54:46, 228.89s/it]                                                       60%|█████▉    | 137/230 [7:50:19<5:54:46, 228.89s/it] 60%|██████    | 138/230 [7:51:27<4:37:04, 180.71s/it]                                                       60%|██████    | 138/230 [7:51:27<4:37:04, 180.71s/it] 60%|██████    | 139/230 [7:57:07<5:46:21, 228.37s/it]                                                       60%|██████    | 139/230 [7:57:07<5:46:21, 228.37s/it] 61%|██████    | 140/230 [7:58:15<4:30:28, 180.32s/it]                                                       61%|██████    | 140/230 [7:58:15<4:30:28, 180.32s/it] 61%|██████▏   | 141/230 [8:03:56<5:38:53, 228.47s/it]                                                       61%|██████▏   | 141/230 [8:03:56<5:38:53, 228.47s/it] 62%|██████▏   | 142/230 [8:05:04<4:24:31, 180.36s/it]                                                       62%|██████▏   | 142/230 [8:05:04<4:24:31, 180.36s/it] 62%|██████▏   | 143/230 [8:10:46<5:31:46, 228.82s/it]                                                       62%|██████▏   | 143/230 [8:10:46<5:31:46, 228.82s/it] 63%|██████▎   | 144/230 [8:11:55<4:19:03, 180.74s/it]                                                       63%|██████▎   | 144/230 [8:11:55<4:19:03, 180.74s/it] 63%|██████▎   | 145/230 [8:17:34<5:23:32, 228.38s/it]                                                       63%|██████▎   | 145/230 [8:17:34<5:23:32, 228.38s/it] 63%|██████▎   | 146/230 [8:18:43<4:12:37, 180.45s/it]                                                       63%|██████▎   | 146/230 [8:18:43<4:12:37, 180.45s/it] 64%|██████▍   | 147/230 [8:24:25<5:16:52, 229.06s/it]                                                       64%|██████▍   | 147/230 [8:24:25<5:16:52, 229.06s/it] 64%|██████▍   | 148/230 [8:25:34<4:07:11, 180.88s/it]                                                       64%|██████▍   | 148/230 [8:25:34<4:07:11, 180.88s/it] 65%|██████▍   | 149/230 [8:31:16<5:09:27, 229.23s/it]                                                       65%|██████▍   | 149/230 [8:31:16<5:09:27, 229.23s/it] 65%|██████▌   | 150/230 [8:32:25<4:01:40, 181.26s/it]                                                       65%|██████▌   | 150/230 [8:32:25<4:01:40, 181.26s/it] 66%|██████▌   | 151/230 [8:38:07<5:02:02, 229.40s/it]                                                       66%|██████▌   | 151/230 [8:38:07<5:02:02, 229.40s/it] 66%|██████▌   | 152/230 [8:39:15<3:55:23, 181.07s/it]                                                       66%|██████▌   | 152/230 [8:39:15<3:55:23, 181.07s/it] 67%|██████▋   | 153/230 [8:44:54<4:53:07, 228.41s/it]                                                       67%|██████▋   | 153/230 [8:44:54<4:53:07, 228.41s/it] 67%|██████▋   | 154/230 [8:46:03<3:48:37, 180.49s/it]                                                       67%|██████▋   | 154/230 [8:46:03<3:48:37, 180.49s/it] 67%|██████▋   | 155/230 [8:51:42<4:45:16, 228.22s/it]                                                       67%|██████▋   | 155/230 [8:51:42<4:45:16, 228.22s/it] 68%|██████▊   | 156/230 [8:52:50<3:42:11, 180.15s/it]                                                       68%|██████▊   | 156/230 [8:52:50<3:42:11, 180.15s/it] 68%|██████▊   | 157/230 [8:58:32<4:38:20, 228.78s/it]                                                       68%|██████▊   | 157/230 [8:58:32<4:38:20, 228.78s/it] 69%|██████▊   | 158/230 [8:59:41<3:37:00, 180.84s/it]                                                       69%|██████▊   | 158/230 [8:59:41<3:37:00, 180.84s/it] 69%|██████▉   | 159/230 [9:05:24<4:31:33, 229.48s/it]                                                       69%|██████▉   | 159/230 [9:05:24<4:31:33, 229.48s/it] 70%|██████▉   | 160/230 [9:06:33<3:31:28, 181.26s/it]                                                       70%|██████▉   | 160/230 [9:06:33<3:31:28, 181.26s/it] 70%|███████   | 161/230 [9:12:14<4:23:25, 229.06s/it]                                                       70%|███████   | 161/230 [9:12:14<4:23:25, 229.06s/it] 70%|███████   | 162/230 [9:13:21<3:24:39, 180.58s/it]                                                       70%|███████   | 162/230 [9:13:21<3:24:39, 180.58s/it] 71%|███████   | 163/230 [9:19:01<4:14:55, 228.29s/it]                                                       71%|███████   | 163/230 [9:19:01<4:14:55, 228.29s/it] 71%|███████▏  | 164/230 [9:20:10<3:18:27, 180.42s/it]                                                       71%|███████▏  | 164/230 [9:20:10<3:18:27, 180.42s/it] 72%|███████▏  | 165/230 [9:25:50<4:07:28, 228.44s/it]                                                       72%|███████▏  | 165/230 [9:25:50<4:07:28, 228.44s/it] 72%|███████▏  | 166/230 [9:26:58<3:12:15, 180.25s/it]                                                       72%|███████▏  | 166/230 [9:26:58<3:12:15, 180.25s/it] 73%|███████▎  | 167/230 [9:32:39<3:59:51, 228.43s/it]                                                       73%|███████▎  | 167/230 [9:32:39<3:59:51, 228.43s/it] 73%|███████▎  | 168/230 [9:33:48<3:06:37, 180.60s/it]                                                       73%|███████▎  | 168/230 [9:33:48<3:06:37, 180.60s/it] 73%|███████▎  | 169/230 [9:39:27<3:52:02, 228.25s/it]                                                       73%|███████▎  | 169/230 [9:39:27<3:52:02, 228.25s/it] 74%|███████▍  | 170/230 [9:40:36<3:00:28, 180.48s/it]                                                       74%|███████▍  | 170/230 [9:40:36<3:00:28, 180.48s/it] 74%|███████▍  | 171/230 [9:46:19<3:45:14, 229.06s/it]                                                       74%|███████▍  | 171/230 [9:46:19<3:45:14, 229.06s/it] 75%|███████▍  | 172/230 [9:47:28<2:55:06, 181.15s/it]                                                       75%|███████▍  | 172/230 [9:47:28<2:55:06, 181.15s/it] 75%|███████▌  | 173/230 [9:53:08<3:37:25, 228.87s/it]                                                       75%|███████▌  | 173/230 [9:53:08<3:37:25, 228.87s/it] 76%|███████▌  | 174/230 [9:54:16<2:48:37, 180.67s/it]                                                       76%|███████▌  | 174/230 [9:54:16<2:48:37, 180.67s/it] 76%|███████▌  | 175/230 [9:59:59<3:30:04, 229.17s/it]                                                       76%|███████▌  | 175/230 [9:59:59<3:30:04, 229.17s/it] 77%|███████▋  | 176/230 [10:01:07<2:42:47, 180.88s/it]                                                        77%|███████▋  | 176/230 [10:01:07<2:42:47, 180.88s/it] 77%|███████▋  | 177/230 [10:06:47<3:22:00, 228.69s/it]                                                        77%|███████▋  | 177/230 [10:06:47<3:22:00, 228.69s/it] 77%|███████▋  | 178/230 [10:07:56<2:36:32, 180.62s/it]                                                        77%|███████▋  | 178/230 [10:07:56<2:36:32, 180.62s/it] 78%|███████▊  | 179/230 [10:13:34<3:13:50, 228.06s/it]                                                        78%|███████▊  | 179/230 [10:13:34<3:13:50, 228.06s/it] 78%|███████▊  | 180/230 [10:14:43<2:30:13, 180.26s/it]                                                        78%|███████▊  | 180/230 [10:14:43<2:30:13, 180.26s/it] 79%|███████▊  | 181/230 [10:20:24<3:06:31, 228.41s/it]                                                        79%|███████▊  | 181/230 [10:20:24<3:06:31, 228.41s/it] 79%|███████▉  | 182/230 [10:21:32<2:24:17, 180.36s/it]                                                        79%|███████▉  | 182/230 [10:21:32<2:24:17, 180.36s/it] 80%|███████▉  | 183/230 [10:27:15<2:59:32, 229.21s/it]                                                        80%|███████▉  | 183/230 [10:27:15<2:59:32, 229.21s/it] 80%|████████  | 184/230 [10:28:23<2:18:36, 180.79s/it]                                                        80%|████████  | 184/230 [10:28:23<2:18:36, 180.79s/it] 80%|████████  | 185/230 [10:34:04<2:51:41, 228.91s/it]                                                        80%|████████  | 185/230 [10:34:04<2:51:41, 228.91s/it] 81%|████████  | 186/230 [10:35:13<2:12:38, 180.87s/it]                                                        81%|████████  | 186/230 [10:35:13<2:12:38, 180.87s/it] 81%|████████▏ | 187/230 [10:40:54<2:44:01, 228.88s/it]                                                        81%|████████▏ | 187/230 [10:40:54<2:44:01, 228.88s/it] 82%|████████▏ | 188/230 [10:42:01<2:06:12, 180.30s/it]                                                        82%|████████▏ | 188/230 [10:42:01<2:06:12, 180.30s/it] 82%|████████▏ | 189/230 [10:47:43<2:36:16, 228.70s/it]                                                        82%|████████▏ | 189/230 [10:47:43<2:36:16, 228.70s/it] 83%|████████▎ | 190/230 [10:48:51<2:00:24, 180.62s/it]                                                        83%|████████▎ | 190/230 [10:48:51<2:00:24, 180.62s/it] 83%|████████▎ | 191/230 [10:54:31<2:28:34, 228.57s/it]                                                        83%|████████▎ | 191/230 [10:54:31<2:28:34, 228.57s/it] 83%|████████▎ | 192/230 [10:55:39<1:54:16, 180.42s/it]                                                        83%|████████▎ | 192/230 [10:55:39<1:54:16, 180.42s/it] 84%|████████▍ | 193/230 [11:01:20<2:20:55, 228.51s/it]                                                        84%|████████▍ | 193/230 [11:01:20<2:20:55, 228.51s/it] 84%|████████▍ | 194/230 [11:02:30<1:48:28, 180.80s/it]                                                        84%|████████▍ | 194/230 [11:02:30<1:48:28, 180.80s/it] 85%|████████▍ | 195/230 [11:08:11<2:13:29, 228.85s/it]                                                        85%|████████▍ | 195/230 [11:08:11<2:13:29, 228.85s/it] 85%|████████▌ | 196/230 [11:09:20<1:42:29, 180.87s/it]                                                        85%|████████▌ | 196/230 [11:09:20<1:42:29, 180.87s/it] 86%|████████▌ | 197/230 [11:15:00<2:05:51, 228.85s/it]                                                        86%|████████▌ | 197/230 [11:15:00<2:05:51, 228.85s/it] 86%|████████▌ | 198/230 [11:16:09<1:36:20, 180.65s/it]                                                        86%|████████▌ | 198/230 [11:16:09<1:36:20, 180.65s/it] 87%|████████▋ | 199/230 [11:21:50<1:58:11, 228.77s/it]                                                        87%|████████▋ | 199/230 [11:21:50<1:58:11, 228.77s/it] 87%|████████▋ | 200/230 [11:22:58<1:30:20, 180.70s/it]                                                        87%|████████▋ | 200/230 [11:22:58<1:30:20, 180.70s/it]Saving model checkpoint to /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-200
loading configuration file config.json from cache at /home/axs7716/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/a009b8db2439814febe725486a5ed388f12a8744/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pad_token_id": 128004,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "unsloth_version": "2024.9",
  "use_cache": true,
  "vocab_size": 128256
}

tokenizer config file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-200/tokenizer_config.json
Special tokens file saved in /scratch/axs7716Arsh/centaur-ptsd-finetuned_0.0002_8870_2_2_10_0.01_115/checkpoint-200/special_tokens_map.json
 87%|████████▋ | 201/230 [11:29:00<1:53:34, 234.99s/it]                                                        87%|████████▋ | 201/230 [11:29:00<1:53:34, 234.99s/it] 88%|████████▊ | 202/230 [11:30:08<1:26:19, 184.99s/it]                                                        88%|████████▊ | 202/230 [11:30:08<1:26:19, 184.99s/it] 88%|████████▊ | 203/230 [11:35:51<1:44:30, 232.25s/it]                                                        88%|████████▊ | 203/230 [11:35:51<1:44:30, 232.25s/it] 89%|████████▊ | 204/230 [11:36:59<1:19:23, 183.20s/it]                                                        89%|████████▊ | 204/230 [11:36:59<1:19:23, 183.20s/it] 89%|████████▉ | 205/230 [11:42:42<1:36:13, 230.92s/it]                                                        89%|████████▉ | 205/230 [11:42:42<1:36:13, 230.92s/it] 90%|████████▉ | 206/230 [11:43:50<1:12:52, 182.19s/it]                                                        90%|████████▉ | 206/230 [11:43:50<1:12:52, 182.19s/it] 90%|█████████ | 207/230 [11:49:32<1:28:12, 230.13s/it]                                                        90%|█████████ | 207/230 [11:49:32<1:28:12, 230.13s/it] 90%|█████████ | 208/230 [11:50:40<1:06:34, 181.58s/it]                                                        90%|█████████ | 208/230 [11:50:40<1:06:34, 181.58s/it]slurmstepd-h100-8s-06: error: *** JOB 7102 ON h100-8s-06 CANCELLED AT 2025-12-01T06:08:24 DUE TO TIME LIMIT ***
